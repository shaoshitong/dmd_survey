@book{LaTeX.Companion,
  author    = {Frank Mittelbach and Michel Goossens and Johannes Braams and David Carlisle and Chris Rowley},
  title     = {The {LaTeX} Companion},
  year      = {1993},
  publisher = {Addison-Wesley},
  address   = {Reading, Massachusetts}
}

@article{KeshavACMSIGCOMMComput.Commun.Rev.2007,
  title        = {How to Read a Paper},
  author       = {Keshav, S.},
  date         = {2007-07-20},
  journaltitle = {ACM SIGCOMM Computer Communication Review},
  shortjournal = {SIGCOMM Comput. Commun. Rev.},
  volume       = {37},
  number       = {3},
  pages        = {83--84},
  doi          = {10.1145/1273445.1273458},
  url          = {https://doi.org/10.1145/1273445.1273458}
}

@article{WhitesidesAdv.Mater.2004,
  title        = {Whitesides' {{Group}}: {{Writing}} a {{Paper}}},
  shorttitle   = {Whitesides' {{Group}}},
  author       = {Whitesides, G. M.},
  date         = {2004},
  journaltitle = {Advanced Materials},
  volume       = {16},
  number       = {15},
  pages        = {1375--1377},
  doi          = {10.1002/adma.200400767},
  url          = {https://onlinelibrary.wiley.com/doi/abs/10.1002/adma.200400767}
}

@article{ChaiACSNano2022,
  title        = {Beyond {{Metrics}}},
  author       = {Chai, Yang and Chen, Xiaodong and Fan, Hong Jin and Lau, Shu Ping and Li, Shuzhou and Liu, Bin and Wong, Wai-Yeung and Zhao, Ni and Zheng, Zijian},
  date         = {2022-08-23},
  journaltitle = {ACS Nano},
  shortjournal = {ACS Nano},
  volume       = {16},
  number       = {8},
  pages        = {11485--11486},
  publisher    = {American Chemical Society},
  doi          = {10.1021/acsnano.2c07662},
  url          = {https://doi.org/10.1021/acsnano.2c07662}
}

@article{HermanNature2007,
  title        = {Following the Law},
  author       = {Herman, Irving P.},
  date         = {2007-01},
  journaltitle = {Nature},
  volume       = {445},
  number       = {7124},
  pages        = {228--228},
  publisher    = {Nature Publishing Group},
  doi          = {10.1038/nj7124-228a},
  url          = {https://www.nature.com/articles/nj7124-228a},
  issue        = {7124}
}

@article{WarrenSci.Adv.2022,
  title = {Beating the Odds for Journal Acceptance},
  author = {Warren, Warren},
  date = {2022-07-27},
  journaltitle = {Science Advances},
  volume = {8},
  number = {30},
  pages = {eadd9147},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/sciadv.add9147},
  url = {https://www.science.org/doi/10.1126/sciadv.add9147}
}

@inproceedings{Babu2020IEEE33rdInt.Conf.MicroElectroMech.Syst.MEMS2020,
  title      = {Plasma-{{Treated PDMS}} as {{Intrinsically Non-Wetting Surface}} for {{Gallium-Alloy Liquid Metal Microfluidics}}},
  booktitle  = {2020 {{IEEE}} 33rd {{International Conference}} on {{Micro Electro Mechanical Systems}} ({{MEMS}})},
  author     = {Babu, Sachin and Lee, Jeong-Bong},
  date       = {2020-01},
  pages      = {1122--1125},
  doi        = {10.1109/MEMS46641.2020.9056134},
  eventtitle = {2020 {{IEEE}} 33rd {{International Conference}} on {{Micro Electro Mechanical Systems}} ({{MEMS}})}
}

@inproceedings{JehleImagingAppl.Opt.20162016Pap.IM4F22016,
  title      = {Spatial {{Light Modulators}} in {{Laser Lithography Systems}}},
  booktitle  = {Imaging and {{Applied Optics}} 2016 (2016), Paper {{IM4F}}.2},
  author     = {Jehle, Achim and Diez, Steffen},
  date       = {2016-07-25},
  pages      = {IM4F.2},
  publisher  = {Optica Publishing Group},
  doi        = {10.1364/ISA.2016.IM4F.2},
  url        = {https://opg.optica.org/abstract.cfm?uri=ISA-2016-IM4F.2},
  eventtitle = {Imaging {{Systems}} and {{Applications}}}
}

@inproceedings{LuConf.LasersElectro-Opt.2021Pap.SW3B12021,
  title      = {On-{{Chip Optical Tweezers Based}} on {{Micro-Reflectors}}},
  booktitle  = {Conference on {{Lasers}} and {{Electro-Optics}} (2021), Paper {{SW3B}}.1},
  author     = {Lu, Jinsheng and Lu, Jinsheng and Yu, Shaoliang and Ginis, Vincent and Ginis, Vincent and Kheifets, Simon and Lim, Soon Wei Daniel and Qiu, Min and Gu, Tian and Hu, Juejun and Hu, Juejun and Capasso, Federico},
  date       = {2021-05-09},
  pages      = {SW3B.1},
  publisher  = {Optica Publishing Group},
  doi        = {10.1364/CLEO_SI.2021.SW3B.1},
  url        = {https://opg.optica.org/abstract.cfm?uri=CLEO_SI-2021-SW3B.1},
  eventtitle = {{{CLEO}}: {{Science}} and {{Innovations}}}
}

% Rolling Forcing: Autoregressive Long Video Diffusion in Real Time

@misc{https://doi.org/10.48550/arxiv.2509.25161,
  doi = {10.48550/ARXIV.2509.25161},
  url = {https://arxiv.org/abs/2509.25161},
  author = {Liu, Kunhao and Hu, Wenbo and Xu, Jiale and Shan, Ying and Lu, Shijian},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Rolling Forcing: Autoregressive Long Video Diffusion in Real Time},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% DVD-Quant: Data-free Video Diffusion Transformers Quantization

@misc{https://doi.org/10.48550/arxiv.2505.18663,
  doi = {10.48550/ARXIV.2505.18663},
  url = {https://arxiv.org/abs/2505.18663},
  author = {Li, Zhiteng and Li, Hanxuan and Wu, Junyi and Liu, Kai and Qin, Haotong and Kong, Linghe and Chen, Guihai and Zhang, Yulun and Yang, Xiaokang},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {DVD-Quant: Data-free Video Diffusion Transformers Quantization},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation

@misc{https://doi.org/10.48550/arxiv.2510.05367,
  doi = {10.48550/ARXIV.2510.05367},
  url = {https://arxiv.org/abs/2510.05367},
  author = {Xiao, Yang and Li, Gen and Deng, Kaiyuan and Wu, Yushu and Zhan, Zheng and Wang, Yanzhi and Ma, Xiaolong and Hui, Bo},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models

@inproceedings{Ge_2023, title={Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models}, url={http://dx.doi.org/10.1109/ICCV51070.2023.02096}, DOI={10.1109/iccv51070.2023.02096}, booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, publisher={IEEE}, author={Ge, Songwei and Nah, Seungjun and Liu, Guilin and Poon, Tyler and Tao, Andrew and Catanzaro, Bryan and Jacobs, David and Huang, Jia-Bin and Liu, Ming-Yu and Balaji, Yogesh}, year={2023}, month=oct, pages={22873–22884} }

% DSV: Exploiting Dynamic Sparsity to Accelerate Large-Scale Video DiT Training

@misc{https://doi.org/10.48550/arxiv.2502.07590,
  doi = {10.48550/ARXIV.2502.07590},
  url = {https://arxiv.org/abs/2502.07590},
  author = {Tan, Xin and Chen, Yuetao and Jiang, Yimin and Chen, Xing and Yan, Kun and Duan, Nan and Zhu, Yibo and Jiang, Daxin and Xu, Hong},
  keywords = {Distributed, Parallel, and Cluster Computing (cs.DC), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {DSV: Exploiting Dynamic Sparsity to Accelerate Large-Scale Video DiT Training},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% FreeNoise: Tuning-Free Longer Video Diffusion via Noise Rescheduling

@misc{https://doi.org/10.48550/arxiv.2310.15169,
  doi = {10.48550/ARXIV.2310.15169},
  url = {https://arxiv.org/abs/2310.15169},
  author = {Qiu, Haonan and Xia, Menghan and Zhang, Yong and He, Yingqing and Wang, Xintao and Shan, Ying and Liu, Ziwei},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {FreeNoise: Tuning-Free Longer Video Diffusion via Noise Rescheduling},
  publisher = {arXiv},
  year = {2023},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets

@misc{https://doi.org/10.48550/arxiv.2311.15127,
  doi = {10.48550/ARXIV.2311.15127},
  url = {https://arxiv.org/abs/2311.15127},
  author = {Blattmann, Andreas and Dockhorn, Tim and Kulal, Sumith and Mendelevitch, Daniel and Kilian, Maciej and Lorenz, Dominik and Levi, Yam and English, Zion and Voleti, Vikram and Letts, Adam and Jampani, Varun and Rombach, Robin},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets},
  publisher = {arXiv},
  year = {2023},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Phased DMD: Few-step Distribution Matching Distillation via Score Matching within Subintervals

@misc{https://doi.org/10.48550/arxiv.2510.27684,
  doi = {10.48550/ARXIV.2510.27684},
  url = {https://arxiv.org/abs/2510.27684},
  author = {Fan, Xiangyu and Qiu, Zesong and Wu, Zhuguanyu and Wang, Fanzhou and Lin, Zhiqian and Ren, Tianxiang and Lin, Dahua and Gong, Ruihao and Yang, Lei},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Phased DMD: Few-step Distribution Matching Distillation via Score Matching within Subintervals},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Motion Consistency Model: Accelerating Video Diffusion with Disentangled Motion-Appearance Distillation

@misc{https://doi.org/10.48550/arxiv.2406.06890,
  doi = {10.48550/ARXIV.2406.06890},
  url = {https://arxiv.org/abs/2406.06890},
  author = {Zhai, Yuanhao and Lin, Kevin and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Chung-Ching and Doermann, David and Yuan, Junsong and Wang, Lijuan},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Motion Consistency Model: Accelerating Video Diffusion with Disentangled Motion-Appearance Distillation},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

% Interactive Character Control with Auto-Regressive Motion Diffusion Models

@article{Shi_2024, title={Interactive Character Control with Auto-Regressive Motion Diffusion Models}, volume={43}, ISSN={1557-7368}, url={http://dx.doi.org/10.1145/3658140}, DOI={10.1145/3658140}, number={4}, journal={ACM Transactions on Graphics}, publisher={Association for Computing Machinery (ACM)}, author={Shi, Yi and Wang, Jingbo and Jiang, Xuekun and Lin, Bingkun and Dai, Bo and Peng, Xue Bin}, year={2024}, month=jul, pages={1–14} }

% Matten: Video Generation with Mamba-Attention

@misc{https://doi.org/10.48550/arxiv.2405.03025,
  doi = {10.48550/ARXIV.2405.03025},
  url = {https://arxiv.org/abs/2405.03025},
  author = {Gao, Yu and Huang, Jiancheng and Sun, Xiaopeng and Jie, Zequn and Zhong, Yujie and Ma, Lin},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Matten: Video Generation with Mamba-Attention},
  publisher = {arXiv},
  year = {2024},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Diffusion Adversarial Post-Training for One-Step Video Generation

@misc{https://doi.org/10.48550/arxiv.2501.08316,
  doi = {10.48550/ARXIV.2501.08316},
  url = {https://arxiv.org/abs/2501.08316},
  author = {Lin, Shanchuan and Xia, Xin and Ren, Yuxi and Yang, Ceyuan and Xiao, Xuefeng and Jiang, Lu},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Diffusion Adversarial Post-Training for One-Step Video Generation},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Efficient Diffusion Models for Vision: A Survey

@misc{https://doi.org/10.48550/arxiv.2210.09292,
  doi = {10.48550/ARXIV.2210.09292},
  url = {https://arxiv.org/abs/2210.09292},
  author = {Ulhaq, Anwaar and Akhtar, Naveed},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Efficient Diffusion Models for Vision: A Survey},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

% InstructVideo: Instructing Video Diffusion Models with Human Feedback

@inproceedings{Yuan_2024, title={InstructVideo: Instructing Video Diffusion Models with Human Feedback}, url={http://dx.doi.org/10.1109/CVPR52733.2024.00618}, DOI={10.1109/cvpr52733.2024.00618}, booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Yuan, Hangjie and Zhang, Shiwei and Wang, Xiang and Wei, Yujie and Feng, Tao and Pan, Yining and Zhang, Yingya and Liu, Ziwei and Albanie, Samuel and Ni, Dong}, year={2024}, month=jun, pages={6463–6474} }

% TR-DQ: Time-Rotation Diffusion Quantization

@misc{https://doi.org/10.48550/arxiv.2503.06564,
  doi = {10.48550/ARXIV.2503.06564},
  url = {https://arxiv.org/abs/2503.06564},
  author = {Shao, Yihua and Lin, Deyang and Zeng, Fanhu and Yan, Minxi and Zhang, Muyang and Chen, Siyu and Fan, Yuxuan and Yan, Ziyang and Wang, Haozhe and Guo, Jingcai and Wang, Yan and Qin, Haotong and Tang, Hao},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {TR-DQ: Time-Rotation Diffusion Quantization},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis

@misc{https://doi.org/10.48550/arxiv.2507.18569,
  doi = {10.48550/ARXIV.2507.18569},
  url = {https://arxiv.org/abs/2507.18569},
  author = {Lu, Yanzuo and Ren, Yuxi and Xia, Xin and Lin, Shanchuan and Wang, Xing and Xiao, Xuefeng and Ma, Andy J. and Xie, Xiaohua and Lai, Jian-Huang},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% DCM: Dual-Expert Consistency Model for Efficient and High-Quality Video Generation

@misc{https://doi.org/10.48550/arxiv.2506.03123,
  doi = {10.48550/ARXIV.2506.03123},
  url = {https://arxiv.org/abs/2506.03123},
  author = {Lv, Zhengyao and Si, Chenyang and Pan, Tianlin and Chen, Zhaoxi and Wong, Kwan-Yee K. and Qiao, Yu and Liu, Ziwei},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Dual-Expert Consistency Model for Efficient and High-Quality Video Generation},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Timestep-Aware Correction for Quantized Diffusion Models

@misc{https://doi.org/10.48550/arxiv.2407.03917,
  doi = {10.48550/ARXIV.2407.03917},
  url = {https://arxiv.org/abs/2407.03917},
  author = {Yao, Yuzhe and Tian, Feng and Chen, Jun and Lin, Haonan and Dai, Guang and Liu, Yong and Wang, Jingdong},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Timestep-Aware Correction for Quantized Diffusion Models},
  publisher = {arXiv},
  year = {2024},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal Generation and Cache Sharing

@misc{https://doi.org/10.48550/arxiv.2411.16375,
  doi = {10.48550/ARXIV.2411.16375},
  url = {https://arxiv.org/abs/2411.16375},
  author = {Gao, Kaifeng and Shi, Jiaxin and Zhang, Hanwang and Wang, Chunping and Xiao, Jun and Chen, Long},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal Generation and Cache Sharing},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

% Open-Sora: Democratizing Efficient Video Production for All

@misc{https://doi.org/10.48550/arxiv.2412.20404,
  doi = {10.48550/ARXIV.2412.20404},
  url = {https://arxiv.org/abs/2412.20404},
  author = {Zheng, Zangwei and Peng, Xiangyu and Yang, Tianji and Shen, Chenhui and Li, Shenggui and Liu, Hongxin and Zhou, Yukun and Li, Tianyi and You, Yang},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Open-Sora: Democratizing Efficient Video Production for All},
  publisher = {arXiv},
  year = {2024},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Video Diffusion Models

@misc{https://doi.org/10.48550/arxiv.2204.03458,
  doi = {10.48550/ARXIV.2204.03458},
  url = {https://arxiv.org/abs/2204.03458},
  author = {Ho, Jonathan and Salimans, Tim and Gritsenko, Alexey and Chan, William and Norouzi, Mohammad and Fleet, David J.},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Video Diffusion Models},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models

@misc{https://doi.org/10.48550/arxiv.2506.03099,
  doi = {10.48550/ARXIV.2506.03099},
  url = {https://arxiv.org/abs/2506.03099},
  author = {Low, Chetwin and Wang, Weimin},
  keywords = {Sound (cs.SD), Artificial Intelligence (cs.AI), Graphics (cs.GR), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Taming Consistency Distillation for Accelerated Human Image Animation

@misc{https://doi.org/10.48550/arxiv.2504.11143,
  doi = {10.48550/ARXIV.2504.11143},
  url = {https://arxiv.org/abs/2504.11143},
  author = {Wang, Xiang and Zhang, Shiwei and Yuan, Hangjie and Wei, Yujie and Zhang, Yingya and Gao, Changxin and Wang, Yuehuan and Sang, Nong},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Taming Consistency Distillation for Accelerated Human Image Animation},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% OS-DiffVSR: Towards One-step Latent Diffusion Model for High-detailed Real-world Video Super-Resolution

@misc{https://doi.org/10.48550/arxiv.2509.16507,
  doi = {10.48550/ARXIV.2509.16507},
  url = {https://arxiv.org/abs/2509.16507},
  author = {Li, Hanting and Tang, Huaao and Han, Jianhong and Zhou, Tianxiong and Cui, Jiulong and Xie, Haizhen and Chen, Yan and Hu, Jie},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {OS-DiffVSR: Towards One-step Latent Diffusion Model for High-detailed Real-world Video Super-Resolution},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Re-ttention: Ultra Sparse Visual Generation via Attention Statistical Reshape

@misc{https://doi.org/10.48550/arxiv.2505.22918,
  doi = {10.48550/ARXIV.2505.22918},
  url = {https://arxiv.org/abs/2505.22918},
  author = {Chen, Ruichen and Mills, Keith G. and Jiang, Liyao and Gao, Chao and Niu, Di},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Re-ttention: Ultra Sparse Visual Generation via Attention Statistical Reshape},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% From Slow Bidirectional to Fast Autoregressive Video Diffusion Models

@inproceedings{Yin_2025, title={From Slow Bidirectional to Fast Autoregressive Video Diffusion Models}, url={http://dx.doi.org/10.1109/CVPR52734.2025.02138}, DOI={10.1109/cvpr52734.2025.02138}, booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Yin, Tianwei and Zhang, Qiang and Zhang, Richard and Freeman, William T. and Durand, Frédo and Shechtman, Eli and Huang, Xun}, year={2025}, month=jun, pages={22963–22974} }

% Diffusion Models for Video Prediction and Infilling

@misc{https://doi.org/10.48550/arxiv.2206.07696,
  doi = {10.48550/ARXIV.2206.07696},
  url = {https://arxiv.org/abs/2206.07696},
  author = {Höppe, Tobias and Mehrjou, Arash and Bauer, Stefan and Nielsen, Didrik and Dittadi, Andrea},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Diffusion Models for Video Prediction and Infilling},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% From Slow Bidirectional to Fast Causal Video Generators

@misc{https://doi.org/10.48550/arxiv.2511.01266,
  doi = {10.48550/ARXIV.2511.01266},
  url = {https://arxiv.org/abs/2511.01266},
  author = {Shin, Joonghyuk and Li, Zhengqi and Zhang, Richard and Zhu, Jun-Yan and Park, Jaesik and Shechtman, Eli and Huang, Xun},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {MotionStream: Real-Time Video Generation with Interactive Motion Controls},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

% Taming Teacher Forcing for Masked Autoregressive Video Generation

@inproceedings{Zhou_2025, title={Taming Teacher Forcing for Masked Autoregressive Video Generation}, url={http://dx.doi.org/10.1109/CVPR52734.2025.00691}, DOI={10.1109/cvpr52734.2025.00691}, booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Zhou, Deyu and Sun, Quan and Peng, Yuang and Yan, Kun and Dong, Runpei and Wang, Duomin and Ge, Zheng and Duan, Nan and Zhang, Xiangyu}, year={2025}, month=jun, pages={7374–7384} }

% Real-Time Motion-Controllable Autoregressive Video Diffusion

@misc{https://doi.org/10.48550/arxiv.2510.08131,
  doi = {10.48550/ARXIV.2510.08131},
  url = {https://arxiv.org/abs/2510.08131},
  author = {Zhao, Kesen and Shi, Jiaxin and Zhu, Beier and Zhou, Junbao and Shen, Xiaolong and Zhou, Yuan and Sun, Qianru and Zhang, Hanwang},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Real-Time Motion-Controllable Autoregressive Video Diffusion},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

% VORTA: Efficient Video Diffusion via Routing Sparse Attention

@misc{https://doi.org/10.48550/arxiv.2505.18809,
  doi = {10.48550/ARXIV.2505.18809},
  url = {https://arxiv.org/abs/2505.18809},
  author = {Sun, Wenhao and Tu, Rong-Cheng and Ding, Yifu and Jin, Zhao and Liao, Jingyi and Liu, Shunyu and Tao, Dacheng},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {VORTA: Efficient Video Diffusion via Routing Sparse Attention},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% TokenFlow: Consistent Diffusion Features for Consistent Video Editing

@misc{https://doi.org/10.48550/arxiv.2307.10373,
  doi = {10.48550/ARXIV.2307.10373},
  url = {https://arxiv.org/abs/2307.10373},
  author = {Geyer, Michal and Bar-Tal, Omer and Bagon, Shai and Dekel, Tali},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {TokenFlow: Consistent Diffusion Features for Consistent Video Editing},
  publisher = {arXiv},
  year = {2023},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation

@inproceedings{Wu_2023, title={Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation}, url={http://dx.doi.org/10.1109/ICCV51070.2023.00701}, DOI={10.1109/iccv51070.2023.00701}, booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, publisher={IEEE}, author={Wu, Jay Zhangjie and Ge, Yixiao and Wang, Xintao and Lei, Stan Weixian and Gu, Yuchao and Shi, Yufei and Hsu, Wynne and Shan, Ying and Qie, Xiaohu and Shou, Mike Zheng}, year={2023}, month=oct, pages={7589–7599} }

% Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion

@misc{https://doi.org/10.48550/arxiv.2407.01392,
  doi = {10.48550/ARXIV.2407.01392},
  url = {https://arxiv.org/abs/2407.01392},
  author = {Chen, Boyuan and Monso, Diego Marti and Du, Yilun and Simchowitz, Max and Tedrake, Russ and Sitzmann, Vincent},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

% FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution

@misc{https://doi.org/10.48550/arxiv.2510.12747,
  doi = {10.48550/ARXIV.2510.12747},
  url = {https://arxiv.org/abs/2510.12747},
  author = {Zhuang, Junhao and Guo, Shi and Cai, Xin and Li, Xiaohui and Liu, Yihao and Yuan, Chun and Xue, Tianfan},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Less is More: CLIPBERT for Video-and-Language Learning via Sparse Sampling

@inproceedings{Lei_2021, title={Less is More: CLIPBERT for Video-and-Language Learning via Sparse Sampling}, url={http://dx.doi.org/10.1109/CVPR46437.2021.00725}, DOI={10.1109/cvpr46437.2021.00725}, booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Lei, Jie and Li, Linjie and Zhou, Luowei and Gan, Zhe and Berg, Tamara L. and Bansal, Mohit and Liu, Jingjing}, year={2021}, month=jun, pages={7327–7337} }

% MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation

@inproceedings{Ruan_2023, title={MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation}, url={http://dx.doi.org/10.1109/CVPR52729.2023.00985}, DOI={10.1109/cvpr52729.2023.00985}, booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Ruan, Ludan and Ma, Yiyang and Yang, Huan and He, Huiguo and Liu, Bei and Fu, Jianlong and Yuan, Nicholas Jing and Jin, Qin and Guo, Baining}, year={2023}, month=jun, pages={10219–10228} }

% Photorealistic Video Generation with Diffusion Models

@misc{https://doi.org/10.48550/arxiv.2312.06662,
  doi = {10.48550/ARXIV.2312.06662},
  url = {https://arxiv.org/abs/2312.06662},
  author = {Gupta, Agrim and Yu, Lijun and Sohn, Kihyuk and Gu, Xiuye and Hahn, Meera and Fei-Fei, Li and Essa, Irfan and Jiang, Lu and Lezama, José},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Photorealistic Video Generation with Diffusion Models},
  publisher = {arXiv},
  year = {2023},
  copyright = {Creative Commons Attribution 4.0 International}
}

% FreeLong: Training-Free Long Video Generation with SpectralBlend Temporal Attention

@misc{https://doi.org/10.48550/arxiv.2407.19918,
  doi = {10.48550/ARXIV.2407.19918},
  url = {https://arxiv.org/abs/2407.19918},
  author = {Lu, Yu and Liang, Yuanzhi and Zhu, Linchao and Yang, Yi},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {FreeLong: Training-Free Long Video Generation with SpectralBlend Temporal Attention},
  publisher = {arXiv},
  year = {2024},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching

@misc{https://doi.org/10.48550/arxiv.2507.02860,
  doi = {10.48550/ARXIV.2507.02860},
  url = {https://arxiv.org/abs/2507.02860},
  author = {Zhou, Xin and Liang, Dingkang and Chen, Kaijin and Feng, Tianrui and Chen, Xiwu and Lin, Hongkai and Ding, Yikang and Tan, Feiyang and Zhao, Hengshuang and Bai, Xiang},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers

@misc{https://doi.org/10.48550/arxiv.2506.03065,
  doi = {10.48550/ARXIV.2506.03065},
  url = {https://arxiv.org/abs/2506.03065},
  author = {Chen, Pengtao and Zeng, Xianfang and Zhao, Maosen and Ye, Peng and Shen, Mingzhu and Cheng, Wei and Yu, Gang and Chen, Tao},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% VideoLCM: Video Latent Consistency Model

@misc{https://doi.org/10.48550/arxiv.2312.09109,
  doi = {10.48550/ARXIV.2312.09109},
  url = {https://arxiv.org/abs/2312.09109},
  author = {Wang, Xiang and Zhang, Shiwei and Zhang, Han and Liu, Yu and Zhang, Yingya and Gao, Changxin and Sang, Nong},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {VideoLCM: Video Latent Consistency Model},
  publisher = {arXiv},
  year = {2023},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% DOVE: Efficient One-Step Diffusion Model for Real-World Video Super-Resolution

@misc{https://doi.org/10.48550/arxiv.2505.16239,
  doi = {10.48550/ARXIV.2505.16239},
  url = {https://arxiv.org/abs/2505.16239},
  author = {Chen, Zheng and Zou, Zichen and Zhang, Kewei and Su, Xiongfei and Yuan, Xin and Guo, Yong and Zhang, Yulun},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {DOVE: Efficient One-Step Diffusion Model for Real-World Video Super-Resolution},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% MPQ-DM: Mixed Precision Quantization for Extremely Low Bit Diffusion Models

@misc{https://doi.org/10.48550/arxiv.2412.11549,
  doi = {10.48550/ARXIV.2412.11549},
  url = {https://arxiv.org/abs/2412.11549},
  author = {Feng, Weilun and Qin, Haotong and Yang, Chuanguang and An, Zhulin and Huang, Libo and Diao, Boyu and Wang, Fei and Tao, Renshuai and Xu, Yongjun and Magno, Michele},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {MPQ-DM: Mixed Precision Quantization for Extremely Low Bit Diffusion Models},
  publisher = {arXiv},
  year = {2024},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Imagen Video: High Definition Video Generation with Diffusion Models

@misc{https://doi.org/10.48550/arxiv.2210.02303,
  doi = {10.48550/ARXIV.2210.02303},
  url = {https://arxiv.org/abs/2210.02303},
  author = {Ho, Jonathan and Chan, William and Saharia, Chitwan and Whang, Jay and Gao, Ruiqi and Gritsenko, Alexey and Kingma, Diederik P. and Poole, Ben and Norouzi, Mohammad and Fleet, David J. and Salimans, Tim},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Imagen Video: High Definition Video Generation with Diffusion Models},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% FIFO-Diffusion: Generating Infinite Videos from Text without Training

@article{https://doi.org/10.48550/arxiv.2405.11473,
  doi = {10.48550/ARXIV.2405.11473},
  url = {https://arxiv.org/abs/2405.11473},
  author = {Kim, Jihwan and Kang, Junoh and Choi, Jinyoung and Han, Bohyung},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {FIFO-Diffusion: Generating Infinite Videos from Text without Training},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

% Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling

@inproceedings{Shi_2024, series={SIGGRAPH ’24}, title={Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling}, url={http://dx.doi.org/10.1145/3641519.3657497}, DOI={10.1145/3641519.3657497}, booktitle={Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers}, publisher={ACM}, author={Shi, Xiaoyu and Huang, Zhaoyang and Wang, Fu-Yun and Bian, Weikang and Li, Dasong and Zhang, Yi and Zhang, Manyuan and Cheung, Ka Chun and See, Simon and Qin, Hongwei and Dai, Jifeng and Li, Hongsheng}, year={2024}, month=jul, pages={1–11}, collection={SIGGRAPH ’24} }

% Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation

@misc{https://doi.org/10.48550/arxiv.2510.02617,
  doi = {10.48550/ARXIV.2510.02617},
  url = {https://arxiv.org/abs/2510.02617},
  author = {Lu, Beijia and Chen, Ziyi and Xiao, Jing and Zhu, Jun-Yan},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Efficient Diffusion Models: A Comprehensive Survey From Principles to Practices

@article{Ma_2025, title={Efficient Diffusion Models: A Comprehensive Survey From Principles to Practices}, volume={47}, ISSN={1939-3539}, url={http://dx.doi.org/10.1109/TPAMI.2025.3569700}, DOI={10.1109/tpami.2025.3569700}, number={9}, journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, publisher={Institute of Electrical and Electronics Engineers (IEEE)}, author={Ma, Zhiyuan and Zhang, Yuzhu and Jia, Guoli and Zhao, Liangliang and Ma, Yichao and Ma, Mingjie and Liu, Gaofeng and Zhang, Kaiyan and Ding, Ning and Li, Jianjun and Zhou, Bowen}, year={2025}, month=sep, pages={7506–7525} }

% MotionAura: Generating High-Quality and Motion Consistent Videos using Discrete Diffusion

@misc{https://doi.org/10.48550/arxiv.2410.07659,
  doi = {10.48550/ARXIV.2410.07659},
  url = {https://arxiv.org/abs/2410.07659},
  author = {Susladkar, Onkar and Gupta, Jishu Sen and Sehgal, Chirag and Mittal, Sparsh and Singhal, Rekha},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {MotionAura: Generating High-Quality and Motion Consistent Videos using Discrete Diffusion},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

% Survey of Video Diffusion Models: Foundations, Implementations, and Applications

@misc{https://doi.org/10.48550/arxiv.2504.16081,
  doi = {10.48550/ARXIV.2504.16081},
  url = {https://arxiv.org/abs/2504.16081},
  author = {Wang, Yimu and Liu, Xuye and Pang, Wei and Ma, Li and Yuan, Shuai and Debevec, Paul and Yu, Ning},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Survey of Video Diffusion Models: Foundations, Implementations, and Applications},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% ProReflow: Progressive Reflow with Decomposed Velocity

@inproceedings{Ke_2025, title={ProReflow: Progressive Reflow with Decomposed Velocity}, url={http://dx.doi.org/10.1109/CVPR52734.2025.02610}, DOI={10.1109/cvpr52734.2025.02610}, booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Ke, Lei and Xu, Haohang and Ning, Xuefei and Li, Yu and Li, Jiajun and Li, Haoling and Lin, Yuxuan and Jiang, Dongsheng and Yang, Yujiu and Zhang, Linfeng}, year={2025}, month=jun, pages={28029–28038} }

% Uni-Instruct: One-step Diffusion Model through Unified Diffusion Divergence Instruction

@misc{https://doi.org/10.48550/arxiv.2505.20755,
  doi = {10.48550/ARXIV.2505.20755},
  url = {https://arxiv.org/abs/2505.20755},
  author = {Wang, Yifei and Bai, Weimin and Zhang, Colin and Zhang, Debing and Luo, Weijian and Sun, He},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Uni-Instruct: One-step Diffusion Model through Unified Diffusion Divergence Instruction},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% DilateQuant: Accurate and Efficient Diffusion Quantization via Weight Dilation

@misc{https://doi.org/10.48550/arxiv.2409.14307,
  doi = {10.48550/ARXIV.2409.14307},
  url = {https://arxiv.org/abs/2409.14307},
  author = {Liu, Xuewen and Li, Zhikai and Jiang, Minhao and Chen, Mengjuan and Li, Jianquan and Gu, Qingyi},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {DilateQuant: Accurate and Efficient Diffusion Quantization via Weight Dilation},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Radial Attention: O(n log n) Sparse Attention with Energy Decay for Long Video Generation

@misc{https://doi.org/10.48550/arxiv.2506.19852,
  doi = {10.48550/ARXIV.2506.19852},
  url = {https://arxiv.org/abs/2506.19852},
  author = {Li, Xingyang and Li, Muyang and Cai, Tianle and Xi, Haocheng and Yang, Shuo and Lin, Yujun and Zhang, Lvmin and Yang, Songlin and Hu, Jinbo and Peng, Kelly and Agrawala, Maneesh and Stoica, Ion and Keutzer, Kurt and Han, Song},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Radial Attention: $O(n\log n)$ Sparse Attention with Energy Decay for Long Video Generation},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric Reduction and Restoration

@misc{https://doi.org/10.48550/arxiv.2412.11706,
  doi = {10.48550/ARXIV.2412.11706},
  url = {https://arxiv.org/abs/2412.11706},
  author = {Sun, Wenhao and Tu, Rong-Cheng and Liao, Jingyi and Jin, Zhao and Tao, Dacheng},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric Reduction and Restoration},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

% VMC: Video Motion Customization Using Temporal Attention Adaption for Text-to-Video Diffusion Models

@inproceedings{Jeong_2024, title={VMC: Video Motion Customization Using Temporal Attention Adaption for Text-to-Video Diffusion Models}, url={http://dx.doi.org/10.1109/CVPR52733.2024.00880}, DOI={10.1109/cvpr52733.2024.00880}, booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Jeong, Hyeonho and Park, Geon Yeong and Ye, Jong Chul}, year={2024}, month=jun, pages={9212–9221} }

% OSV: One Step is Enough for High-Quality Image to Video Generation

@inproceedings{Mao_2025, title={OSV: One Step is Enough for High-Quality Image to Video Generation}, url={http://dx.doi.org/10.1109/CVPR52734.2025.01174}, DOI={10.1109/cvpr52734.2025.01174}, booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Mao, Xiaofeng and Jiang, Zhengkai and Wang, Fu-yun and Zhang, Jiangning and Chen, Hao and Chi, Mingmin and Wang, Yabiao and Luo, Wenhan}, year={2025}, month=jun, pages={12585–12594} }

% VMoBA: Mixture-of-Block Attention for Video Diffusion Models

@misc{https://doi.org/10.48550/arxiv.2506.23858,
  doi = {10.48550/ARXIV.2506.23858},
  url = {https://arxiv.org/abs/2506.23858},
  author = {Wu, Jianzong and Hou, Liang and Yang, Haotian and Tao, Xin and Tian, Ye and Wan, Pengfei and Zhang, Di and Tong, Yunhai},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {VMoBA: Mixture-of-Block Attention for Video Diffusion Models},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% SwinBERT: End-to-End Transformers with Sparse Attention for Video Captioning

@inproceedings{Lin_2022, title={SwinBERT: End-to-End Transformers with Sparse Attention for Video Captioning}, url={http://dx.doi.org/10.1109/CVPR52688.2022.01742}, DOI={10.1109/cvpr52688.2022.01742}, booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Lin, Kevin and Li, Linjie and Lin, Chung-Ching and Ahmed, Faisal and Gan, Zhe and Liu, Zicheng and Lu, Yumao and Wang, Lijuan}, year={2022}, month=jun, pages={17928–17937} }

% Flexible Diffusion Modeling of Long Videos

@misc{https://doi.org/10.48550/arxiv.2205.11495,
  doi = {10.48550/ARXIV.2205.11495},
  url = {https://arxiv.org/abs/2205.11495},
  author = {Harvey, William and Naderiparizi, Saeid and Masrani, Vaden and Weilbach, Christian and Wood, Frank},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Flexible Diffusion Modeling of Long Videos},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% SViTT: Temporal Learning of Sparse Video-Text Transformers

@inproceedings{Li_2023, title={SViTT: Temporal Learning of Sparse Video-Text Transformers}, url={http://dx.doi.org/10.1109/CVPR52729.2023.01814}, DOI={10.1109/cvpr52729.2023.01814}, booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Li, Yi and Min, Kyle and Tripathi, Subarna and Vasconcelos, Nuno}, year={2023}, month=jun, pages={18919–18929} }

% Fast and Memory-Efficient Video Diffusion Using Streamlined Inference

@misc{https://doi.org/10.48550/arxiv.2411.01171,
  doi = {10.48550/ARXIV.2411.01171},
  url = {https://arxiv.org/abs/2411.01171},
  author = {Zhan, Zheng and Wu, Yushu and Gong, Yifan and Meng, Zichong and Kong, Zhenglun and Yang, Changdi and Yuan, Geng and Zhao, Pu and Niu, Wei and Wang, Yanzhi},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Fast and Memory-Efficient Video Diffusion Using Streamlined Inference},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Autoregressive Video Generation without Vector Quantization

@misc{https://doi.org/10.48550/arxiv.2412.14169,
  doi = {10.48550/ARXIV.2412.14169},
  url = {https://arxiv.org/abs/2412.14169},
  author = {Deng, Haoge and Pan, Ting and Diao, Haiwen and Luo, Zhengxiong and Cui, Yufeng and Lu, Huchuan and Shan, Shiguang and Qi, Yonggang and Wang, Xinlong},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Autoregressive Video Generation without Vector Quantization},
  publisher = {arXiv},
  year = {2024},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Hardware-Friendly Static Quantization Method for Video Diffusion Transformers

@inproceedings{Yi_2025, title={Hardware-Friendly Static Quantization Method for Video Diffusion Transformers}, url={http://dx.doi.org/10.1109/MIPR67560.2025.00042}, DOI={10.1109/mipr67560.2025.00042}, booktitle={2025 IEEE 8th International Conference on Multimedia Information Processing and Retrieval (MIPR)}, publisher={IEEE}, author={Yi, Sanghyun and Liu, Qingfeng and El-Khamy, Mostafa}, year={2025}, month=aug, pages={219–225} }

% Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning

@inproceedings{Piergiovanni_2023, title={Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning}, url={http://dx.doi.org/10.1109/CVPR52729.2023.00220}, DOI={10.1109/cvpr52729.2023.00220}, booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Piergiovanni, AJ and Kuo, Weicheng and Angelova, Anelia}, year={2023}, month=jun, pages={2214–2224} }

% Understanding Attention Mechanism in Video Diffusion Models

@misc{https://doi.org/10.48550/arxiv.2504.12027,
  doi = {10.48550/ARXIV.2504.12027},
  url = {https://arxiv.org/abs/2504.12027},
  author = {Liu, Bingyan and Wang, Chengyu and Su, Tongtong and Ten, Huan and Huang, Jun and Guo, Kailing and Jia, Kui},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Understanding Attention Mechanism in Video Diffusion Models},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Imagine Flash: Accelerating Emu Diffusion Models with Backward Distillation

@misc{https://doi.org/10.48550/arxiv.2405.05224,
  doi = {10.48550/ARXIV.2405.05224},
  url = {https://arxiv.org/abs/2405.05224},
  author = {Kohler, Jonas and Pumarola, Albert and Schönfeld, Edgar and Sanakoyeu, Artsiom and Sumbaly, Roshan and Vajda, Peter and Thabet, Ali},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Imagine Flash: Accelerating Emu Diffusion Models with Backward Distillation},
  publisher = {arXiv},
  year = {2024},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment

@misc{https://doi.org/10.48550/arxiv.2508.06082,
  doi = {10.48550/ARXIV.2508.06082},
  url = {https://arxiv.org/abs/2508.06082},
  author = {Sun, Yanxiao and Wu, Jiafu and Cao, Yun and Xu, Chengming and Wang, Yabiao and Cao, Weijian and Luo, Donghao and Wang, Chengjie and Fu, Yanwei},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Latent Video Diffusion Models for High-Fidelity Video Generation with Arbitrary Lengths

@misc{https://doi.org/10.48550/arxiv.2211.13221,
  doi = {10.48550/ARXIV.2211.13221},
  url = {https://arxiv.org/abs/2211.13221},
  author = {He, Yingqing and Yang, Tianyu and Zhang, Yong and Shan, Ying and Chen, Qifeng},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Latent Video Diffusion Models for High-Fidelity Long Video Generation with Arbitrary Lengths},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{karnewar2025neodragon,
  title={Neodragon: Mobile Video Generation using Diffusion Transformer},
  author={Karnewar, Animesh and Korzhenkov, Denis and Lelekas, Ioannis and Karjauv, Adil and Fathima, Noor and Xiong, Hanwen and Vaidyanathan, Vancheeswaran and Zeng, Will and Esteves, Rafael and Singhal, Tushar and others},
  journal={arXiv preprint arXiv:2511.06055},
  year={2025}
}

@article{feng2025s,
  title={S $\^{} 2$ Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation},
  author={Feng, Weilun and Qin, Haotong and Yang, Chuanguang and Li, Xiangqi and Yang, Han and Li, Yuqi and An, Zhulin and Huang, Libo and Magno, Michele and Xu, Yongjun},
  journal={arXiv preprint arXiv:2508.04016},
  year={2025}
}

@article{shmilovich2025liteattention,
  title={LiteAttention: A Temporal Sparse Attention for Diffusion Transformers},
  author={Shmilovich, Dor and Wu, Tony and Dahan, Aviad and Domb, Yuval},
  journal={arXiv preprint arXiv:2511.11062},
  year={2025}
}

@article{wu2025usv,
  title={USV: Unified Sparsification for Accelerating Video Diffusion Models},
  author={Wu, Xinjian and Wang, Hongmei and Zhou, Yuan and Lu, Qinglin},
  journal={arXiv preprint arXiv:2512.05754},
  year={2025}
}

@article{gu2025blade,
  title={BLADE: Block-Sparse Attention Meets Step Distillation for Efficient Video Generation},
  author={Gu, Youping and Li, Xiaolong and Hu, Yuhao and Chen, Minqi and Zhuang, Bohan},
  journal={arXiv preprint arXiv:2508.10774},
  year={2025}
}

@article{cheng2025pose,
  title={Pose: Phased one-step adversarial equilibrium for video diffusion models},
  author={Cheng, Jiaxiang and Ma, Bing and Ren, Xuhua and Jin, Hongyi and Yu, Kai and Zhang, Peng and Li, Wenyue and Zhou, Yuan and Zheng, Tianxiang and Lu, Qinglin},
  journal={arXiv e-prints},
  pages={arXiv--2508},
  year={2025}
}

@article{lv2025dcm,
  title={DCM: Dual-Expert Consistency Model for Efficient and High-Quality Video Generation},
  author={Lv, Zhengyao and Si, Chenyang and Pan, Tianlin and Chen, Zhaoxi and Wong, Kwan-Yee K and Qiao, Yu and Liu, Ziwei},
  journal={arXiv preprint arXiv:2506.03123},
  year={2025}
}

@article{zhang2025mobilei2v,
  title={MobileI2V: Fast and High-Resolution Image-to-Video on Mobile Devices},
  author={Zhang, Shuai and Tang, Bao and Yu, Siyuan and Zhu, Yueting and Yao, Jingfeng and Zou, Ya and Yuan, Shanglin and Yu, Li and Liu, Wenyu and Wang, Xinggang},
  journal={arXiv preprint arXiv:2511.21475},
  year={2025}
}

@article{lu2025reward,
  title={Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation},
  author={Lu, Yunhong and Zeng, Yanhong and Li, Haobo and Ouyang, Hao and Wang, Qiuyu and Cheng, Ka Leong and Zhu, Jiapeng and Cao, Hengyuan and Zhang, Zhipeng and Zhu, Xing and others},
  journal={arXiv preprint arXiv:2512.04678},
  year={2025}
}

% Pyramidal Flow Matching for Efficient Video Generative Modeling

@misc{https://doi.org/10.48550/arxiv.2410.05954,
  doi = {10.48550/ARXIV.2410.05954},
  url = {https://arxiv.org/abs/2410.05954},
  author = {Jin, Yang and Sun, Zhicheng and Li, Ningyuan and Xu, Kun and Xu, Kun and Jiang, Hao and Zhuang, Nan and Huang, Quzhe and Song, Yang and Mu, Yadong and Lin, Zhouchen},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Pyramidal Flow Matching for Efficient Video Generative Modeling},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

% One Step Diffusion via Shortcut Models

@misc{https://doi.org/10.48550/arxiv.2410.12557,
  doi = {10.48550/ARXIV.2410.12557},
  url = {https://arxiv.org/abs/2410.12557},
  author = {Frans, Kevin and Hafner, Danijar and Levine, Sergey and Abbeel, Pieter},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {One Step Diffusion via Shortcut Models},
  publisher = {arXiv},
  year = {2024},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Latent-Shift: Latent Diffusion with Temporal Shift for Efficient Text-to-Video Generation

@misc{https://doi.org/10.48550/arxiv.2304.08477,
  doi = {10.48550/ARXIV.2304.08477},
  url = {https://arxiv.org/abs/2304.08477},
  author = {An, Jie and Zhang, Songyang and Yang, Harry and Gupta, Sonal and Huang, Jia-Bin and Luo, Jiebo and Yin, Xi},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Latent-Shift: Latent Diffusion with Temporal Shift for Efficient Text-to-Video Generation},
  publisher = {arXiv},
  year = {2023},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% UniCP: A Unified Caching and Pruning Framework for Efficient Video Generation

@inproceedings{Sun_2025, series={MMAsia ’25}, title={UniCP: A Unified Caching and Pruning Framework for Efficient Video Generation}, url={http://dx.doi.org/10.1145/3743093.3770981}, DOI={10.1145/3743093.3770981}, booktitle={Proceedings of the 7th ACM International Conference on Multimedia in Asia}, publisher={ACM}, author={Sun, Wenzhang and Hou, Qirui and Di, Donglin and Yang, Jiahui and Ma, Yongjia and Cui, Jianxun}, year={2025}, month=dec, pages={1–7}, collection={MMAsia ’25} }

% PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models

@misc{https://doi.org/10.48550/arxiv.2506.16054,
  doi = {10.48550/ARXIV.2506.16054},
  url = {https://arxiv.org/abs/2506.16054},
  author = {Zhao, Tianchen and Hong, Ke and Yang, Xinhao and Xiao, Xuefeng and Li, Huixia and Ling, Feng and Xie, Ruiqi and Chen, Siqi and Zhu, Hongyu and Zhang, Yichong and Wang, Yu},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Graphics (cs.GR), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% MoVideo: Motion-Aware Video Generation with Diffusion Models

@misc{https://doi.org/10.48550/arxiv.2311.11325,
  doi = {10.48550/ARXIV.2311.11325},
  url = {https://arxiv.org/abs/2311.11325},
  author = {Liang, Jingyun and Fan, Yuchen and Zhang, Kai and Timofte, Radu and Van Gool, Luc and Ranjan, Rakesh},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Image and Video Processing (eess.IV), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  title = {MoVideo: Motion-Aware Video Generation with Diffusion Models},
  publisher = {arXiv},
  year = {2023},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

% QuantSparse: Comprehensively Compressing Video Diffusion Transformer with Model Quantization and Attention Sparsification

@misc{https://doi.org/10.48550/arxiv.2509.23681,
  doi = {10.48550/ARXIV.2509.23681},
  url = {https://arxiv.org/abs/2509.23681},
  author = {Feng, Weilun and Yang, Chuanguang and Qin, Haotong and Wu, Mingqiang and Li, Yuqi and Li, Xiangqi and An, Zhulin and Huang, Libo and Zhang, Yulun and Magno, Michele and Xu, Yongjun},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {QuantSparse: Comprehensively Compressing Video Diffusion Transformer with Model Quantization and Attention Sparsification},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% DGQ: Distribution-Aware Group Quantization for Text-to-Image Diffusion Models

@misc{https://doi.org/10.48550/arxiv.2501.04304,
  doi = {10.48550/ARXIV.2501.04304},
  url = {https://arxiv.org/abs/2501.04304},
  author = {Ryu, Hyogon and Park, NaHyeon and Shim, Hyunjung},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {DGQ: Distribution-Aware Group Quantization for Text-to-Image Diffusion Models},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Sparse VideoGen: Accelerating Video Diffusion Transformers with Spatial-Temporal Sparsity

@misc{https://doi.org/10.48550/arxiv.2502.01776,
  doi = {10.48550/ARXIV.2502.01776},
  url = {https://arxiv.org/abs/2502.01776},
  author = {Xi, Haocheng and Yang, Shuo and Zhao, Yilong and Xu, Chenfeng and Li, Muyang and Li, Xiuyu and Lin, Yujun and Cai, Han and Zhang, Jintao and Li, Dacheng and Chen, Jianfei and Stoica, Ion and Keutzer, Kurt and Han, Song},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Sparse VideoGen: Accelerating Video Diffusion Transformers with Spatial-Temporal Sparsity},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition

@misc{https://doi.org/10.48550/arxiv.2403.14148,
  doi = {10.48550/ARXIV.2403.14148},
  url = {https://arxiv.org/abs/2403.14148},
  author = {Yu, Sihyun and Nie, Weili and Huang, De-An and Li, Boyi and Shin, Jinwoo and Anandkumar, Anima},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition},
  publisher = {arXiv},
  year = {2024},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models

@inproceedings{Huang_2024, title={TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models}, url={http://dx.doi.org/10.1109/CVPR52733.2024.00703}, DOI={10.1109/cvpr52733.2024.00703}, booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Huang, Yushi and Gong, Ruihao and Liu, Jing and Chen, Tianlong and Liu, Xianglong}, year={2024}, month=jun, pages={7362–7371} }

% LTX-Video: Realtime Video Latent Diffusion

@misc{https://doi.org/10.48550/arxiv.2501.00103,
  doi = {10.48550/ARXIV.2501.00103},
  url = {https://arxiv.org/abs/2501.00103},
  author = {HaCohen, Yoav and Chiprut, Nisan and Brazowski, Benny and Shalem, Daniel and Moshe, Dudu and Richardson, Eitan and Levin, Eran and Shiran, Guy and Zabari, Nir and Gordon, Ori and Panet, Poriya and Weissbuch, Sapir and Kulikov, Victor and Bitterman, Yaki and Melumian, Zeev and Bibi, Ofir},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {LTX-Video: Realtime Video Latent Diffusion},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% DiffCollage: Parallel Generation of Large Content with Diffusion Models

@inproceedings{Zhang_2023, title={DiffCollage: Parallel Generation of Large Content with Diffusion Models}, url={http://dx.doi.org/10.1109/CVPR52729.2023.00982}, DOI={10.1109/cvpr52729.2023.00982}, booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Zhang, Qinsheng and Song, Jiaming and Huang, Xun and Chen, Yongxin and Liu, Ming-Yu}, year={2023}, month=jun, pages={10188–10198} }

% DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving

@misc{https://doi.org/10.48550/arxiv.2309.09777,
  doi = {10.48550/ARXIV.2309.09777},
  url = {https://arxiv.org/abs/2309.09777},
  author = {Wang, Xiaofeng and Zhu, Zheng and Huang, Guan and Chen, Xinze and Zhu, Jiagang and Lu, Jiwen},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving},
  publisher = {arXiv},
  year = {2023},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Post-training Quantization with Progressive Calibration and Activation Relaxing for Text-to-Image Diffusion Models

@misc{https://doi.org/10.48550/arxiv.2311.06322,
  doi = {10.48550/ARXIV.2311.06322},
  url = {https://arxiv.org/abs/2311.06322},
  author = {Tang, Siao and Wang, Xin and Chen, Hong and Guan, Chaoyu and Wu, Zewen and Tang, Yansong and Zhu, Wenwu},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Post-training Quantization for Text-to-Image Diffusion Models with Progressive Calibration and Activation Relaxing},
  publisher = {arXiv},
  year = {2023},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation

@article{Zhang_2024, title={Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation}, volume={133}, ISSN={1573-1405}, url={http://dx.doi.org/10.1007/s11263-024-02271-9}, DOI={10.1007/s11263-024-02271-9}, number={4}, journal={International Journal of Computer Vision}, publisher={Springer Science and Business Media LLC}, author={Zhang, David Junhao and Wu, Jay Zhangjie and Liu, Jia-Wei and Zhao, Rui and Ran, Lingmin and Gu, Yuchao and Gao, Difei and Shou, Mike Zheng}, year={2024}, month=oct, pages={1879–1893} }

% SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models

@misc{https://doi.org/10.48550/arxiv.2507.14811,
  doi = {10.48550/ARXIV.2507.14811},
  url = {https://arxiv.org/abs/2507.14811},
  author = {Zhang, Jiaji and Sun, Ruichao and Zhao, Hailiang and Wu, Jiaju and Chen, Peng and Li, Hao and Liu, Yuying and Chow, Kingsum and Xiong, Gang and Deng, Shuiguang},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models

@inproceedings{Blattmann_2023, title={Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models}, url={http://dx.doi.org/10.1109/CVPR52729.2023.02161}, DOI={10.1109/cvpr52729.2023.02161}, booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Blattmann, Andreas and Rombach, Robin and Ling, Huan and Dockhorn, Tim and Kim, Seung Wook and Fidler, Sanja and Kreis, Karsten}, year={2023}, month=jun, pages={22563–22575} }

% Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation

@misc{https://doi.org/10.48550/arxiv.2508.12969,
  doi = {10.48550/ARXIV.2508.12969},
  url = {https://arxiv.org/abs/2508.12969},
  author = {Li, Qirui and Zheng, Guangcong and Zhao, Qi and Li, Jie and Dong, Bin and Yao, Yiwu and Li, Xi},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% FlashVideo: Flowing Fidelity to Detail for Efficient High-Resolution Video Generation

@misc{https://doi.org/10.48550/arxiv.2502.05179,
  doi = {10.48550/ARXIV.2502.05179},
  url = {https://arxiv.org/abs/2502.05179},
  author = {Zhang, Shilong and Li, Wenbo and Chen, Shoufa and Ge, Chongjian and Sun, Peize and Zhang, Yida and Jiang, Yi and Yuan, Zehuan and Peng, Binyue and Luo, Ping},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {FlashVideo: Flowing Fidelity to Detail for Efficient High-Resolution Video Generation},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Q-DiT: Accurate Post-Training Quantization for Diffusion Transformers

@inproceedings{Chen_2025, title={Q-DiT: Accurate Post-Training Quantization for Diffusion Transformers}, url={http://dx.doi.org/10.1109/CVPR52734.2025.02636}, DOI={10.1109/cvpr52734.2025.02636}, booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Chen, Lei and Meng, Yuan and Tang, Chen and Ma, Xinzhu and Jiang, Jingyan and Wang, Xin and Wang, Zhi and Zhu, Wenwu}, year={2025}, month=jun, pages={28306–28315} }

% Bidirectional Sparse Attention for Faster Video Diffusion Training

@misc{https://doi.org/10.48550/arxiv.2509.01085,
  doi = {10.48550/ARXIV.2509.01085},
  url = {https://arxiv.org/abs/2509.01085},
  author = {Zhan, Chenlu and Li, Wen and Shen, Chuyu and Zhang, Jun and Wu, Suhui and Zhang, Hao},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Bidirectional Sparse Attention for Faster Video Diffusion Training},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% FQ4DM: Full Quantization for Diffusion Model

@inproceedings{Wang_2024, title={FQ4DM: Full Quantization for Diffusion Model}, url={http://dx.doi.org/10.1109/MLSP58920.2024.10734817}, DOI={10.1109/mlsp58920.2024.10734817}, booktitle={2024 IEEE 34th International Workshop on Machine Learning for Signal Processing (MLSP)}, publisher={IEEE}, author={Wang, Chieh-En and Tai, Yu-Shan and Wu, An-Yeu}, year={2024}, month=sep, pages={1–6} }

% Analysis of Attention in Video Diffusion Transformers

@misc{https://doi.org/10.48550/arxiv.2504.10317,
  doi = {10.48550/ARXIV.2504.10317},
  url = {https://arxiv.org/abs/2504.10317},
  author = {Wen, Yuxin and Wu, Jim and Jain, Ajay and Goldstein, Tom and Panda, Ashwinee},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Analysis of Attention in Video Diffusion Transformers},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% LaVie: High-Quality Video Generation with Cascaded Latent Diffusion Models

@article{Wang_2024, title={LaVie: High-Quality Video Generation with Cascaded Latent Diffusion Models}, volume={133}, ISSN={1573-1405}, url={http://dx.doi.org/10.1007/s11263-024-02295-1}, DOI={10.1007/s11263-024-02295-1}, number={5}, journal={International Journal of Computer Vision}, publisher={Springer Science and Business Media LLC}, author={Wang, Yaohui and Chen, Xinyuan and Ma, Xin and Zhou, Shangchen and Huang, Ziqi and Wang, Yi and Yang, Ceyuan and He, Yinan and Yu, Jiashuo and Yang, Peiqing and Guo, Yuwei and Wu, Tianxing and Si, Chenyang and Jiang, Yuming and Chen, Cunjian and Loy, Chen Change and Dai, Bo and Lin, Dahua and Qiao, Yu and Liu, Ziwei}, year={2024}, month=dec, pages={3059–3078} }

% ViD-GPT: Introducing GPT-style Autoregressive Generation in Video Diffusion Models

@misc{https://doi.org/10.48550/arxiv.2406.10981,
  doi = {10.48550/ARXIV.2406.10981},
  url = {https://arxiv.org/abs/2406.10981},
  author = {Gao, Kaifeng and Shi, Jiaxin and Zhang, Hanwang and Wang, Chunping and Xiao, Jun},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {ViD-GPT: Introducing GPT-style Autoregressive Generation in Video Diffusion Models},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

% T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through Data, Reward, and Conditional Guidance Design

@misc{https://doi.org/10.48550/arxiv.2410.05677,
  doi = {10.48550/ARXIV.2410.05677},
  url = {https://arxiv.org/abs/2410.05677},
  author = {Li, Jiachen and Long, Qian and Zheng, Jian and Gao, Xiaofeng and Piramuthu, Robinson and Chen, Wenhu and Wang, William Yang},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through Data, Reward, and Conditional Guidance Design},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

% SF-V: Single Forward Video Generation Model

@misc{https://doi.org/10.48550/arxiv.2406.04324,
  doi = {10.48550/ARXIV.2406.04324},
  url = {https://arxiv.org/abs/2406.04324},
  author = {Zhang, Zhixing and Li, Yanyu and Wu, Yushu and Xu, Yanwu and Kag, Anil and Skorokhodov, Ivan and Menapace, Willi and Siarohin, Aliaksandr and Cao, Junli and Metaxas, Dimitris and Tulyakov, Sergey and Ren, Jian},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Image and Video Processing (eess.IV), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  title = {SF-V: Single Forward Video Generation Model},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

% Real-Time Video Generation with Pyramid Attention Broadcast

@misc{https://doi.org/10.48550/arxiv.2408.12588,
  doi = {10.48550/ARXIV.2408.12588},
  url = {https://arxiv.org/abs/2408.12588},
  author = {Zhao, Xuanlei and Jin, Xiaolong and Wang, Kai and You, Yang},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Distributed, Parallel, and Cluster Computing (cs.DC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Real-Time Video Generation with Pyramid Attention Broadcast},
  publisher = {arXiv},
  year = {2024},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Diffusion Models: A Comprehensive Survey of Methods and Applications

@article{Yang_2023, title={Diffusion Models: A Comprehensive Survey of Methods and Applications}, volume={56}, ISSN={1557-7341}, url={http://dx.doi.org/10.1145/3626235}, DOI={10.1145/3626235}, number={4}, journal={ACM Computing Surveys}, publisher={Association for Computing Machinery (ACM)}, author={Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan}, year={2023}, month=nov, pages={1–39} }

% Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation

@inproceedings{Stypu_kowski_2024, title={Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation}, url={http://dx.doi.org/10.1109/WACV57701.2024.00502}, DOI={10.1109/wacv57701.2024.00502}, booktitle={2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, publisher={IEEE}, author={Stypułkowski, Michał and Vougioukas, Konstantinos and He, Sen and Zięba, Maciej and Petridis, Stavros and Pantic, Maja}, year={2024}, month=jan, pages={5089–5098} }

% Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation

@misc{https://doi.org/10.48550/arxiv.2506.09350,
  doi = {10.48550/ARXIV.2506.09350},
  url = {https://arxiv.org/abs/2506.09350},
  author = {Lin, Shanchuan and Yang, Ceyuan and He, Hao and Jiang, Jianwen and Ren, Yuxi and Xia, Xin and Zhao, Yang and Xiao, Xuefeng and Jiang, Lu},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% MoCA: Identity-Preserving Text-to-Video Generation via Mixture of Cross Attention

@inproceedings{Xie_2025, series={MMAsia ’25}, title={MoCA: Identity-Preserving Text-to-Video Generation via Mixture of Cross Attention}, url={http://dx.doi.org/10.1145/3743093.3770948}, DOI={10.1145/3743093.3770948}, booktitle={Proceedings of the 7th ACM International Conference on Multimedia in Asia}, publisher={ACM}, author={Xie, Qi and Ma, Yongjia and Di, Donglin and Gao, Xuehao and Yang, Xun}, year={2025}, month=dec, pages={1–8}, collection={MMAsia ’25} }

% Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model

@misc{https://doi.org/10.48550/arxiv.2508.13009,
  doi = {10.48550/ARXIV.2508.13009},
  url = {https://arxiv.org/abs/2508.13009},
  author = {He, Xianglong and Peng, Chunli and Liu, Zexiang and Wang, Boyang and Zhang, Yifan and Cui, Qi and Kang, Fei and Jiang, Biao and An, Mengyin and Ren, Yangyang and Xu, Baixin and Guo, Hao-Xiang and Gong, Kaixiong and Wu, Size and Li, Wei and Song, Xuchen and Liu, Yang and Li, Yangguang and Zhou, Yahui},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Matrix-game 2.0: An open-source real-time and streaming interactive world model},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Self-Forcing++: Towards Minute-Scale High-Quality Video Generation

@misc{https://doi.org/10.48550/arxiv.2510.02283,
  doi = {10.48550/ARXIV.2510.02283},
  url = {https://arxiv.org/abs/2510.02283},
  author = {Cui, Justin and Wu, Jie and Li, Ming and Yang, Tao and Li, Xiaojie and Wang, Rui and Bai, Andrew and Ban, Yuanhao and Hsieh, Cho-Jui},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Self-Forcing++: Towards Minute-Scale High-Quality Video Generation},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

% Pix2Video: Video Editing using Image Diffusion

@inproceedings{Ceylan_2023, title={Pix2Video: Video Editing using Image Diffusion}, url={http://dx.doi.org/10.1109/ICCV51070.2023.02121}, DOI={10.1109/iccv51070.2023.02121}, booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, publisher={IEEE}, author={Ceylan, Duygu and Huang, Chun-Hao P. and Mitra, Niloy J.}, year={2023}, month=oct, pages={23149–23160} }

% Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile

@misc{https://doi.org/10.48550/arxiv.2502.06155,
  doi = {10.48550/ARXIV.2502.06155},
  url = {https://arxiv.org/abs/2502.06155},
  author = {Ding, Hangliang and Li, Dacheng and Su, Runlong and Zhang, Peiyuan and Deng, Zhijie and Stoica, Ion and Zhang, Hao},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation

@misc{https://doi.org/10.48550/arxiv.2508.08248,
  doi = {10.48550/ARXIV.2508.08248},
  url = {https://arxiv.org/abs/2508.08248},
  author = {Tu, Shuyuan and Pan, Yueming and Huang, Yinming and Han, Xintong and Xing, Zhen and Dai, Qi and Luo, Chong and Wu, Zuxuan and Jiang, Yu-Gang},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% OmniCache: A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for Diffusion Transformer Models

@misc{https://doi.org/10.48550/arxiv.2508.16212,
  doi = {10.48550/ARXIV.2508.16212},
  url = {https://arxiv.org/abs/2508.16212},
  author = {Chu, Huanpeng and Wu, Wei and Fen, Guanyu and Zhang, Yutao},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {OmniCache: A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for Diffusion Transformer Models},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text

@inproceedings{Henschel_2025, title={StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text}, url={http://dx.doi.org/10.1109/CVPR52734.2025.00245}, DOI={10.1109/cvpr52734.2025.00245}, booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Henschel, Roberto and Khachatryan, Levon and Poghosyan, Hayk and Hayrapetyan, Daniil and Tadevosyan, Vahram and Wang, Zhangyang and Navasardyan, Shant and Shi, Humphrey}, year={2025}, month=jun, pages={2568–2577} }

% ControlVideo: Training-free Controllable Text-to-Video Generation

@misc{https://doi.org/10.48550/arxiv.2305.13077,
  doi = {10.48550/ARXIV.2305.13077},
  url = {https://arxiv.org/abs/2305.13077},
  author = {Zhang, Yabo and Wei, Yuxiang and Jiang, Dongsheng and Zhang, Xiaopeng and Zuo, Wangmeng and Tian, Qi},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {ControlVideo: Training-free Controllable Text-to-Video Generation},
  publisher = {arXiv},
  year = {2023},
  copyright = {Creative Commons Attribution 4.0 International}
}

% MagicVideo: Efficient Video Generation With Latent Diffusion Models

@misc{https://doi.org/10.48550/arxiv.2211.11018,
  doi = {10.48550/ARXIV.2211.11018},
  url = {https://arxiv.org/abs/2211.11018},
  author = {Zhou, Daquan and Wang, Weimin and Yan, Hanshu and Lv, Weiwei and Zhu, Yizhe and Feng, Jiashi},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {MagicVideo: Efficient Video Generation With Latent Diffusion Models},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion

@misc{https://doi.org/10.48550/arxiv.2506.08009,
  doi = {10.48550/ARXIV.2506.08009},
  url = {https://arxiv.org/abs/2506.08009},
  author = {Huang, Xun and Li, Zhengqi and He, Guande and Zhou, Mingyuan and Shechtman, Eli},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

% BitsFusion: 1.99 bits Weight Quantization of Diffusion Model

@misc{https://doi.org/10.48550/arxiv.2406.04333,
  doi = {10.48550/ARXIV.2406.04333},
  url = {https://arxiv.org/abs/2406.04333},
  author = {Sui, Yang and Li, Yanyu and Kag, Anil and Idelbayev, Yerlan and Cao, Junli and Hu, Ju and Sagar, Dhritiman and Yuan, Bo and Tulyakov, Sergey and Ren, Jian},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {BitsFusion: 1.99 bits Weight Quantization of Diffusion Model},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

% DraftAttention: Fast Video Diffusion via Low-Resolution Attention Guidance

@misc{https://doi.org/10.48550/arxiv.2505.14708,
  doi = {10.48550/ARXIV.2505.14708},
  url = {https://arxiv.org/abs/2505.14708},
  author = {Shen, Xuan and Han, Chenxia and Zhou, Yufa and Xie, Yanyue and Gong, Yifan and Wang, Quanyi and Wang, Yiwei and Wang, Yanzhi and Zhao, Pu and Gu, Jiuxiang},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {DraftAttention: Fast Video Diffusion via Low-Resolution Attention Guidance},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

% Video Diffusion Models: A Survey

@misc{https://doi.org/10.48550/arxiv.2405.03150,
  doi = {10.48550/ARXIV.2405.03150},
  url = {https://arxiv.org/abs/2405.03150},
  author = {Melnik, Andrew and Ljubljanac, Michal and Lu, Cong and Yan, Qi and Ren, Weiming and Ritter, Helge},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Video Diffusion Models: A Survey},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Single Trajectory Distillation for Accelerating Image and Video Style Transfer

@inproceedings{Xu_2025, series={MM ’25}, title={Single Trajectory Distillation for Accelerating Image and Video Style Transfer}, url={http://dx.doi.org/10.1145/3746027.3754900}, DOI={10.1145/3746027.3754900}, booktitle={Proceedings of the 33rd ACM International Conference on Multimedia}, publisher={ACM}, author={Xu, Sijie and Wang, Runqi and Zhu, Wei and Song, Dejia and Chen, Nemo and Tang, Xu and Hu, Yao}, year={2025}, month=oct, pages={9463–9471}, collection={MM ’25} }

% FastVideoEdit: Leveraging Consistency Models for Efficient Text-to-Video Editing

@inproceedings{Zhang_2025, title={FastVideoEdit: Leveraging Consistency Models for Efficient Text-to-Video Editing}, url={http://dx.doi.org/10.1109/WACV61041.2025.00360}, DOI={10.1109/wacv61041.2025.00360}, booktitle={2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, publisher={IEEE}, author={Zhang, Youyuan and Ju, Xuan and Clark, James J.}, year={2025}, month=feb, pages={3657–3666} }

% Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation

@article{Wang_2025, title={Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation}, volume={133}, ISSN={1573-1405}, url={http://dx.doi.org/10.1007/s11263-025-02349-y}, DOI={10.1007/s11263-025-02349-y}, number={7}, journal={International Journal of Computer Vision}, publisher={Springer Science and Business Media LLC}, author={Wang, Wenjing and Yang, Huan and Tuo, Zixi and He, Huiguo and Zhu, Junchen and Fu, Jianlong and Liu, Jiaying}, year={2025}, month=feb, pages={4177–4195} }

% QVD: Post-training Quantization for Video Diffusion Models

@inproceedings{Tian_2024, series={MM ’24}, title={QVD: Post-training Quantization for Video Diffusion Models}, url={http://dx.doi.org/10.1145/3664647.3681050}, DOI={10.1145/3664647.3681050}, booktitle={Proceedings of the 32nd ACM International Conference on Multimedia}, publisher={ACM}, author={Tian, Shilong and Chen, Hong and Lv, Chengtao and Liu, Yu and Guo, Jinyang and Liu, Xianglong and Li, Shengxi and Yang, Hao and Xie, Tao}, year={2024}, month=oct, pages={10572–10581}, collection={MM ’24} }

% Diffusion Models Are Real-Time Game Engines

@misc{https://doi.org/10.48550/arxiv.2408.14837,
  doi = {10.48550/ARXIV.2408.14837},
  url = {https://arxiv.org/abs/2408.14837},
  author = {Valevski, Dani and Leviathan, Yaniv and Arar, Moab and Fruchter, Shlomi},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Diffusion Models Are Real-Time Game Engines},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Towards One-step Causal Video Generation via Adversarial Self-Distillation

@misc{https://doi.org/10.48550/arxiv.2511.01419,
  doi = {10.48550/ARXIV.2511.01419},
  url = {https://arxiv.org/abs/2511.01419},
  author = {Yang, Yongqi and Huang, Huayang and Peng, Xu and Hu, Xiaobin and Luo, Donghao and Zhang, Jiangning and Wang, Chengjie and Wu, Yu},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Towards One-step Causal Video Generation via Adversarial Self-Distillation},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% DOLLAR: Few-Step Video Generation via Distillation and Latent Reward Optimization

@misc{https://doi.org/10.48550/arxiv.2412.15689,
  doi = {10.48550/ARXIV.2412.15689},
  url = {https://arxiv.org/abs/2412.15689},
  author = {Ding, Zihan and Jin, Chi and Liu, Difan and Zheng, Haitian and Singh, Krishna Kumar and Zhang, Qiang and Kang, Yan and Lin, Zhe and Liu, Yuchen},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {DOLLAR: Few-Step Video Generation via Distillation and Latent Reward Optimization},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

% MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation

@misc{https://doi.org/10.48550/arxiv.2510.18692,
  doi = {10.48550/ARXIV.2510.18692},
  url = {https://arxiv.org/abs/2510.18692},
  author = {Jia, Weinan and Lu, Yuning and Huang, Mengqi and Wang, Hualiang and Huang, Binyuan and Chen, Nan and Liu, Mu and Jiang, Jidong and Mao, Zhendong},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% SkyReels-V2: Infinite-length Film Generative Model

@misc{https://doi.org/10.48550/arxiv.2504.13074,
  doi = {10.48550/ARXIV.2504.13074},
  url = {https://arxiv.org/abs/2504.13074},
  author = {Chen, Guibin and Lin, Dixuan and Yang, Jiangping and Lin, Chunze and Zhu, Junchen and Fan, Mingyuan and Zhang, Hao and Chen, Sheng and Chen, Zheng and Ma, Chengcheng and Xiong, Weiming and Wang, Wei and Pang, Nuo and Kang, Kang and Xu, Zhiheng and Jin, Yuzhe and Liang, Yupeng and Song, Yubing and Zhao, Peng and Xu, Boyuan and Qiu, Di and Li, Debang and Fei, Zhengcong and Li, Yang and Zhou, Yahui},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {SkyReels-V2: Infinite-length Film Generative Model},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% RainFusion: Adaptive Video Generation Acceleration via Multi-Dimensional Visual Redundancy

@misc{https://doi.org/10.48550/arxiv.2505.21036,
  doi = {10.48550/ARXIV.2505.21036},
  url = {https://arxiv.org/abs/2505.21036},
  author = {Chen, Aiyue and Dong, Bin and Li, Jingru and Lin, Jing and Tian, Kun and Yao, Yiwu and Wang, Gongyi},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {RainFusion: Adaptive Video Generation Acceleration via Multi-Dimensional Visual Redundancy},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% QVGen: Pushing the Limit of Quantized Video Generative Models

@misc{https://doi.org/10.48550/arxiv.2505.11497,
  doi = {10.48550/ARXIV.2505.11497},
  url = {https://arxiv.org/abs/2505.11497},
  author = {Huang, Yushi and Gong, Ruihao and Liu, Jing and Ding, Yifu and Lv, Chengtao and Qin, Haotong and Zhang, Jun},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {QVGen: Pushing the Limit of Quantized Video Generative Models},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Training-free Diffusion Acceleration with Bottleneck Sampling

@misc{https://doi.org/10.48550/arxiv.2503.18940,
  doi = {10.48550/ARXIV.2503.18940},
  url = {https://arxiv.org/abs/2503.18940},
  author = {Tian, Ye and Xia, Xin and Ren, Yuxi and Lin, Shanchuan and Wang, Xing and Xiao, Xuefeng and Tong, Yunhai and Yang, Ling and Cui, Bin},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Training-free Diffusion Acceleration with Bottleneck Sampling},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset

@misc{https://doi.org/10.48550/arxiv.2503.19462,
  doi = {10.48550/ARXIV.2503.19462},
  url = {https://arxiv.org/abs/2503.19462},
  author = {Zhang, Haiyu and Chen, Xinyuan and Wang, Yaohui and Liu, Xihui and Wang, Yunhong and Qiao, Yu},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

% Mixture of Contexts for Long Video Generation

@misc{https://doi.org/10.48550/arxiv.2508.21058,
  doi = {10.48550/ARXIV.2508.21058},
  url = {https://arxiv.org/abs/2508.21058},
  author = {Cai, Shengqu and Yang, Ceyuan and Zhang, Lvmin and Guo, Yuwei and Xiao, Junfei and Yang, Ziyan and Xu, Yinghao and Yang, Zhenheng and Yuille, Alan and Guibas, Leonidas and Agrawala, Maneesh and Jiang, Lu and Wetzstein, Gordon},
  keywords = {Graphics (cs.GR), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Mixture of Contexts for Long Video Generation},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

% POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models

@misc{https://doi.org/10.48550/arxiv.2508.21019,
  doi = {10.48550/ARXIV.2508.21019},
  url = {https://arxiv.org/abs/2508.21019},
  author = {Cheng, Jiaxiang and Ma, Bing and Ren, Xuhua and Jin, Hongyi Henry and Yu, Kai and Zhang, Peng and Li, Wenyue and Zhou, Yuan and Zheng, Tianxiang and Lu, Qinglin},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Phased One-Step Adversarial Equilibrium for Video Diffusion Models},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion Transformers

@misc{https://doi.org/10.48550/arxiv.2408.17131,
  doi = {10.48550/ARXIV.2408.17131},
  url = {https://arxiv.org/abs/2408.17131},
  author = {Deng, Juncan and Li, Shuaiting and Wang, Zeyu and Gu, Hong and Xu, Kedong and Huang, Kejie},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2; I.4},
  title = {VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion Transformers},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Efficient Diffusion Models: A Comprehensive Survey from Principles to Practices

@misc{https://doi.org/10.48550/arxiv.2505.18875,
  doi = {10.48550/ARXIV.2505.18875},
  url = {https://arxiv.org/abs/2505.18875},
  author = {Yang, Shuo and Xi, Haocheng and Zhao, Yilong and Li, Muyang and Zhang, Jintao and Cai, Han and Lin, Yujun and Li, Xiuyu and Xu, Chenfeng and Peng, Kelly and Chen, Jianfei and Han, Song and Keutzer, Kurt and Stoica, Ion},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via Semantic-Aware Permutation},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% AR-Diffusion: Asynchronous Video Generation with Auto-Regressive Diffusion

@inproceedings{Sun_2025, title={AR-Diffusion: Asynchronous Video Generation with Auto-Regressive Diffusion}, url={http://dx.doi.org/10.1109/CVPR52734.2025.00690}, DOI={10.1109/cvpr52734.2025.00690}, booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Sun, Mingzhen and Wang, Weining and Li, Gen and Liu, Jiawei and Sun, Jiahui and Feng, Wanquan and Lao, Shanshan and Zhou, Siyu and He, Qian and Liu, Jing}, year={2025}, month=jun, pages={7364–7373} }

% SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer

@misc{https://doi.org/10.48550/arxiv.2509.24695,
  doi = {10.48550/ARXIV.2509.24695},
  url = {https://arxiv.org/abs/2509.24695},
  author = {Chen, Junsong and Zhao, Yuyang and Yu, Jincheng and Chu, Ruihang and Chen, Junyu and Yang, Shuai and Wang, Xianbang and Pan, Yicheng and Zhou, Daquan and Ling, Huan and Liu, Haozhe and Yi, Hongwei and Zhang, Hao and Li, Muyang and Chen, Yukang and Cai, Han and Fidler, Sanja and Luo, Ping and Han, Song and Xie, Enze},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

% FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers

@misc{https://doi.org/10.48550/arxiv.2509.16518,
  doi = {10.48550/ARXIV.2509.16518},
  url = {https://arxiv.org/abs/2509.16518},
  author = {Durvasula, Sankeerth and Sreedhar, Kavya and Moustafa, Zain and Kothawade, Suraj and Gondimalla, Ashish and Subramanian, Suvinay and Shahidi, Narges and Vijaykumar, Nandita},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Hardware Architecture (cs.AR), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

% SSM Meets Video Diffusion Models: Efficient Video Generation with Structured State Spaces

@misc{https://doi.org/10.48550/arxiv.2403.07711,
  doi = {10.48550/ARXIV.2403.07711},
  url = {https://arxiv.org/abs/2403.07711},
  author = {Oshima, Yuta and Taniguchi, Shohei and Suzuki, Masahiro and Matsuo, Yutaka},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {SSM Meets Video Diffusion Models: Efficient Long-Term Video Generation with Structured State Spaces},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Mobile Video Diffusion

@misc{https://doi.org/10.48550/arxiv.2412.07583,
  doi = {10.48550/ARXIV.2412.07583},
  url = {https://arxiv.org/abs/2412.07583},
  author = {Yahia, Haitam Ben and Korzhenkov, Denis and Lelekas, Ioannis and Ghodrati, Amir and Habibian, Amirhossein},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Mobile Video Diffusion},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

% MoViE: Mobile Diffusion for Video Editing

@misc{https://doi.org/10.48550/arxiv.2412.06578,
  doi = {10.48550/ARXIV.2412.06578},
  url = {https://arxiv.org/abs/2412.06578},
  author = {Karjauv, Adil and Fathima, Noor and Lelekas, Ioannis and Porikli, Fatih and Ghodrati, Amir and Habibian, Amirhossein},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {MoViE: Mobile Diffusion for Video Editing},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Learning Few-Step Diffusion Models by Trajectory Distribution Matching

@misc{https://doi.org/10.48550/arxiv.2503.06674,
  doi = {10.48550/ARXIV.2503.06674},
  url = {https://arxiv.org/abs/2503.06674},
  author = {Luo, Yihong and Hu, Tianyang and Sun, Jiacheng and Cai, Yujun and Tang, Jing},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Learning Few-Step Diffusion Models by Trajectory Distribution Matching},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Video Prediction by Modeling Videos as Continuous Multi-Dimensional Processes

@inproceedings{Shrivastava_2024, title={Video Prediction by Modeling Videos as Continuous Multi-Dimensional Processes}, url={http://dx.doi.org/10.1109/CVPR52733.2024.00691}, DOI={10.1109/cvpr52733.2024.00691}, booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Shrivastava, Gaurav and Shrivastava, Abhinav}, year={2024}, month=jun, pages={7236–7245} }

% Diffusion Probabilistic Modeling for Video Generation

@article{Yang_2023, title={Diffusion Probabilistic Modeling for Video Generation}, volume={25}, ISSN={1099-4300}, url={http://dx.doi.org/10.3390/e25101469}, DOI={10.3390/e25101469}, number={10}, journal={Entropy}, publisher={MDPI AG}, author={Yang, Ruihan and Srivastava, Prakhar and Mandt, Stephan}, year={2023}, month=oct, pages={1469} }

% Consistency Models

@inbook{Harrison_2015, title={Consistency Models}, ISBN={9781484213292}, url={http://dx.doi.org/10.1007/978-1-4842-1329-2_9}, DOI={10.1007/978-1-4842-1329-2_9}, booktitle={Next Generation Databases}, publisher={Apress}, author={Harrison, Guy}, year={2015}, pages={127–144} }

% TCAQ-DM: Timestep-Channel Adaptive Quantization for Diffusion Models

@misc{https://doi.org/10.48550/arxiv.2412.16700,
  doi = {10.48550/ARXIV.2412.16700},
  url = {https://arxiv.org/abs/2412.16700},
  author = {Huang, Haocheng and Chen, Jiaxin and Guo, Jinyang and Zhan, Ruiyi and Wang, Yunhong},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {TCAQ-DM: Timestep-Channel Adaptive Quantization for Diffusion Models},
  publisher = {arXiv},
  year = {2024},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Motion-Guided Latent Diffusion for Temporally Consistent Real-world Video Super-resolution

@misc{https://doi.org/10.48550/arxiv.2312.00853,
  doi = {10.48550/ARXIV.2312.00853},
  url = {https://arxiv.org/abs/2312.00853},
  author = {Yang, Xi and He, Chenhang and Ma, Jianqi and Zhang, Lei},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Motion-Guided Latent Diffusion for Temporally Consistent Real-world Video Super-resolution},
  publisher = {arXiv},
  year = {2023},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Adaptive Caching for Faster Video Generation with Diffusion Transformers

@misc{https://doi.org/10.48550/arxiv.2411.02397,
  doi = {10.48550/ARXIV.2411.02397},
  url = {https://arxiv.org/abs/2411.02397},
  author = {Kahatapitiya, Kumara and Liu, Haozhe and He, Sen and Liu, Ding and Jia, Menglin and Zhang, Chenyang and Ryoo, Michael S. and Xie, Tian},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Adaptive Caching for Faster Video Generation with Diffusion Transformers},
  publisher = {arXiv},
  year = {2024},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Accelerating Video Diffusion Models via Distribution Matching

@misc{https://doi.org/10.48550/arxiv.2412.05899,
  doi = {10.48550/ARXIV.2412.05899},
  url = {https://arxiv.org/abs/2412.05899},
  author = {Zhu, Yuanzhi and Yan, Hanshu and Yang, Huan and Zhang, Kai and Li, Junnan},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Accelerating Video Diffusion Models via Distribution Matching},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Structure and Content-Guided Video Synthesis with Diffusion Models

@inproceedings{Esser_2023, title={Structure and Content-Guided Video Synthesis with Diffusion Models}, url={http://dx.doi.org/10.1109/ICCV51070.2023.00675}, DOI={10.1109/iccv51070.2023.00675}, booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, publisher={IEEE}, author={Esser, Patrick and Chiu, Johnathan and Atighehchian, Parmida and Granskog, Jonathan and Germanidis, Anastasis}, year={2023}, month=oct, pages={7312–7322} }

% FPSAttention: Training-Aware FP8 and Sparsity Co-Design for Fast Video Diffusion

@misc{https://doi.org/10.48550/arxiv.2506.04648,
  doi = {10.48550/ARXIV.2506.04648},
  url = {https://arxiv.org/abs/2506.04648},
  author = {Liu, Akide and Zhang, Zeyu and Li, Zhexin and Bai, Xuehai and Han, Yizeng and Tang, Jiasheng and Xing, Yuanjie and Wu, Jichao and Yang, Mingyang and Chen, Weihua and He, Jiahao and He, Yuanyu and Wang, Fan and Haffari, Gholamreza and Zhuang, Bohan},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {FPSAttention: Training-Aware FP8 and Sparsity Co-Design for Fast Video Diffusion},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

% Video Probabilistic Diffusion Models in Projected Latent Space

@inproceedings{Yu_2023, title={Video Probabilistic Diffusion Models in Projected Latent Space}, url={http://dx.doi.org/10.1109/CVPR52729.2023.01770}, DOI={10.1109/cvpr52729.2023.01770}, booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Yu, Sihyun and Sohn, Kihyuk and Kim, Subin and Shin, Jinwoo}, year={2023}, month=jun, pages={18456–18466} }

% Timestep Embedding Tells: It’s Time to Cache for Video Diffusion Model

@inproceedings{Liu_2025, title={Timestep Embedding Tells: It’s Time to Cache for Video Diffusion Model}, url={http://dx.doi.org/10.1109/CVPR52734.2025.00689}, DOI={10.1109/cvpr52734.2025.00689}, booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Liu, Feng and Zhang, Shiwei and Wang, Xiaofeng and Wei, Yujie and Qiu, Haonan and Zhao, Yuzhong and Zhang, Yingya and Ye, Qixiang and Wan, Fang}, year={2025}, month=jun, pages={7353–7363} }

% T2V-Turbo: Breaking the Quality Bottleneck of Video Consistency Model with Mixed Reward Feedback

@misc{https://doi.org/10.48550/arxiv.2405.18750,
  doi = {10.48550/ARXIV.2405.18750},
  url = {https://arxiv.org/abs/2405.18750},
  author = {Li, Jiachen and Feng, Weixi and Fu, Tsu-Jui and Wang, Xinyi and Basu, Sugato and Chen, Wenhu and Wang, William Yang},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {T2V-Turbo: Breaking the Quality Bottleneck of Video Consistency Model with Mixed Reward Feedback},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Temporal Dynamic Quantization for Diffusion Models

@misc{https://doi.org/10.48550/arxiv.2306.02316,
  doi = {10.48550/ARXIV.2306.02316},
  url = {https://arxiv.org/abs/2306.02316},
  author = {So, Junhyuk and Lee, Jungwon and Ahn, Daehyun and Kim, Hyungjun and Park, Eunhyeok},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Temporal Dynamic Quantization for Diffusion Models},
  publisher = {arXiv},
  year = {2023},
  copyright = {Creative Commons Attribution 4.0 International}
}

% AdaDiff: Adaptive Step Selection for Fast Diffusion Models

@article{Zhang_2025, title={AdaDiff: Adaptive Step Selection for Fast Diffusion Models}, volume={39}, ISSN={2159-5399}, url={http://dx.doi.org/10.1609/aaai.v39i9.33075}, DOI={10.1609/aaai.v39i9.33075}, number={9}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, publisher={Association for the Advancement of Artificial Intelligence (AAAI)}, author={Zhang, Hui and Wu, Zuxuan and Xing, Zhen and Shao, Jie and Jiang, Yu-Gang}, year={2025}, month=apr, pages={9914–9922} }

% AccuQuant: Simulating Multiple Denoising Steps for Quantizing Diffusion Models

@misc{https://doi.org/10.48550/arxiv.2510.20348,
  doi = {10.48550/ARXIV.2510.20348},
  url = {https://arxiv.org/abs/2510.20348},
  author = {Lee, Seunghoon and Choi, Jeongwoo and Son, Byunggwan and Moon, Jaehyeon and Jeon, Jeimin and Ham, Bumsub},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {AccuQuant: Simulating Multiple Denoising Steps for Quantizing Diffusion Models},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

% Dancing Avatar: Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model

@misc{https://doi.org/10.48550/arxiv.2308.07749,
  doi = {10.48550/ARXIV.2308.07749},
  url = {https://arxiv.org/abs/2308.07749},
  author = {Qin, Bosheng and Ye, Wentao and Yu, Qifan and Tang, Siliang and Zhuang, Yueting},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Dancing Avatar: Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model},
  publisher = {arXiv},
  year = {2023},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Learning Spatial Adaptation and Temporal Coherence in Diffusion Models for Video Super-Resolution

@inproceedings{Chen_2024, title={Learning Spatial Adaptation and Temporal Coherence in Diffusion Models for Video Super-Resolution}, url={http://dx.doi.org/10.1109/CVPR52733.2024.00882}, DOI={10.1109/cvpr52733.2024.00882}, booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Chen, Zhikai and Long, Fuchen and Qiu, Zhaofan and Yao, Ting and Zhou, Wengang and Luo, Jiebo and Mei, Tao}, year={2024}, month=jun, pages={9232–9241} }

% SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention

@misc{https://doi.org/10.48550/arxiv.2509.24006,
  doi = {10.48550/ARXIV.2509.24006},
  url = {https://arxiv.org/abs/2509.24006},
  author = {Zhang, Jintao and Wang, Haoxu and Jiang, Kai and Yang, Shuo and Zheng, Kaiwen and Xi, Haocheng and Wang, Ziteng and Zhu, Hongzhou and Zhao, Min and Stoica, Ion and Gonzalez, Joseph E. and Zhu, Jun and Chen, Jianfei},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% AnimateDiff-Lightning: Cross-Model Diffusion Distillation

@misc{https://doi.org/10.48550/arxiv.2403.12706,
  doi = {10.48550/ARXIV.2403.12706},
  url = {https://arxiv.org/abs/2403.12706},
  author = {Lin, Shanchuan and Yang, Xiao},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {AnimateDiff-Lightning: Cross-Model Diffusion Distillation},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Modulated Diffusion: Accelerating Generative Modeling with Modulated Quantization

@misc{https://doi.org/10.48550/arxiv.2506.22463,
  doi = {10.48550/ARXIV.2506.22463},
  url = {https://arxiv.org/abs/2506.22463},
  author = {Gao, Weizhi and Hou, Zhichao and Yin, Junqi and Wang, Feiyi and Peng, Linyu and Liu, Xiaorui},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Modulated Diffusion: Accelerating Generative Modeling with Modulated Quantization},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft

@misc{https://doi.org/10.48550/arxiv.2510.03198,
  doi = {10.48550/ARXIV.2510.03198},
  url = {https://arxiv.org/abs/2510.03198},
  author = {Huang, Junchao and Hu, Xinting and Han, Boyao and Shi, Shaoshuai and Tian, Zhuotao and He, Tianyu and Jiang, Li},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Q-VDiT: Towards Accurate Quantization and Distillation of Video-Generation Diffusion Transformers

@misc{https://doi.org/10.48550/arxiv.2505.22167,
  doi = {10.48550/ARXIV.2505.22167},
  url = {https://arxiv.org/abs/2505.22167},
  author = {Feng, Weilun and Yang, Chuanguang and Qin, Haotong and Li, Xiangqi and Wang, Yu and An, Zhulin and Huang, Libo and Diao, Boyu and Zhao, Zixiang and Xu, Yongjun and Magno, Michele},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Q-VDiT: Towards Accurate Quantization and Distillation of Video-Generation Diffusion Transformers},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Training-free and Adaptive Sparse Attention for Efficient Long Video Generation

@misc{https://doi.org/10.48550/arxiv.2502.21079,
  doi = {10.48550/ARXIV.2502.21079},
  url = {https://arxiv.org/abs/2502.21079},
  author = {Xia, Yifei and Ling, Suhan and Fu, Fangcheng and Wang, Yujie and Li, Huixia and Xiao, Xuefeng and Cui, Bin},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Training-free and Adaptive Sparse Attention for Efficient Long Video Generation},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Lumiere: A Space-Time Diffusion Model for Video Generation

@inproceedings{Bar_Tal_2024, series={SA ’24}, title={Lumiere: A Space-Time Diffusion Model for Video Generation}, url={http://dx.doi.org/10.1145/3680528.3687614}, DOI={10.1145/3680528.3687614}, booktitle={SIGGRAPH Asia 2024 Conference Papers}, publisher={ACM}, author={Bar-Tal, Omer and Chefer, Hila and Tov, Omer and Herrmann, Charles and Paiss, Roni and Zada, Shiran and Ephrat, Ariel and Hur, Junhwa and Liu, Guanghui and Raj, Amit and Li, Yuanzhen and Rubinstein, Michael and Michaeli, Tomer and Wang, Oliver and Sun, Deqing and Dekel, Tali and Mosseri, Inbar}, year={2024}, month=dec, pages={1–11}, collection={SA ’24} }

% RewardDance: Reward Scaling in Visual Generation

@misc{https://doi.org/10.48550/arxiv.2509.08826,
  doi = {10.48550/ARXIV.2509.08826},
  url = {https://arxiv.org/abs/2509.08826},
  author = {Wu, Jie and Gao, Yu and Ye, Zilyu and Li, Ming and Li, Liang and Guo, Hanzhong and Liu, Jie and Xue, Zeyue and Hou, Xiaoxia and Liu, Wei and Zeng, Yan and Huang, Weilin},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {RewardDance: Reward Scaling in Visual Generation},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Playing with Transformer at 30+ FPS via Next-Frame Diffusion

@misc{https://doi.org/10.48550/arxiv.2506.01380,
  doi = {10.48550/ARXIV.2506.01380},
  url = {https://arxiv.org/abs/2506.01380},
  author = {Cheng, Xinle and He, Tianyu and Xu, Jiayi and Guo, Junliang and He, Di and Bian, Jiang},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Playing with Transformer at 30+ FPS via Next-Frame Diffusion},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% Magic 1-For-1: Generating One Minute Video Clips within One Minute

@misc{https://doi.org/10.48550/arxiv.2502.07701,
  doi = {10.48550/ARXIV.2502.07701},
  url = {https://arxiv.org/abs/2502.07701},
  author = {Yi, Hongwei and Shao, Shitong and Ye, Tian and Zhao, Jiantong and Yin, Qingyu and Lingelbach, Michael and Yuan, Li and Tian, Yonghong and Xie, Enze and Zhou, Daquan},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Magic 1-For-1: Generating One Minute Video Clips within One Minute},
  publisher = {arXiv},
  year = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% SparseCtrl: Adding Sparse Controls to Text-to-Video Diffusion Models

@misc{https://doi.org/10.48550/arxiv.2311.16933,
  doi = {10.48550/ARXIV.2311.16933},
  url = {https://arxiv.org/abs/2311.16933},
  author = {Guo, Yuwei and Yang, Ceyuan and Rao, Anyi and Agrawala, Maneesh and Lin, Dahua and Dai, Bo},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {SparseCtrl: Adding Sparse Controls to Text-to-Video Diffusion Models},
  publisher = {arXiv},
  year = {2023},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

% ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation

@misc{https://doi.org/10.48550/arxiv.2406.02540,
  doi = {10.48550/ARXIV.2406.02540},
  url = {https://arxiv.org/abs/2406.02540},
  author = {Zhao, Tianchen and Fang, Tongcheng and Huang, Haofeng and Liu, Enshu and Wan, Rui and Soedarmadji, Widyadewi and Li, Shiyao and Lin, Zinan and Dai, Guohao and Yan, Shengen and Yang, Huazhong and Ning, Xuefei and Wang, Yu},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

% FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality

@misc{https://doi.org/10.48550/arxiv.2410.19355,
  doi = {10.48550/ARXIV.2410.19355},
  url = {https://arxiv.org/abs/2410.19355},
  author = {Lv, Zhengyao and Si, Chenyang and Song, Junhao and Yang, Zhenyu and Qiao, Yu and Liu, Ziwei and Wong, Kwan-Yee K.},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality},
  publisher = {arXiv},
  year = {2024},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% ART•V: Auto-Regressive Text-to-Video Generation with Diffusion Models

@inproceedings{Weng_2024, title={ART•V: Auto-Regressive Text-to-Video Generation with Diffusion Models}, url={http://dx.doi.org/10.1109/CVPRW63382.2024.00735}, DOI={10.1109/cvprw63382.2024.00735}, booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, publisher={IEEE}, author={Weng, Wenming and Feng, Ruoyu and Wang, Yanhui and Dai, Qi and Wang, Chunyu and Yin, Dacheng and Zhao, Zhiyuan and Qiu, Kai and Bao, Jianmin and Yuan, Yuhui and Luo, Chong and Zhang, Yueyi and Xiong, Zhiwei}, year={2024}, month=jun, pages={7395–7405} }

% Rethinking the Self-Attention in Vision Transformers

@inproceedings{Kim_2021, title={Rethinking the Self-Attention in Vision Transformers}, url={http://dx.doi.org/10.1109/CVPRW53098.2021.00342}, DOI={10.1109/cvprw53098.2021.00342}, booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, publisher={IEEE}, author={Kim, Kyungmin and Wu, Bichen and Dai, Xiaoliang and Zhang, Peizhao and Yan, Zhicheng and Vajda, Peter and Kim, Seon}, year={2021}, month=jun }

% UltraVSR: Achieving Ultra-Realistic Video Super-Resolution with Efficient One-Step Diffusion Space

@inproceedings{Liu_2025, series={MM ’25}, title={UltraVSR: Achieving Ultra-Realistic Video Super-Resolution with Efficient One-Step Diffusion Space}, url={http://dx.doi.org/10.1145/3746027.3755117}, DOI={10.1145/3746027.3755117}, booktitle={Proceedings of the 33rd ACM International Conference on Multimedia}, publisher={ACM}, author={Liu, Yong and Pan, Jinshan and Li, Yinchuan and Dong, Qingji and Zhu, Chao and Guo, Yu and Wang, Fei}, year={2025}, month=oct, pages={7785–7794}, collection={MM ’25} }

% VSA: Faster Video Diffusion with Trainable Sparse Attention

@misc{https://doi.org/10.48550/arxiv.2505.13389,
  doi = {10.48550/ARXIV.2505.13389},
  url = {https://arxiv.org/abs/2505.13389},
  author = {Zhang, Peiyuan and Chen, Yongqi and Huang, Haofeng and Lin, Will and Liu, Zhengzhong and Stoica, Ion and Xing, Eric and Zhang, Hao},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {VSA: Faster Video Diffusion with Trainable Sparse Attention},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% LongLive: Real-time Interactive Long Video Generation

@misc{https://doi.org/10.48550/arxiv.2509.22622,
  doi = {10.48550/ARXIV.2509.22622},
  url = {https://arxiv.org/abs/2509.22622},
  author = {Yang, Shuai and Huang, Wei and Chu, Ruihang and Xiao, Yicheng and Zhao, Yuyang and Wang, Xianbang and Li, Muyang and Xie, Enze and Chen, Yingcong and Lu, Yao and Han, Song and Chen, Yukang},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {LongLive: Real-time Interactive Long Video Generation},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution 4.0 International}
}

% VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step

@inproceedings{Wang_2025, title={VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step}, url={http://dx.doi.org/10.1109/CVPR52734.2025.01536}, DOI={10.1109/cvpr52734.2025.01536}, booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Wang, Hanyang and Liu, Fangfu and Chi, Jiawei and Duan, Yueqi}, year={2025}, month=jun, pages={16475–16485} }

% Diffusion Model Quantization: A Review

@misc{https://doi.org/10.48550/arxiv.2505.05215,
  doi = {10.48550/ARXIV.2505.05215},
  url = {https://arxiv.org/abs/2505.05215},
  author = {Zeng, Qian and Hu, Chenggong and Song, Mingli and Song, Jie},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Diffusion Model Quantization: A Review},
  publisher = {arXiv},
  year = {2025},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

% Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation

@misc{https://doi.org/10.48550/arxiv.2404.04057,
  doi = {10.48550/ARXIV.2404.04057},
  url = {https://arxiv.org/abs/2404.04057},
  author = {Zhou, Mingyuan and Zheng, Huangjie and Wang, Zhendong and Yin, Mingzhang and Huang, Hai},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{franceschi2020stochastic,
  title={Stochastic latent residual video prediction},
  author={Franceschi, Jean-Yves and Delasalles, Edouard and Chen, Micka{\"e}l and Lamprier, Sylvain and Gallinari, Patrick},
  booktitle={International Conference on Machine Learning},
  pages={3233--3246},
  year={2020},
  organization={PMLR}
}

@article{huang2025linvideo,
  title={Linvideo: A post-training framework towards o (n) attention in efficient video generation},
  author={Huang, Yushi and Ge, Xingtong and Gong, Ruihao and Lv, Chengtao and Zhang, Jun},
  journal={arXiv preprint arXiv:2510.08318},
  year={2025}
}

@article{ghafoorian2025attention,
  title={Attention surgery: An efficient recipe to linearize your video diffusion transformer},
  author={Ghafoorian, Mohsen and Korzhenkov, Denis and Habibian, Amirhossein},
  journal={arXiv preprint arXiv:2509.24899},
  year={2025}
}

@article{liu2025rectified,
  title={Rectified SpaAttn: Revisiting Attention Sparsity for Efficient Video Generation},
  author={Liu, Xuewen and Li, Zhikai and Zhang, Jing and Chen, Mengjuan and Gu, Qingyi},
  journal={arXiv preprint arXiv:2511.19835},
  year={2025}
}

@article{huang2025live,
  title={Live avatar: Streaming real-time audio-driven avatar generation with infinite length},
  author={Huang, Yubo and Guo, Hailong and Wu, Fangtai and Zhang, Shifeng and Huang, Shijie and Gan, Qijun and Liu, Lin and Zhao, Sirui and Chen, Enhong and Liu, Jiaming and others},
  journal={arXiv preprint arXiv:2512.04677},
  year={2025}
}

@article{ma2024latte,
  title={Latte: Latent diffusion transformer for video generation},
  author={Ma, Xin and Wang, Yaohui and Chen, Xinyuan and Jia, Gengyun and Liu, Ziwei and Li, Yuan-Fang and Chen, Cunjian and Qiao, Yu},
  journal={arXiv preprint arXiv:2401.03048},
  year={2024}
}

@article{feng2025streamdiffusionv2,
  title={StreamDiffusionV2: A Streaming System for Dynamic and Interactive Video Generation},
  author={Feng, Tianrui and Li, Zhi and Yang, Shuo and Xi, Haocheng and Li, Muyang and Li, Xiuyu and Zhang, Lvmin and Yang, Keting and Peng, Kelly and Han, Song and others},
  journal={arXiv preprint arXiv:2511.07399},
  year={2025}
}

@article{wu2020integer,
  title={Integer quantization for deep learning inference: Principles and empirical evaluation},
  author={Wu, Hao and Judd, Patrick and Zhang, Xiaojie and Isaev, Mikhail and Micikevicius, Paulius},
  journal={arXiv preprint arXiv:2004.09602},
  year={2020}
}

@article{yesiltepe2025infinity,
  title={Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout},
  author={Yesiltepe, Hidir and Meral, Tuna Han Salih and Akan, Adil Kaan and Oktay, Kaan and Yanardag, Pinar},
  journal={arXiv preprint arXiv:2511.20649},
  year={2025}
}

@article{feng2025s,
  title={S $\^{} 2$ Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation},
  author={Feng, Weilun and Qin, Haotong and Yang, Chuanguang and Li, Xiangqi and Yang, Han and Li, Yuqi and An, Zhulin and Huang, Libo and Magno, Michele and Xu, Yongjun},
  journal={arXiv preprint arXiv:2508.04016},
  year={2025}
}

@article{wang2024phased,
  title={Phased consistency models},
  author={Wang, Fu-Yun and Huang, Zhaoyang and Bergman, Alexander and Shen, Dazhong and Gao, Peng and Lingelbach, Michael and Sun, Keqiang and Bian, Weikang and Song, Guanglu and Liu, Yu and others},
  journal={Advances in neural information processing systems},
  volume={37},
  pages={83951--84009},
  year={2024}
}

@article{cheng2025twinflow,
  title={TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows},
  author={Cheng, Zhenglin and Sun, Peng and Li, Jianguo and Lin, Tao},
  journal={arXiv preprint arXiv:2512.05150},
  year={2025}
}

@article{yi2025deep,
  title={Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression},
  author={Yi, Jung and Jang, Wooseok and Cho, Paul Hyunbin and Nam, Jisu and Yoon, Heeji and Kim, Seungryong},
  journal={arXiv preprint arXiv:2512.05081},
  year={2025}
}

ng(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})

@String(IJCV = {Int. J. Comput. Vis.})

@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})

@String(ICCV= {Int. Conf. Comput. Vis.})

@String(ECCV= {Eur. Conf. Comput. Vis.})

@String(NIPS= {Adv. Neural Inform. Process. Syst.})

@String(ICPR = {Int. Conf. Pattern Recog.})

@String(BMVC= {Brit. Mach. Vis. Conf.})

@String(TOG= {ACM Trans. Graph.})

@String(TIP  = {IEEE Trans. Image Process.})

@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})

@String(TMM  = {IEEE Trans. Multimedia})

@String(ACMMM= {ACM Int. Conf. Multimedia})

@String(ICME = {Int. Conf. Multimedia and Expo})

@String(ICASSP=	{ICASSP})

@String(ICIP = {IEEE Int. Conf. Image Process.})

@String(ACCV  = {ACCV})

@String(ICLR = {Int. Conf. Learn. Represent.})

@String(IJCAI = {IJCAI})

@String(PR   = {Pattern Recognition})

@String(AAAI = {AAAI})

@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})

@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})

@String(VR   = {Vis. Res.})

@String(JOV	 = {J. Vis.})

@String(TVC  = {The Vis. Comput.})

@String(JCST  = {J. Comput. Sci. Tech.})

@String(CGF  = {Comput. Graph. Forum})

@String(CVM = {Computational Visual Media})

@String(PAMI  = {IEEE TPAMI})

@String(IJCV  = {IJCV})

@String(CVPR  = {CVPR})

@String(ICCV  = {ICCV})

@String(ECCV  = {ECCV})

@String(NIPS  = {NeurIPS})

@String(ICPR  = {ICPR})

@String(BMVC  =	{BMVC})

@String(TOG   = {ACM TOG})

@String(TIP   = {IEEE TIP})

@String(TVCG  = {IEEE TVCG})

@String(TCSVT = {IEEE TCSVT})

@String(TMM   =	{IEEE TMM})

@String(ACMMM = {ACM MM})

@String(ICME  =	{ICME})

@String(ICASSP=	{ICASSP})

@String(ICIP  = {ICIP})

@String(ACCV  = {ACCV})

@String(ICLR  = {ICLR})

@String(IJCAI = {IJCAI})

@String(PR = {PR})

@String(AAAI = {AAAI})

@String(CVPRW= {CVPRW})

@String(CSVT = {IEEE TCSVT})

@misc{Authors14,
 author = {FirstName LastName},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {FirstName LastName},
 title = {Frobnication tutorial},
 note = {Supplied as supplemental material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {FirstName Alpher},
title = {Frobnication},
journal = PAMI,
volume = 12,
number = 1,
pages = {234--778},
year = 2002
}

@article{Alpher03,
author = {FirstName Alpher and  FirstName Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13,
number = 1,
pages = {234--778},
year = 2003
}

@misc{kirstain2023pickapicopendatasetuser,
      title={Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation}, 
      author={Yuval Kirstain and Adam Polyak and Uriel Singer and Shahbuland Matiana and Joe Penna and Omer Levy},
      year={2023},
      eprint={2305.01569},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2305.01569}, 
}

@misc{khanam2024yolov11overviewkeyarchitectural,
      title={YOLOv11: An Overview of the Key Architectural Enhancements}, 
      author={Rahima Khanam and Muhammad Hussain},
      year={2024},
      eprint={2410.17725},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.17725}, 
}

@misc{liu2025improvingvideogenerationhuman,
      title={Improving Video Generation with Human Feedback}, 
      author={Jie Liu and Gongye Liu and Jiajun Liang and Ziyang Yuan and Xiaokun Liu and Mingwu Zheng and Xiele Wu and Qiulin Wang and Menghan Xia and Xintao Wang and Xiaohong Liu and Fei Yang and Pengfei Wan and Di Zhang and Kun Gai and Yujiu Yang and Wanli Ouyang},
      year={2025},
      eprint={2501.13918},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2501.13918}, 
}

@misc{kirillov2023segment,
      title={Segment Anything}, 
      author={Alexander Kirillov and Eric Mintun and Nikhila Ravi and Hanzi Mao and Chloe Rolland and Laura Gustafson and Tete Xiao and Spencer Whitehead and Alexander C. Berg and Wan-Yen Lo and Piotr Dollár and Ross Girshick},
      year={2023},
      eprint={2304.02643},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.02643}, 
}

@article{koala_dataset,
  title={Koala-36m: A large-scale video dataset improving consistency between fine-grained conditions and video content},
  author={Wang, Qiuheng and Shi, Yukai and Ou, Jiarong and Chen, Rui and Lin, Ke and Wang, Jiahao and Jiang, Boyuan and Yang, Haotian and Zheng, Mingwu and Tao, Xin and others},
  journal={arXiv preprint arXiv:2410.08260},
  year={2024}
}

@misc{chen2024videocrafter2,
      title={VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models}, 
      author={Haoxin Chen and Yong Zhang and Xiaodong Cun and Menghan Xia and Xintao Wang and Chao Weng and Ying Shan},
      year={2024},
      eprint={2401.09047},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{REPA,
  title={Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think},
  author={Sihyun Yu and Sangkyung Kwak and Huiwon Jang and Jongheon Jeong and Jonathan Huang and Jinwoo Shin and Saining Xie},
  year={2025},
  booktitle={International Conference on Learning Representations},
}

@inproceedings{chen2023seine,
  title={Seine: Short-to-long video diffusion model for generative transition and prediction},
  author={Chen, Xinyuan and Wang, Yaohui and Zhang, Lingjun and Zhuang, Shaobin and Ma, Xin and Yu, Jiashuo and Wang, Yali and Lin, Dahua and Qiao, Yu and Liu, Ziwei},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{2023i2vgenxl,
  title={I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models},
  author={Zhang, Shiwei and Wang, Jiayu and Zhang, Yingya and Zhao, Kang and Yuan, Hangjie and Qing, Zhiwu and Wang, Xiang  and Zhao, Deli and Zhou, Jingren},
  booktitle={arXiv preprint arXiv:2311.04145},
  year={2023}
}

@article{yang2024cogvideox,
  title={Cogvideox: Text-to-video diffusion models with an expert transformer},
  author={Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others},
  journal={arXiv preprint arXiv:2408.06072},
  year={2024}
}

@article{intern4k_dataset,
  title={AdaPool: Exponential Adaptive Pooling for Information-Retaining Downsampling},
  author={Stergiou, Alexandros and Poppe, Ronald},
  booktitle={arXiv},
  year={2021}
}

@misc{stepfunvideo,
      title={Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model}, 
      author={Guoqing Ma and Haoyang Huang and Kun Yan and Liangyu Chen and Nan Duan and Shengming Yin and Changyi Wan and Ranchen Ming and Xiaoniu Song and Xing Chen and Yu Zhou and Deshan Sun and Deyu Zhou and Jian Zhou and Kaijun Tan and Kang An and Mei Chen and Wei Ji and Qiling Wu and Wen Sun and Xin Han and Yanan Wei and Zheng Ge and Aojie Li and Bin Wang and Bizhu Huang and Bo Wang and Brian Li and Changxing Miao and Chen Xu and Chenfei Wu and Chenguang Yu and Dapeng Shi and Dingyuan Hu and Enle Liu and Gang Yu and Ge Yang and Guanzhe Huang and Gulin Yan and Haiyang Feng and Hao Nie and Haonan Jia and Hanpeng Hu and Hanqi Chen and Haolong Yan and Heng Wang and Hongcheng Guo and Huilin Xiong and Huixin Xiong and Jiahao Gong and Jianchang Wu and Jiaoren Wu and Jie Wu and Jie Yang and Jiashuai Liu and Jiashuo Li and Jingyang Zhang and Junjing Guo and Junzhe Lin and Kaixiang Li and Lei Liu and Lei Xia and Liang Zhao and Liguo Tan and Liwen Huang and Liying Shi and Ming Li and Mingliang Li and Muhua Cheng and Na Wang and Qiaohui Chen and Qinglin He and Qiuyan Liang and Quan Sun and Ran Sun and Rui Wang and Shaoliang Pang and Shiliang Yang and Sitong Liu and Siqi Liu and Shuli Gao and Tiancheng Cao and Tianyu Wang and Weipeng Ming and Wenqing He and Xu Zhao and Xuelin Zhang and Xianfang Zeng and Xiaojia Liu and Xuan Yang and Yaqi Dai and Yanbo Yu and Yang Li and Yineng Deng and Yingming Wang and Yilei Wang and Yuanwei Lu and Yu Chen and Yu Luo and Yuchu Luo and Yuhe Yin and Yuheng Feng and Yuxiang Yang and Zecheng Tang and Zekai Zhang and Zidong Yang and Binxing Jiao and Jiansheng Chen and Jing Li and Shuchang Zhou and Xiangyu Zhang and Xinhao Zhang and Yibo Zhu and Heung-Yeung Shum and Daxin Jiang},
      year={2025},
      eprint={2502.10248},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.10248}, 
}

@article{flashattn3,
  title={Flashattention-3: Fast and accurate attention with asynchrony and low-precision},
  author={Shah, Jay and Bikshandi, Ganesh and Zhang, Ying and Thakkar, Vijay and Ramani, Pradeep and Dao, Tri},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={68658--68685},
  year={2024}
}

@article{Alpher04,
author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14,
number = 1,
pages = {234--778},
year = 2004
}

@inproceedings{Alpher05,
author = {FirstName Alpher and FirstName Gamow},
title = {Can a computer frobnicate?},
booktitle = CVPR,
pages = {234--778},
year = 2005
}

@Misc{TeXFAQ,
  title =        {{UK} List of {\TeX} Frequently Asked Questions},
  author =       {{UK \TeX{} Users Group}},
  year =         2019,
  howpublished = {\url{https://texfaq.org}}
}

@Manual{Downes04:amsart,
  title =        {The \textsf{amsart}, \textsf{amsproc}, and
                  \textsf{amsbook} document~classes},
  author =       {Michael Downes and Barbara Beeton},
  organization = {American Mathematical Society},
  year =         2004,
  month =        aug,
  note =         {\url{http://www.ctan.org/pkg/amslatex}}
}

@article{DQN,
  title={Playing Atari with deep reinforcement learning},
  author={Chung, Jonathan},
  journal={Comput. Ence},
  volume={21},
  pages={351--362},
  year={2013}
}

@inproceedings{dmd,
  title={One-step diffusion with distribution matching distillation},
  author={Yin, Tianwei and Gharbi, Micha{\"e}l and Zhang, Richard and Shechtman, Eli and Durand, Fredo and Freeman, William T and Park, Taesung},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6613--6623},
  year={2024}
}

@InProceedings{vbench,
     title={{VBench}: Comprehensive Benchmark Suite for Video Generative Models},
     author={Huang, Ziqi and He, Yinan and Yu, Jiashuo and Zhang, Fan and Si, Chenyang and Jiang, Yuming and Zhang, Yuanhan and Wu, Tianxing and Jin, Qingyang and Chanpaisit, Nattapol and Wang, Yaohui and Chen, Xinyuan and Wang, Limin and Lin, Dahua and Qiao, Yu and Liu, Ziwei},
     booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
     year={2024}
 }

@inproceedings{vfhq,
  title={Vfhq: A high-quality dataset and benchmark for video face super-resolution},
  author={Xie, Liangbin and Wang, Xintao and Zhang, Honglun and Dong, Chao and Shan, Ying},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={657--666},
  year={2022}
}

@inproceedings{internvl2,
  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24185--24198},
  year={2024}
}

@inproceedings{celebv,
  title={CelebV-HQ: A large-scale video facial attributes dataset},
  author={Zhu, Hao and Wu, Wayne and Zhu, Wentao and Jiang, Liming and Tang, Siwei and Zhang, Li and Liu, Ziwei and Loy, Chen Change},
  booktitle={European conference on computer vision},
  pages={650--667},
  year={2022},
  organization={Springer}
}

@inproceedings{htdf,
  title={Flow-Guided One-Shot Talking Face Generation With a High-Resolution Audio-Visual Dataset},
  author={Zhang, Zhimeng and Li, Lincheng and Ding, Yu and Fan, Changjie},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3661--3670},
  year={2021}
}

@article{vbench++,
     title={VBench++: Comprehensive and Versatile Benchmark Suite for Video Generative Models},
     author={Huang, Ziqi and Zhang, Fan and Xu, Xiaojie and He, Yinan and Yu, Jiashuo and Dong, Ziyue and Ma, Qianli and Chanpaisit, Nattapol and Si, Chenyang and Jiang, Yuming and Wang, Yaohui and Chen, Xinyuan and Chen, Ying-Cong and Wang, Limin and Lin, Dahua and Qiao, Yu and Liu, Ziwei},
     journal={arXiv preprint arXiv:2411.13503},
     year={2024}
 }

@article{dmd2,
  title={Improved Distribution Matching Distillation for Fast Image Synthesis},
  author={Yin, Tianwei and Gharbi, Micha{\"e}l and Park, Taesung and Zhang, Richard and Shechtman, Eli and Durand, Fredo and Freeman, William T},
  journal={arXiv preprint arXiv:2405.14867},
  year={2024}
}

@inproceedings{zero3,
  title={Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning},
  author={Rajbhandari, Samyam and Ruwase, Olatunji and Rasley, Jeff and Smith, Shaden and He, Yuxiong},
  booktitle={Proceedings of the international conference for high performance computing, networking, storage and analysis},
  pages={1--14},
  year={2021}
}

@inproceedings{AutoGAN,
  title={Autogan: Neural architecture search for generative adversarial networks},
  author={Gong, Xinyu and Chang, Shiyu and Jiang, Yifan and Wang, Zhangyang},
  booktitle={International Conference on Computer Vision},
  publisher = {IEEE},
  month = {Oct.-Nov.},
  address = {Seoul, South Korea},
  pages={3224--3234},
  year={2019}
}

@article{meng2021sdedit,
  title={Sdedit: Guided image synthesis and editing with stochastic differential equations},
  author={Meng, Chenlin and He, Yutong and Song, Yang and Song, Jiaming and Wu, Jiajun and Zhu, Jun-Yan and Ermon, Stefano},
  journal={arXiv preprint arXiv:2108.01073},
  year={2021}
}

@inproceedings{
zhang2023fast,
title={Fast Sampling of Diffusion Models with Exponential Integrator},
author={Qinsheng Zhang and Yongxin Chen},
booktitle={International Conference on Learning Representations },
year={2023},
publisher = {OpenReview.net},
}

@article{qi2024not,
  title={Not all noises are created equally: Diffusion noise selection and optimization},
  author={Qi, Zipeng and Bai, Lichen and Xiong, Haoyi and others},
  journal={arXiv preprint arXiv:2407.14041},
  year={2024}
}

@article{shao2023diffuseexpand,
  title={DiffuseExpand: Expanding dataset for 2D medical image segmentation using diffusion models},
  author={Shao, Shitong and Yuan, Xiaohan and Huang, Zhen and Qiu, Ziming and Wang, Shuai and Zhou, Kevin},
  journal={arXiv preprint arXiv:2304.13416},
  year={2023}
}

@article{liu2024alignment,
  title={Alignment of diffusion models: Fundamentals, challenges, and future},
  author={Liu, Buhua and Shao, Shitong and Li, Bao and Bai, Lichen and Xiong, Haoyi and Kwok, James and Helal, Sumi and Xie, Zeke},
  journal={arXiv preprint arXiv:2409.07253},
  year={2024}
}

@inproceedings{zheng2022fast,
  title={Fast Sampling of Diffusion Models via Operator Learning},
  author={Zheng, Hongkai and Nie, Weili and Vahdat, Arash and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  booktitle={NeurIPS 2022 Workshop on Score-Based Methods}
}

@inproceedings{mirzadeh2020improved,
  title={Improved knowledge distillation via teacher assistant},
  author={Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Li, Ang and Levine, Nir and Matsukawa, Akihiro and Ghasemzadeh, Hassan},
  booktitle={Association for the Advance of Artificial Intelligence},
  volume={34},
  number={04},
  pages={5191--5198},
  address = {New York, NY, USA},
  month = {Feb.},
  publisher = {AAAI Press},
  year={2020}
}

@inproceedings{StyleGAN,
  title={A style-based generator architecture for generative adversarial networks},
  author={Karras, Tero and Laine, Samuli and Aila, Timo},
  booktitle={Computer Vision and Pattern Recognition},
  pages={4401--4410},
    month = {Oct.-Nov.},
  address = {Seoul, South Korea},
  year={2019}
}

@inproceedings{BigGAN,
  title={Large Scale GAN Training for High Fidelity Natural Image Synthesis},
  author={Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
  booktitle={International Conference on Learning Representations},
  address = {New Orleans, LA, USA},
  month = {May},
  publisher = {OpenReview.net},
  year = {2019}
}

@article{Graham,
  title={Bi-linearly weighted fractional max pooling},
  author={Hang, Siang Thye and Aono, Masaki},
  journal={Multimedia Tools and Applications},
  volume={76},
  number={21},
  pages={22095--22117},
  year={2017},
  publisher={Springer}
}

@inproceedings{Multilearning,
 author = {Singh, Bharat and Najibi, Mahyar and Davis, Larry S},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {SNIPER: Efficient Multi-Scale Training},
 volume = {31},
 year = {2018}
}

@article{Fastautoaugment,
  title={Fast autoaugment},
  author={Lim, Sungbin and Kim, Ildoo and Kim, Taesup and Kim, Chiheon and Kim, Sungwoong},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{PBA,
  title={Population based augmentation: Efficient learning of augmentation policy schedules},
  author={Ho, Daniel and Liang, Eric and Chen, Xi and Stoica, Ion and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={2731--2741},
  year={2019},
  organization={PMLR}
}

@inproceedings{SPKD,
  title={Similarity-preserving knowledge distillation},
  author={Tung, Frederick and Mori, Greg},
  booktitle={International Conference on Computer Vision},
  pages={1365--1374},
  year={2019}
}

@article{kdsurvey,
  title={Knowledge distillation: A survey},
  author={Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao, Dacheng},
  journal={International Journal of Computer Vision},
  volume={129},
  number={6},
  pages={1789--1819},
  year={2021},
  publisher={Springer}
}

@article{fu2019role,
  title={Role-Wise Data Augmentation for Knowledge Distillation},
  author={Fu, Jie and Geng, Xue and Zhuang, Bohan and Yuan, Xingdi and Trischler, Adam and Lin, Jie and Chandrasekhar, Vijay and Pal, Chris},
  year={2019}
}

@article{wang2020knowledge,
  title={Knowledge distillation thrives on data augmentation},
  author={Wang, Huan and Lohit, Suhas and Jones, Michael and Fu, Yun},
  journal={arXiv preprint arXiv:2012.02909},
  year={2020}
}

@inproceedings{CRD,
  title={Contrastive Representation Distillation},
  author={Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{das2020empirical,
  title={An Empirical Analysis of the Impact of Data Augmentation on Distillation},
  author={Das, Deepan and Massa, Haley and Kulkarni, Abhimanyu and Rekatsinas, Theodoros}
}

@InProceedings{SSKD,
author="Xu, Guodong
and Liu, Ziwei
and Li, Xiaoxiao
and Loy, Chen Change",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="Knowledge Distillation Meets Self-supervision",
booktitle="European Conference on Computer Vision",
year="2020",
publisher="Springer",
address="Cham",
pages="588--604"
}

@inproceedings{HSAKD,
  title={Hierarchical Self-supervised Augmented Knowledge Distillation},
  author={Chuanguang, Yang and Zhulin, An and Linhang, Cai and Yongjun, Xu},
  booktitle={International Joint Conference on Artificial Intelligence},
  pages = {1217--1223},
  publisher = {IJCAI},
  address = {Virtual Event},
  month = {Aug.},
  year={2021}
}

@misc{vanillakd,
  doi = {10.48550/ARXIV.1503.02531},
  
  
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Distilling the Knowledge in a Neural Network},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{SWIN-T,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={International Conference on Computer Vision},
  pages={10012--10022},
  year={2021}
}

@article{ brachman-schmolze:kl-one,
  author = "Ronald~J. Brachman and James~G. Schmolze",
  title = "An overview of the {KL-ONE} knowledge representation system",
  journal = "Cognitive Science",
  volume = "9",
  number = "2",
  pages = "171--216",
  month = "April--June",
  year = "1985"
}

@InProceedings{aware,
author = {Du Chen.},
title = {Orientation-Aware Deep Neural Network for Real Image Super-Resolution},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
month = {June},
year = {2019}
}

@inproceedings{cuhk032,
  title={Re-ranking person re-identification with k-reciprocal encoding},
  author={Zhong Zhun.},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1318--1327},
  year={2017}
}

@inproceedings{cuhk03,
  title={Deepreid: Deep filter pairing neural network for person re-identification},
  author={Li, Wei and Zhao, Rui and Xiao, Tong and Wang, Xiaogang},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={152--159},
  year={2014}
}

@inproceedings{howvitwork,
  title={How Do Vision Transformers Work?},
  author={Park, Namuk and Kim, Songkuk},
  booktitle={International Conference on Learning Representations},
  year={2021},
  publisher={Openreview.net},
  address={Virtual Event},
  month= {May},
}

@article{GPT3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{ gottlob:nonmon,
  author = "Georg Gottlob",
  title = "Complexity results for nonmonotonic logics",
  journal = "Journal of Logic and Computation",
  volume = "2",
  number = "3",
  pages = "397--425",
  month = "June",
  year = "1992"
}

@article{ gls:hypertrees,
  author = "Georg Gottlob and Nicola Leone and Francesco Scarcello",
  title = "Hypertree Decompositions and Tractable Queries",
  journal = "Journal of Computer and System Sciences",
  volume = "64",
  number = "3",
  pages = "579--627",
  month = "May",
  year = "2002"
}

@article{ levesque:functional-foundations,
  author = "Hector~J. Levesque",
  title = "Foundations of a functional approach to knowledge representation",
  journal = "Artificial Intelligence",
  volume = "23",
  number = "2",
  pages = "155--212",
  month = "July",
  year = "1984"
}

@inproceedings{ levesque:belief,
  author = "Hector~J. Levesque",
  title = "A logic of implicit and explicit belief",
  booktitle = "Proceedings of the Fourth National Conference on Artificial Intelligence",
  publisher = "American Association for Artificial Intelligence",
  pages = "198--202",
  address = "Austin, Texas",
  month = "August",
  year = "1984"
}

@article{ nebel:jair-2000,
  author = "Bernhard Nebel",
  title = "On the compilability and expressive power of propositional planning formalisms",
  journal = "Journal of Artificial Intelligence Research",
  volume = "12",
  pages = "271--315",
  year = "2000"
}

@inproceedings{Mixup,
  title={mixup: Beyond Empirical Risk Minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{AutoAugment,
  title={Randaugment: Practical automated data augmentation with a reduced search space},
  author={Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops},
  pages={702--703},
  year={2020}
}

@inproceedings{PCLs,
  title={Online knowledge distillation via collaborative learning},
  author={Guo, Qiushan and Wang, Xinjiang and Wu, Yichao and Yu, Zhipeng and Liang, Ding and Hu, Xiaolin and Luo, Ping},
  booktitle={Computer Vision and Pattern Recognition},
  pages={11020--11029},
  year={2020}
}

@inproceedings{ONE,
  title={Online knowledge distillation with diverse peers},
  author={Chen, Defang and Mei, Jian-Ping and Wang, Can and Feng, Yan and Chen, Chun},
  booktitle={Association for the Advance of Artificial Intelligence},
  volume={34},
  number={04},
  pages={3430--3437},
  year={2020}
}

@inproceedings{mobilenetv2,
  author       = {Mark Sandler and
                  Andrew G. Howard and
                  Menglong Zhu and
                  Andrey Zhmoginov and
                  Liang{-}Chieh Chen},
  title        = {MobileNetV2: Inverted Residuals and Linear Bottlenecks},
  booktitle    = {Computer Vision and Pattern Recognition},
  pages        = {4510--4520},
  publisher    = {IEEE},
  month        = {Jun.},
  address      = {Salt Lake City, UT, USA},
  year         = {2018},
}

@article{CIFAR,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@misc{proceedings,
  author = {{IJCAI Proceedings}},
  title = {{IJCAI} Camera Ready Submission},
  howpublished = {\url{https://proceedings.ijcai.org/info}},
}

@String(PAMI  = {IEEE TPAMI})

@String(IJCV  = {IJCV})

@String(CVPR  = {CVPR})

@String(ICCV  = {ICCV})

@String(ECCV  = {ECCV})

@String(NIPS  = {NeurIPS})

@String(ICPR  = {ICPR})

@String(BMVC  =	{BMVC})

@String(TOG   = {ACM TOG})

@String(TIP   = {IEEE TIP})

@String(TVCG  = {IEEE TVCG})

@String(TCSVT = {IEEE TCSVT})

@String(TMM   =	{IEEE TMM})

@String(ACMMM = {ACM MM})

@String(ICME  =	{ICME})

@String(ICASSP=	{ICASSP})

@String(ICIP  = {ICIP})

@String(ACCV  = {ACCV})

@String(ICLR  = {ICLR})

@String(IJCAI = {IJCAI})

@String(PR = {PR})

@String(AAAI = {AAAI})

@String(CVPRW= {CVPRW})

@String(CSVT = {IEEE TCSVT})

@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})

@String(IJCV = {Int. J. Comput. Vis.})

@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})

@String(ICCV= {Int. Conf. Comput. Vis.})

@String(ECCV= {Eur. Conf. Comput. Vis.})

@String(NIPS= {Adv. Neural Inform. Process. Syst.})

@String(ICPR = {Int. Conf. Pattern Recog.})

@String(BMVC= {Brit. Mach. Vis. Conf.})

@String(ICML= {International Conference on Machine Learning})

@String(TOG= {ACM Trans. Graph.})

@String(TIP  = {IEEE Trans. Image Process.})

@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})

@String(TMM  = {IEEE Trans. Multimedia})

@String(ACMMM= {ACM Int. Conf. Multimedia})

@String(ICME = {Int. Conf. Multimedia and Expo})

@String(ICASSP=	{ICASSP})

@String(ICIP = {IEEE Int. Conf. Image Process.})

@String(ACCV  = {ACCV})

@String(ICLR = {Int. Conf. Learn. Represent.})

@String(IJCAI = {IJCAI})

@String(PR   = {Pattern Recognition})

@String(AAAI = {AAAI})

@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})

@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})

@String(VR   = {Vis. Res.})

@String(JOV	 = {J. Vis.})

@String(TVC  = {The Vis. Comput.})

@String(JCST  = {J. Comput. Sci. Tech.})

@String(CGF  = {Comput. Graph. Forum})

@String(CVM = {Computational Visual Media})

@article{Bengio1994,
author = {Y.~Bengio, P.~Simard, and P.~Frasconi.},
title  = {Learning long-term dependencies with gradient descent is difficult.},
booktitle = {IEEE Transactions on Neural Networks},
year = {1994},
}

@article{Glorot2010,
author = {X.~Glorot and Y.~Bengio.},
title  = {Understanding the difficulty of training deep feedforward neural networks.},
booktitle = {AISTATS},
year = {2010},
}

@inproceedings{ResNet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing},
  booktitle={Computer Vision and Pattern Recognition},
  pages={770--778},
  year={2016},
  publisher = {IEEE},
  month = {Jun.},
  address = {Las Vegas, NV, USA},
}

@inproceedings{ResNetv2,
  title={Identity mappings in deep residual networks},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={European Conference on Computer Vision},
  pages={630--645},
  year={2016},
  publisher={Springer},
  address = {Amsterdam, North Holland, The Netherlands},
  month = {Oct.},
}

@article{BN,
author={S. Ioffe and C. Szegedy.}, 
title={Batch normalization: Accelerating
deep network training by reducing internal covariate shift.},
booktitle = {International Conference on Machine Learning (ICML)},
year={2015}
}

@inproceedings{multi_fusion2,
  title={Multi-Scale Dynamic Convolution for Classification},
  author={Wang, Yunlong and Yang, Lu and Li, Yukun and Fu, Lei},
  booktitle={2021 IEEE 4th International Conference on Information Systems and Computer Aided Education (ICISCAE)},
  pages={142--146},
  year={2021},
  organization={IEEE}
}

@inproceedings{song2019generative,
 author = {Song, Yang and Ermon, Stefano},
 booktitle = {Neural Information Processing Systems},
 pages = {},
 publisher = {NeurIPS},
 title = {Generative Modeling by Estimating Gradients of the Data Distribution},
 volume = {32},
 year = {2019}
}

@ARTICLE{multi_fusion,
  author={Li, Haifeng and Song, Dezhen and Liu, Yu and Li, Binbin},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Automatic Pavement Crack Detection by Multi-Scale Image Fusion}, 
  year={2019},
  volume={20},
  number={6},
  pages={2025-2036},
  doi={10.1109/TITS.2018.2856928}}

@article{ReLU,
author={L. Gatys, A. Ecker, and M. Bethge.},
title={A neural algorithm of artistic style.},
booktitle={Nature Communications},
year={2015}
}

@ARTICLE{Conv,
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE}, 
  title={Gradient-based learning applied to document recognition}, 
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  doi={10.1109/5.726791}}

@INPROCEEDINGS{sift,
  author={Lowe, D.G.},
  booktitle={Proceedings of the Seventh IEEE International Conference on Computer Vision}, 
  title={Object recognition from local scale-invariant features}, 
  year={1999},
  volume={2},
  number={},
  pages={1150-1157 vol.2},
  doi={10.1109/ICCV.1999.790410}}

@paper{Inceptionv4,
	author = {Inception-v4, inception-resnet and the impact of residual connections on learning},
	title = {Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning},
	conference = {AAAI Conference on Artificial Intelligence},
	year = {2017},
	keywords = {deep learning; convolutional neural network; vision; ILSVRC; image classification; Inception},
}

@article{swish,
  title={Searching for Activation Functions},
  author={Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
  year={2018}
}

@article{silu,
  title={Sigmoid-weighted linear units for neural network function approximation in reinforcement learning},
  author={Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
  journal={Neural Networks},
  volume={107},
  pages={3--11},
  year={2018},
  publisher={Elsevier}
}

@InProceedings{Xnception,
author = {Chollet, Francois},
title = {Xception: Deep Learning With Depthwise Separable Convolutions},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@InProceedings{Inceptionv2v3,
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
title = {Rethinking the Inception Architecture for Computer Vision},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@InProceedings{Inceptionv1,
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
title = {Going Deeper With Convolutions},
booktitle = {Computer Vision and Pattern Recognition},
month = {Jun.},
year = {2015},
address = {Boston, Massachusetts},
publisher = {IEEE},
}

@inproceedings{controlnet,
  title={Adding conditional control to text-to-image diffusion models},
  author={Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={3836--3847},
  year={2023}
}

@book{villani2009optimal,
  title={Optimal transport: old and new},
  author={Villani, C{\'e}dric and others},
  volume={338},
  year={2009},
  publisher={Springer}
}

@article{xie2021artificial,
  title={Artificial neural variability for deep learning: On overfitting, noise memorization, and catastrophic forgetting},
  author={Xie, Zeke and He, Fengxiang and Fu, Shaopeng and Sato, Issei and Tao, Dacheng and Sugiyama, Masashi},
  journal={Neural computation},
  volume={33},
  number={8},
  pages={2163--2192},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@software{open-sora-plan,
  author       = {PKU-Yuan Lab and Tuzhan AI etc.},
  title        = {Open-Sora-Plan},
  month        = apr,
  year         = 2024,
  publisher    = {GitHub},
  doi          = {10.5281/zenodo.10948109},
}

@article{guo2024i4vgen,
  title={I4VGen: Image as Stepping Stone for Text-to-Video Generation},
  author={Guo, Xiefan and Liu, Jinlin and Cui, Miaomiao and Huang, Di},
  journal={arXiv preprint arXiv:2406.02230},
  year={2024}
}

@inproceedings{BigLittleNet,
  title={Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition},
  author={Chen, Chun-Fu Richard and Fan, Quanfu and Mallinar, Neil and Sercu, Tom and Feris, Rogerio},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{MobileNetV1,
  title={Mobilenets: Efficient convolutional neural networks for mobile vision applications},
  author={Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal={arXiv preprint arXiv:1704.04861},
  year={2017}
}

@article{DPN,
  title={Dual path networks},
  author={Chen, Yunpeng and Li, Jianan and Xiao, Huaxin and Jin, Xiaojie and Yan, Shuicheng and Feng, Jiashi},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{stacknet,
  title={Stacked hourglass networks for human pose estimation},
  author={Newell, Alejandro and Yang, Kaiyu and Deng, Jia},
  booktitle={European conference on computer vision},
  pages={483--499},
  year={2016},
  organization={Springer}
}

@article{huang2019convolutional,
  title={Convolutional networks with dense connectivity},
  author={Huang, Gao and Liu, Zhuang and Pleiss, Geoff and Van Der Maaten, Laurens and Weinberger, Kilian},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  year={2019},
  publisher={IEEE}
}

@inproceedings{transformer,
  title={Exploring self-attention for image recognition},
  author={Zhao, Hengshuang and Jia, Jiaya and Koltun, Vladlen},
  booktitle={Computer Vision and Pattern Recognition},
  pages={10076--10085},
  year={2020}
}

@misc{bochkovskiy2020yolov4,
      title={YOLOv4: Optimal Speed and Accuracy of Object Detection}, 
      author={Alexey Bochkovskiy and Chien-Yao Wang and Hong-Yuan Mark Liao},
      year={2020},
      eprint={2004.10934},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{Wang_2021_CVPR,
    author    = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
    title     = {{Scaled-YOLOv4}: Scaling Cross Stage Partial Network},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {13029-13038}
}

@InProceedings{Dilation,
    author    = {Fisher Yu, Vladlen Koltun},
    title     = {Multi-Scale Context Aggregation by Dilated Convolutions},
    booktitle = {International Conference on Learning Representations(ICLR)},
    year      = {2016},
}

@inproceedings{deeplabv3plus2018,
  title={Encoder-decoder with atrous separable convolution for semantic image segmentation},
  author={Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={801--818},
  year={2018}
}

@inproceedings{Wang3dpoint,
  title={Pointaugmenting: Cross-modal augmentation for 3d object detection},
  author={Wang, Chunwei and Ma, Chao and Zhu, Ming and Yang, Xiaokang},
  booktitle={Computer Vision and Pattern Recognition},
  pages={11794--11803},
  year={2021},
  address={Virtual Event},
}

@article{GCN,
  title={Semi-Supervised Classification with Graph Convolutional Networks},
  author={Kipf, Thomas N and Welling, Max},
  year={2016}
  }

@inproceedings{attention-channel,
  title={Residual attention network for image classification},
  author={Wang, Fei and Jiang, Mengqing and Qian, Chen and Yang, Shuo and Li, Cheng and Zhang, Honggang and Wang, Xiaogang and Tang, Xiaoou},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3156--3164},
  year={2017}
}

@inproceedings{SSH,
  title={Ssh: Single stage headless face detector},
  author={Najibi, Mahyar and Samangouei, Pouya and Chellappa, Rama and Davis, Larry S},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={4875--4884},
  year={2017}
}

@article{yolov3,
  author    = {Joseph Redmon and
               Ali Farhadi},
  title     = {YOLOv3: An Incremental Improvement},
  journal   = {CoRR},
  volume    = {abs/1804.02767},
  year      = {2018},
  eprinttype = {arXiv},
  eprint    = {1804.02767},
  timestamp = {Mon, 13 Aug 2018 16:48:24 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-02767.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{hrnet,
  title={Deep high-resolution representation learning for visual recognition},
  author={Wang, Jingdong and Sun, Ke and Cheng, Tianheng and Jiang, Borui and Deng, Chaorui and Zhao, Yang and Liu, Dong and Mu, Yadong and Tan, Mingkui and Wang, Xinggang and others},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={43},
  number={10},
  pages={3349--3364},
  year={2020},
  publisher={IEEE}
}

@inproceedings{ACB,
  title={Acnet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks},
  author={Ding, Xiaohan and Guo, Yuchen and Ding, Guiguang and Han, Jungong},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1911--1920},
  year={2019}
}

@article{mixerlayer,
  title={Mlp-mixer: An all-mlp architecture for vision},
  author={Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24261--24272},
  year={2021}
}

@inproceedings{patch-netvlad,
  title={Patch-netvlad: Multi-scale fusion of locally-global descriptors for place recognition},
  author={Hausler, Stephen and Garg, Sourav and Xu, Ming and Milford, Michael and Fischer, Tobias},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14141--14152},
  year={2021}
}

@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4700--4708},
  year={2017}
}

@article{larsson2016fractalnet,
  title={FractalNet: Ultra-Deep Neural Networks without Residuals},
  author={Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
  year={2016}
}

@inproceedings{cutmix,
  title={Cutmix: Regularization strategy to train strong classifiers with localizable features},
  author={Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6023--6032},
  year={2019}
}

@article{ILSVRC15,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International Journal of Computer Vision},
  volume={115},
  number={3},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

@inproceedings{ResNext,
  title={Aggregated residual transformations for deep neural networks},
  author={Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1492--1500},
  year={2017}
}

@article{selfattention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@InProceedings{tripletloss,
author = {Dong, Xingping and Shen, Jianbing},
title = {Triplet Loss in Siamese Network for Object Tracking},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}

@inproceedings{market,
  title={Scalable person re-identification: A benchmark},
  author={Zheng, Liang and Shen, Liyue and Tian, Lu and Wang, Shengjin and Wang, Jingdong and Tian, Qi},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1116--1124},
  year={2015}
}

@inproceedings{centreidloss,
  title={On the unreasonable effectiveness of centroids in image retrieval},
  author={Wieczorek, Miko{\l}aj and Rychalska, Barbara and D{\k{a}}browski, Jacek},
  booktitle={International Conference on Neural Information Processing},
  pages={212--223},
  year={2021},
  organization={Springer}
}

@inproceedings{RandomEasing,
  title={Random erasing data augmentation},
  author={Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
  booktitle={Association for the Advance of Artificial Intelligence},
  volume={34},
  number={07},
  pages={13001--13008},
  year={2020}
}

@INPROCEEDINGS{IDNet,
  author={Kim, Cheol-jin and Ha, Young-guk},
  booktitle={2020 IEEE International Conference on Big Data and Smart Computing (BigComp)}, 
  title={IDNet-A: Variant of DenseNet with Inception-Family}, 
  year={2020},
  volume={},
  number={},
  pages={109-112},
  doi={10.1109/BigComp48618.2020.00-91}}

@article{Market1501,
  title={A survey of pruning methods for efficient person re-identification across domains},
  author={Masson, Hugo and Bhuiyan, Amran and Le Thanh Nguyen-Meidine, Mehrsan Javan and Siva, Parthipan and Ayed, Ismail Ben and Granger, Eric},
  year={2019}
}

@inproceedings{ResNetIBN,
  title={Two at once: Enhancing learning and generalization capacities via ibn-net},
  author={Pan, Xingang and Luo, Ping and Shi, Jianping and Tang, Xiaoou},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={464--479},
  year={2018}
}

@InProceedings{EfficientNetV2,
  title = 	 {EfficientNetV2: Smaller Models and Faster Training},
  author =       {Tan, Mingxing and Le, Quoc},
  booktitle = 	 {International Conference on Machine Learning},
  pages = 	 {10096--10106},
  year = 	 {2021},
  address = {Virtual Event},
  volume = 	 {139},
  month = 	 {Jul.},
  publisher =    {PMLR},
}

@inproceedings{SimCLR,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

@article{2015The,
  title={The Power of Depth for Feedforward Neural Networks},
  author={ Eldan, R.  and  Shamir, O. },
  journal={Computer Science},
  year={2015},
}

@article{depth,
  title={The loss surface and expressivity of deep convolutional neural networks},
  author={Nguyen, Quynh and Hein, Matthias},
  booktitle={International Conference on Learning Representations},
  publisher ={OpenReview.net},
  address = {BC, Canada},
  month = {Apr.-May},
  year={2018}
}

@inproceedings{INN,
  title={Integral Neural Networks},
  author={Solodskikh, Kirill and Kurbanov, Azim and Aydarkhanov, Ruslan and Zhelavskaya, Irina and Parfenov, Yury and Song, Dehua and Lefkimmiatis, Stamatios},
  booktitle={Computer Vision and Pattern Recognition},
  pages={16113--16122},
  year={2023},
  publisher={{IEEE}},
  address={Vancouver, BC},
  month={Jun.},
}

@inproceedings{FastKD,
  title={A fast knowledge distillation framework for visual recognition},
  author={Shen, Zhiqiang and Xing, Eric},
  booktitle={European Conference on Computer Vision},
  pages={673--690},
  year={2022},
  organization={Springer}
}

@article{vote_classifier_ensemble,
  title={A weighted voting framework for classifiers ensembles},
  author={Kuncheva, Ludmila I and Rodr{\'\i}guez, Juan J},
  journal={Knowledge and information systems},
  volume={38},
  pages={259--275},
  year={2014},
  publisher={Springer}
}

@INPROCEEDINGS{Zagoruyko2016WRN,
    author = {Sergey Zagoruyko and Nikos Komodakis},
    title = {Wide Residual Networks},
    booktitle = {British Machine Vision Conference},
    year = {2016},
    publisher    = {{BMVA} Press},
    address = {York, UK},
    month = {Spet.},}

@inproceedings{VIT,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={International Conference on Learning Representations},
  year={2020},
  publisher = {OpenReview.net},
  month = {May},
  address = {Event Virtual},
}

@article{STL,
author = { Adam Coates},
title = {Ng An Analysis of Single Layer Networks in Unsupervised Feature Learning AISTATS},
year = {2011}
}

@inproceedings{UNet,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={Medical Image Computing and Computer-Assisted Intervention},
  pages={234--241},
  year={2015},
  publisher={Springer},
  address = {Germany, Central Europe},
  month = {Oct.},
}

@inproceedings{FPN,
  title={Feature pyramid networks for object detection},
  author={Lin, Tsung-Yi and Doll{\'a}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  booktitle={Computer Vision and Pattern Recognition},
  address = {Honolulu, Hawaii},
  publisher = {IEEE},
  month = {Jul.},
  pages={2117--2125},
  year={2017}
}

@inproceedings{convnext,
  title={A convnet for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11976--11986},
  year={2022}
}

@article{PANet,
author={Shu Liu},
title={Path aggregation network for instance segmentation},
journal={CVPR},
year={2018},
}

@article{contrastivebegin,
  author    = {Raia Hadsell and Sumit Chopra and and Yann LeCun},
  title     = {Dimensionality reduction by learning an invariant mapping},
  journal   = {CVPR},
  year      = {2006},
}

@inproceedings{MOCO,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9729--9738},
  year={2020}
}

@article{kdunderstandingsmoothing,
  author    = {Jiaxi Tang and
               Rakesh Shivanna and
               Zhe Zhao and
               Dong Lin and
               Anima Singh and
               Ed H. Chi and
               Sagar Jain},
  title     = {Understanding and Improving Knowledge Distillation},
  journal   = {CoRR},
  volume    = {abs/2002.03532},
  year      = {2020},
  eprinttype = {arXiv},
  eprint    = {2002.03532},
  timestamp = {Wed, 12 Feb 2020 16:38:55 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2002-03532.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DOD,
  title={Distilling object detectors with fine-grained feature imitation},
  author={Wang, Tao and Yuan, Li and Zhang, Xiaopeng and Feng, Jiashi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4933--4942},
  year={2019}
}

@inproceedings{CWD,
  title={Channel-wise knowledge distillation for dense prediction},
  author={Shu, Changyong and Liu, Yifan and Gao, Jianfei and Yan, Zheng and Shen, Chunhua},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={5311--5320},
  year={2021}
}

@inproceedings{COFD,
  title={A comprehensive overhaul of feature distillation},
  author={Heo, Byeongho and Kim, Jeesoo and Yun, Sangdoo and Park, Hyojin and Kwak, Nojun and Choi, Jin Young},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1921--1930},
  year={2019}
}

@article{LEKD,
  title={Learning efficient object detection models with knowledge distillation},
  author={Chen, Guobin and Choi, Wongun and Yu, Xiang and Han, Tony and Chandraker, Manmohan},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@InProceedings{kdrevisitingsmoothing,
author = {Yuan, Li and Tay, Francis EH and Li, Guilin and Wang, Tao and Feng, Jiashi},
title = {Revisiting Knowledge Distillation via Label Smoothing Regularization},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@inproceedings{shufflenet,
  title={Shufflenet: An extremely efficient convolutional neural network for mobile devices},
  author={Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
  booktitle={Computer Vision and Pattern Recognition},
  pages={6848--6856},
  year={2018}
}

@inproceedings{DPK,
  author       = {Martin Zong and
                  Zengyu Qiu and
                  Xinzhu Ma and
                  Kunlin Yang and
                  Chunya Liu and
                  Jun Hou and
                  Shuai Yi and
                  Wanli Ouyang},
  title        = {Better Teacher Better Student: Dynamic Prior Knowledge for Knowledge Distillation},
  booktitle    = {International Conference on Learning Representations},
  publisher    = {OpenReview.net},
  address      = {Kigali, Rwanda},
  month        = {May},
  year         = {2023},
}

@inproceedings{tiny_imagenet,
  author    = {Amirhossein Tavanaei},
  title     = {Embedded Encoder-Decoder in Convolutional Networks Towards Explainable
               {AI}},
  journal   = {CoRR},
  volume    = {abs/2007.06712},
  year      = {2020},
  eprinttype = {arXiv},
  eprint    = {2007.06712},
  timestamp = {Tue, 21 Jul 2020 12:53:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-06712.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{torchdistill,
  title={torchdistill: A modular, configuration-driven framework for knowledge distillation},
  author={Matsubara, Yoshitomo},
  booktitle={International Workshop on Reproducible Research in Pattern Recognition},
  pages={24--44},
  year={2021},
  organization={Springer}
}

@InProceedings{RKD,
author = {Park, Wonpyo and Kim, Dongju and Lu, Yan and Cho, Minsu},
title = {Relational Knowledge Distillation},
booktitle = {Computer Vision and Pattern Recognition},
month = {June},
year = {2019},
publisher = {IEEE},
address = {Long Beach, CA, USA},
}

@article{epoch_gnn_2022,
  title={Don't Be So Dense: Sparse-to-Sparse GAN Training Without Sacrificing Performance},
  author={Liu, Shiwei and Tian, Yuesong and Chen, Tianlong and Shen, Li},
  journal={International Journal of Computer Vision},
  volume={20},
  number={X},
  year={2022},
  publisher={Springer}
}

@article{epoch_attack_2021,
  title={Game Theory for Adversarial Attacks and Defenses},
  author={Sharma, Shorya},
  journal={arXiv preprint arXiv:2110.06166},
  year={2021}
}

@InProceedings{epoch_ICCV_2017,
author = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
title = {Revisiting Unreasonable Effectiveness of Data in Deep Learning Era},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}

@inproceedings{CCKD,
  title={Correlation congruence for knowledge distillation},
  author={Peng, Baoyun and Jin, Xiao and Liu, Jiaheng and Li, Dongsheng and Wu, Yichao and Liu, Yu and Zhou, Shunfeng and Zhang, Zhaoning},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={5007--5016},
  year={2019}
}

@inproceedings{ATKD,
  title={Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  booktitle={International Conference on Learning Representations},
  year={2016}
}

@InProceedings{DKD,
    author    = {Zhao, Borui and Cui, Quan and Song, Renjie and Qiu, Yiyu and Liang, Jiajun},
    title     = {Decoupled Knowledge Distillation},
    booktitle = {Computer Vision and Pattern Recognition},
    month     = {June},
    year      = {2022},
    pages     = {11953-11962}
}

@inproceedings{efficientnet,
  title={Efficientnet: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc},
  booktitle={International conference on machine learning},
  pages={6105--6114},
  year={2019},
  organization={PMLR}
}

@InProceedings{Liu_2021_ICCV,
    author    = {Liu, Songhua and Lin, Tianwei and He, Dongliang and Li, Fu and Deng, Ruifeng and Li, Xin and Ding, Errui and Wang, Hao},
    title     = {Paint Transformer: Feed Forward Neural Painting With Stroke Prediction},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {6598-6607}
}

@article{razavi2022automatic,
  title={An automatic system to monitor the physical distance and face mask wearing of construction workers in COVID-19 pandemic},
  author={Razavi, Moein and Alikhani, Hamed and Janfaza, Vahid and Sadeghi, Benyamin and Alikhani, Ehsan},
  journal={SN computer science},
  volume={3},
  number={1},
  pages={1--8},
  year={2022},
  publisher={Springer}
}

@article{liu2021isetauto,
  title={ISETAuto: Detecting vehicles with depth and radiance information},
  author={Liu, Zhenyi and Farrell, Joyce and Wandell, Brian A},
  journal={IEEE Access},
  volume={9},
  pages={41799--41808},
  year={2021},
  publisher={IEEE}
}

@inproceedings{kumar2021fine,
  title={Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution},
  author={Kumar, Ananya and Raghunathan, Aditi and Jones, Robbie Matthew and Ma, Tengyu and Liang, Percy},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{guo2021dpn,
  title={DPN: Detail-preserving network with high resolution representation for efficient segmentation of retinal vessels},
  author={Guo, Song},
  journal={Journal of Ambient Intelligence and Humanized Computing},
  pages={1--14},
  year={2021},
  publisher={Springer}
}

@article{YOCO,
  title={You Only Cut Once: Boosting Data Augmentation with a Single Cut},
  author={Han, Junlin and Fang, Pengfei and Li, Weihao and Hong, Jie and Armin, Mohammad Ali and Reid, Ian and Petersson, Lars and Li, Hongdong},
  booktitle = {International Conference on Machine Learning (ICML)},
  year={2022}
}

@inproceedings{2022knowledge_train_best_student,
  title={Knowledge distillation: A good teacher is patient and consistent},
  author={Beyer, Lucas and Zhai, Xiaohua and Royer, Am{\'e}lie and Markeeva, Larisa and Anil, Rohan and Kolesnikov, Alexander},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10925--10934},
  year={2022}
}

@inproceedings{COCO,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European Conference on Computer Vision},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@article{FitNet,
  title={Fitnets: Hints for thin deep nets},
  author={Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.6550},
  year={2014}
}

@inproceedings{
timm,
title={ResNet strikes back: An improved training procedure in timm},
author={Ross Wightman and Hugo Touvron and Herve Jegou},
booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future},
year={2021},
}

@inproceedings{FKD,
  title={Improve object detection with feature-based knowledge distillation: Towards accurate and efficient detectors},
  author={Zhang, Linfeng and Ma, Kaisheng},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{RNN,
  author       = {Ronald J. Williams and
                  David Zipser},
  title        = {A Learning Algorithm for Continually Running Fully Recurrent Neural Networks},
  journal      = {Neural Computation},
  volume       = {1},
  number       = {2},
  pages        = {270--280},
  year         = {1989},
}

@inproceedings{gflownet1,
  author       = {Emmanuel Bengio and
                  Moksh Jain and
                  Maksym Korablyov and
                  Doina Precup and
                  Yoshua Bengio},
  title        = {Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation},
  booktitle    = {Neural Information Processing Systems},
  address      = {Virtual Event},
  month        = {Dec.},
  pages        = {27381--27394},
  year         = {2021},
}

@inproceedings{gflownet2,
  author       = {Dinghuai Zhang and
                  Nikolay Malkin and
                  Zhen Liu and
                  Alexandra Volokhova and
                  Aaron C. Courville and
                  Yoshua Bengio},
  title        = {Generative Flow Networks for Discrete Probabilistic Modeling},
  booktitle    = {International Conference on Machine Learning},
  address = {Baltimore, Maryland, {USA}},
  month = {Jul.},
  volume       = {162},
  pages        = {26412--26428},
  publisher    = {{PMLR}},
  year         = {2022},
}

@article{LSTM,
  title={Long short-term memory},
  author={Hochreiter S and Schmidhuber J},
  journal={Neural Computation},
  pages={1375-1780},
  year={1997},
  publisher={MIT Press Journals}
}

@article{NODE,
  title={Neural ordinary differential equations},
  author={Chen, Ricky TQ and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
  journal={Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{TST,
  author       = {Shitong Shao and
                  Huanran Chen and
                  Zhen Huang and
                  Linrui Gong and
                  Shuai Wang and
                  Xinxiao Wu},
  title        = {Teaching What You Should Teach: {A} Data-Based Distillation Method},
  booktitle    = {International Joint Conference on
                  Artificial Intelligence},
  address      = {Macao, SAR, China},
  month         = {Aug.},
  pages        = {1351--1359},
  publisher    = {ijcai.org},
  year         = {2023},
}

@inproceedings{zhang2019your,
  title={Be your own teacher: Improve the performance of convolutional neural networks via self distillation},
  author={Zhang, Linfeng and Song, Jiebo and Gao, Anni and Chen, Jingwei and Bao, Chenglong and Ma, Kaisheng},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={3713--3722},
  year={2019}
}

@inproceedings{wu2021peer,
  title={Peer collaborative learning for online knowledge distillation},
  author={Wu, Guile and Gong, Shaogang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={12},
  pages={10302--10310},
  year={2021}
}

@inproceedings{AADetection,
  title={Learning data augmentation strategies for object detection},
  author={Zoph, Barret and Cubuk, Ekin D and Ghiasi, Golnaz and Lin, Tsung-Yi and Shlens, Jonathon and Le, Quoc V},
  booktitle={European conference on computer vision},
  pages={566--583},
  year={2020},
  organization={Springer}
}

@InProceedings{mutuallearning,
author = {Zhang, Ying and Xiang, Tao and Hospedales, Timothy M. and Lu, Huchuan},
title = {Deep Mutual Learning},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@article{icml23_curvature,
  title={Minimizing Trajectory Curvature of ODE-based Generative Models},
  author={Lee, Sangyun and Kim, Beomsu and Ye, Jong Chul},
  journal={arXiv preprint arXiv:2301.12003},
  year={2023}
}

@article{iclr22_rect,
  title={Flow straight and fast: Learning to generate and transfer data with rectified flow},
  author={Liu, Xingchao and Gong, Chengyue and Liu, Qiang},
  journal={arXiv preprint arXiv:2209.03003},
  year={2022}
}

@article{nips22_design,
  title={Elucidating the design space of diffusion-based generative models},
  author={Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli},
  journal={arXiv preprint arXiv:2206.00364},
  year={2022}
}

@inproceedings{adm2021,
  title={Diffusion models beat gans on image synthesis},
  author={Dhariwal, Prafulla and Nichol, Alexander},
  booktitle={Neural Information Processing Systems},
  volume={34},
  pages={8780--8794},
  year={2021},
  publisher = {NeurIPS},
  address = {Virtual Event},
 month = {Dec.},
}

@article{adm2022_medical,
  title={Diffusion Probabilistic Models beat GANs on Medical Images},
  author={M{\"u}ller-Franzes, Gustav and Niehues, Jan Moritz and Khader, Firas and Arasteh, Soroosh Tayebi and Haarburger, Christoph and Kuhl, Christiane and Wang, Tianci and Han, Tianyu and Nebelung, Sven and Kather, Jakob Nikolas and others},
  journal={arXiv preprint arXiv:2212.07501},
  year={2022}
}

@inproceedings{
poole2023dreamfusion,
title={DreamFusion: Text-to-3D using 2D Diffusion},
author={Ben Poole and Ajay Jain and Jonathan T. Barron and Ben Mildenhall},
booktitle={International Conference on Learning Representations},
year={2023},
publisher = {OpenReview.net},
address = {kigali, rwanda},
month = {May},
}

@inproceedings{sde,
  title={Score-Based Generative Modeling through Stochastic Differential Equations},
  author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  booktitle={International Conference on Learning Representations},
  publisher = {OpenReview.net},
address = {kigali, rwanda},
month = {May.},
year={2023},
}

@inproceedings{dpm_solver,
  title={DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps},
  author={Lu, Cheng and Zhou, Yuhao and Bao, Fan and Chen, Jianfei and Li, Chongxuan and Zhu, Jun},
  booktitle={Neural Information Processing Systems},
  publisher = {NeurIPS},
  month = {Nov.-Dec.},
  year = {2022},
  address = {New Orleans, LA, USA},
}

@article{dpm_solver++,
  title={Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models},
  author={Lu, Cheng and Zhou, Yuhao and Bao, Fan and Chen, Jianfei and Li, Chongxuan},
  journal={arXiv preprint arXiv:2211.01095},
  year={2022}
}

@book{fpequation,
  title={Stochastic differential equations},
  author={{\O}ksendal, Bernt and {\O}ksendal, Bernt},
  year={2003},
  publisher={Springer}
}

@inproceedings{ddim,
  title={Denoising Diffusion Implicit Models},
  author={Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  booktitle={International Conference on Learning Representations},
    publisher = {OpenReview.net},
address = {kigali, rwanda},
month = {May.},
year={2023},
}

@online{shujian,
        title={Generating Diffusion Models Ramblings (IX)\: Conditional Control of Generating Results},
        author={Jianlin Su},
        year={2022},
        month={Aug}}

@misc{cgmh,
    title={CGMH Pelvis Datasets for Research And Education Purposes Only},
    author={Tommy NgX},
    year={2021},
}

@inproceedings{is,
  title={Improved techniques for training gans},
  author={Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  booktitle={Neural Information Processing Systems},
  volume={29},
  year={2016},
  address = {Centre Convencions Internacional Barcelona, Barcelona SPAIN},
  month = {Dec.},
  publisher = {NeurIPS},
}

@inproceedings{fid,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  booktitle={Neural Information Processing Systems},
  volume={30},
  year={2017},
address = {Long Beach Convention Center, Long Beach},
  month = {Dec.},
  publisher = {NeurIPS},
}

@article{covid,
  title={COVID-19 image data collection},
  author={Joseph Paul Cohen and Paul Morrison and Lan Dao},
  journal={arXiv 2003.11597},
  year={2020}
}

@article{wang2022medical,
  title={Medical image segmentation using deep learning: A survey},
  author={Wang, Risheng and Lei, Tao and Cui, Ruixia and Zhang, Bingtao and Meng, Hongying and Nandi, Asoke K},
  journal={IET Image Processing},
  volume={16},
  year={2022},
  publisher={Wiley Online Library}
}

@article{anaya2022fourier,
  title={Fourier transform-based data augmentation in deep learning for diabetic foot thermograph classification},
  author={Anaya-Isaza, Andr{\'e}s and Zequera-Diaz, Martha},
  journal={Biocybernetics and Biomedical Engineering},
  volume={42},
  year={2022},
  publisher={Elsevier}
}

@article{chen2022generative,
  title={Generative adversarial networks in medical image augmentation: a review},
  author={Chen, Yizhou and Yang, Xu-Hua and Wei, Zihan and Heidari, Ali Asghar and Zheng, Nenggan and Li, Zhicheng and Chen, Huiling and Hu, Haigen and Zhou, Qianwei and Guan, Qiu},
  journal={Computers in Biology and Medicine},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{Goodfellow2014GenerativeAN,
  title={Generative Adversarial Nets},
  author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron C. Courville and Yoshua Bengio},
  journal={Neural Information Processing Systems},
  year={2014},
    address = {Palais des Congrès de Montréal, Montréal CANADA},
  month = {Dec.},
  publisher = {NeurIPS},
}

@article{mirza2014conditional,
  title={Conditional generative adversarial nets},
  author={Mirza, Mehdi and Osindero, Simon},
  journal={arXiv preprint arXiv:1411.1784},
  year={2014}
}

@inproceedings{zhu2017unpaired,
  title={Unpaired image-to-image translation using cycle-consistent adversarial networks},
  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
  booktitle={International Conference on Computer Vision},
  year={2017},
  address = {Venice, Italy},
  month = {Oct.},
  publisher = {IEEE},
}

@inproceedings{tronchin2021evaluating,
  title={Evaluating GANs in medical imaging},
  author={Tronchin, Lorenzo and Sicilia, Rosa and Cordelli, Ermanno and Ramella, Sara and Soda, Paolo},
  booktitle={International Conference on Medical Image Computing and Computer Assisted Intervention},
  year={2021},
  publisher={Springer},
  month = {Oct.},
  address = {Strasbourg, France}
  
}

@inproceedings{Bermdez2018LearningIB,
  title={Learning implicit brain MRI manifolds with deep learning},
  author={Camilo Berm{\'u}dez and Andrew J. Plassard and Larry Taylor Davis and Allen T. Newton and Susan M. Resnick and Bennett A. Landman},
  booktitle={Medical Imaging},
  year={2018},
  publisher = {SPIE},
}

@article{Madani2018SemisupervisedLW,
  title={Semi-supervised learning with generative adversarial networks for chest X-ray classification with ability of data domain adaptation},
  author={Ali Madani and Mehdi Moradi and Alexandros Karargyris and Tanveer F. Syeda-Mahmood},
  journal={International Symposium on Biomedical Imaging},
  year={2018},
  publisher ={IEEE},
}

@article{Nie2018MedicalIS,
  title={Medical Image Synthesis with Deep Convolutional Adversarial Networks},
  author={Dong Nie and Roger Trullo and Jun Lian and Li Wang and Caroline Petitjean and Su Ruan and Qian Wang and Dinggang Shen},
  journal={IEEE Transactions on Biomedical Engineering},
  year={2018},
  volume={65},
  pages={2720-2730}
}

@article{privacy_medical,
  title={Publishing data from electronic health records while preserving privacy: a survey of algorithms.},
  author={Aris Gkoulalas-Divanis and Grigorios Loukides and Jimeng Sun},
  journal={Journal of Biomedical Informatics},
  year={2014}
}

@InProceedings{Wang_2022_ACCV,
    author    = {Wang, Zicong and Ren, Qiang and Wang, Junli and Yan, Chungang and Jiang, Changjun},
    title     = {MUSH: Multi-Scale Hierarchical Feature Extraction for Semantic Image Synthesis},
    booktitle = {Asian Conference on Computer Vision},
    month     = {Dec.},
    year      = {2022},
    pages     = {4126-4142}
}

@inproceedings{public_privacy,
  title={Privacy for free: How does dataset condensation help privacy?},
  author={Dong, Tian and Zhao, Bo and Lyu, Lingjuan},
  booktitle={International Conference on Machine Learning},
  pages={5378--5396},
  year={2022},
  publisher={PMLR},
  month = {Jul.},
  address = {Baltimore MD},
  
}

@article{compare_1,
  title={Synthetic medical images from dual generative adversarial networks},
  author={Guibas, John T and Virdi, Tejpal S and Li, Peter S},
  journal={arXiv preprint arXiv:1709.01872},
  year={2017}
}

@inproceedings{compre_2,
  title={Xlsor: A robust and accurate lung segmentor on chest x-rays using criss-cross attention and customized radiorealistic abnormalities generation},
  author={Tang, You-Bao and Tang, Yu-Xing and Xiao, Jing and Summers, Ronald M},
  booktitle={Medical Imaging with Deep Learning},
  pages={457--467},
  year={2019},
  publisher={PMLR},
  address = {London},
  month = {Jul.},
}

@inproceedings{attnunet,
  title={Attention U-Net: Learning Where to Look for the Pancreas},
  author={Oktay, Ozan and Schlemper, Jo and Le Folgoc, Loic and Lee, Matthew and Heinrich, Mattias and Misawa, Kazunari and Mori, Kensaku and McDonagh, Steven and Hammerla, Nils Y and Kainz, Bernhard and others},
  booktitle={Medical Imaging with Deep Learning},
  address = {Amsterdam},
  month = {Jul.},
  year = {2018},
  publisher = {OpenReview.net}
}

@article{transunet,
  title={Transunet: Transformers make strong encoders for medical image segmentation},
  author={Chen, Jieneng and Lu, Yongyi and Yu, Qihang and Luo, Xiangde and Adeli, Ehsan and Wang, Yan and Lu, Le and Yuille, Alan L and Zhou, Yuyin},
  journal={arXiv preprint arXiv:2102.04306},
  year={2021}
}

@inproceedings{iclr22_analytic,
  title={Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models},
  author={Bao, Fan and Li, Chongxuan and Zhu, Jun and Zhang, Bo},
  booktitle={International Conference on Learning Representations},
  address = {Virtual Event},
  month = {Apr.},
  year = {2022},
  publisher = {OpenReview.net},
}

@inproceedings{cvpr22_sd,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Computer Vision and Pattern Recognition},
  pages={10684--10695},
  publisher = {IEEE},
  year={2022}
}

@inproceedings{iclr22_progressive,
  title={Progressive Distillation for Fast Sampling of Diffusion Models},
  author={Salimans, Tim and Ho, Jonathan},
  booktitle={International Conference on Learning Representations},
  publisher = {OpenReview.net},
  address = {Virtual Event},
  month = {Apr.},
  year = {2022},
}

@article{icme22_kd_classifier_based,
  title={Accelerating Diffusion Sampling with Classifier-based Feature Distillation},
  author={Sun, Wujie and Chen, Defang and Wang, Can and Ye, Deshi and Feng, Yan and Chen, Chun},
  journal={arXiv preprint arXiv:2211.12039},
  year={2022}
}

@article{cvpr22_kd_guided,
  title={On distillation of guided diffusion models},
  author={Meng, Chenlin and Gao, Ruiqi and Kingma, Diederik P and Ermon, Stefano and Ho, Jonathan and Salimans, Tim},
  journal={arXiv preprint arXiv:2210.03142},
  year={2022}
}

@inproceedings{jacob2018quantization,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2704--2713},
  year={2018}
}

@article{arxiv21_kd_diffusion,
  title={Knowledge distillation in iterative generative models for improved sampling speed},
  author={Luhman, Eric},
  journal={arXiv preprint arXiv:2101.02388},
  year={2021}
}

@inproceedings{iclr2020_sam,
  title={Sharpness-aware Minimization for Efficiently Improving Generalization},
  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  booktitle={International Conference on Learning Representations},
  year={2020},
}

@inproceedings{nips2022_sam,
  title={Sharpness-aware training for free},
  author={Du, Jiawei and Zhou, Daquan and Feng, Jiashi and Tan, Vincent and Zhou, Joey Tianyi},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  publisher = {NeurIPS},
  pages={23439--23451},
  address = {New Orleans, Louisiana, USA},
  month = {Dec.},
  year={2022}
}

@inproceedings{RDED,
  title={On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm},
  author={Sun, Peng and Shi, Bei and Yu, Daiwei and Lin, Tao},
  booktitle={Computer Vision and Pattern Recognition},
  publisher = {IEEE},
  year={2024}
}

@InProceedings{LPIPS,
author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
title = {The Unreasonable Effectiveness of Deep Features as a Perceptual Metric},
booktitle = {Computer Vision and Pattern Recognition},
month = {June},
year = {2018},
address = {Salt Lake City, Utah, USA},
publisher = {IEEE}
}

@InProceedings{runge_kutta,
author = {Runge, C.},
title = {Ueber die numerische Aufl\"{o}sung von Differentialgleichungen},
month = {Jun.},
year = {1895},
journal = {Mathematische Annalen},
pages = {1432-1807},
}

@inproceedings{2021pmlr_improved,
  title={Improved denoising diffusion probabilistic models},
  author={Nichol, Alexander Quinn and Dhariwal, Prafulla},
  booktitle={International Conference on Machine Learning},
  pages={8162--8171},
  year={2021},
  publisher={PMLR}
}

@inproceedings{vae_1,
  title={Grammar variational autoencoder},
  author={Kusner, Matt J and Paige, Brooks and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel},
  booktitle={International Conference on Machine Learning},
  pages={1945--1954},
  year={2017},
  organization={PMLR}
}

@inproceedings{vae_2,
  title={NVAE: A deep hierarchical variational autoencoder},
  author={Vahdat, Arash and Kautz, Jan},
  journal={Neural Information Processing Systems},
  volume={33},
  pages={19667--19679},
  year={2020},
  month = {Dec.},
  address = {Virtual Event},
  publisher = {NeurIPS},
}

@article{vae_3,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@inproceedings{nips2021_v_diffusion_model,
  title={Variational diffusion models},
  author={Kingma, Diederik and Salimans, Tim and Poole, Ben and Ho, Jonathan},
  booktitle={Neural Information Processing Systems},
  volume={34},
  publisher = {NeurIPS},
  address={Virtual Event},
  pages={21696--21707},
  year={2021}
}

@inproceedings{nips2021_classifier_free_guidance,
title={Classifier-Free Diffusion Guidance},
author={Jonathan Ho and Tim Salimans},
booktitle={Neural Information Processing Systems Workshop},
year={2021},
month={Dec.},
address={Virtual Event},
publisher = {NeurIPS},
}

@article{ostle1963statistics,
  title={Statistics in research.},
  author={Ostle, Bernard and others},
  journal={Statistics in research.},
  number={2nd Ed},
  year={1963},
  publisher={The Iowa State University Press, Ames.}
}

@inproceedings{audio_1,
title={DiffWave: A Versatile Diffusion Model for Audio Synthesis},
author={Zhifeng Kong and Wei Ping and Jiaji Huang and Kexin Zhao and Bryan Catanzaro},
booktitle={International Conference on Learning Representations},
year={2021},
publisher = {OpenReview.net},
month = {May},
address={Virtual Event},
}

@article{nc_sr_diffusion_model,
title = {SRDiff: Single image super-resolution with diffusion probabilistic models},
journal = {Neurocomputing},
volume = {479},
pages = {47-59},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.01.029},
author = {Haoying Li and Yifan Yang and Meng Chang and Shiqi Chen and Huajun Feng and Zhihai Xu and Qi Li and Yueting Chen},
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@inproceedings{ncsn,
  author       = {Yang Song and
                  Stefano Ermon},
  title        = {Generative Modeling by Estimating Gradients of the Data Distribution},
  booktitle    = {Neural Information Processing Systems},
  pages        = {11895--11907},
  year         = {2019},
  month = {Dec.},
  address = {Vancouver, BC, Canada},
}

@inproceedings{VID,
  author       = {Sungsoo Ahn and
                  Shell Xu Hu and
                  Andreas C. Damianou and
                  Neil D. Lawrence and
                  Zhenwen Dai},
  title        = {Variational Information Distillation for Knowledge Transfer},
  booktitle    = {Computer Vision and Pattern Recognition},
  pages        = {9163--9171},
  address      = {Long Beach, CA, USA},
  month        = {Jun.},
  publisher    = {Computer Vision Foundation / {IEEE}},
  year         = {2019},
}

@article{DiffKD,
  title={Knowledge Diffusion for Distillation},
  author={Huang, Tao and Zhang, Yuan and Zheng, Mingkai and You, Shan and Wang, Fei and Qian, Chen and Xu, Chang},
  journal={arXiv preprint arXiv:2305.15712},
  year={2023}
}

@inproceedings{GSD,
  author       = {Wei Huang and
                  Zhiliang Peng and
                  Li Dong and
                  Furu Wei and
                  Jianbin Jiao and
                  Qixiang Ye},
  title        = {Generic-to-Specific Distillation of Masked Autoencoders},
  booktitle    = {Computer Vision and Pattern Recognition},
  year        = {2022},
  month        = {Jun.},
  address      = {New Orleans, LA, USA},
  

}

@inproceedings{Maskd,
  author       = {Zhendong Yang and
                  Zhe Li and
                  Mingqi Shao and
                  Dachuan Shi and
                  Zehuan Yuan and
                  Chun Yuan},
  title        = {Masked Generative Distillation},
  booktitle    = {European Conference Computer Vision},
  address      = {Tel Aviv, Israel},
  month        = {Oct.},
  volume       = {13671},
  pages        = {53--69},
  publisher    = {Springer},
  year         = {2022},
}

@inproceedings{AHBF-OKD, title={Adaptive Hierarchy-Branch Fusion for Online Knowledge Distillation}, volume={37}, number={6}, booktitle={Association for the Advancement of Artificial Intelligence}, author={Gong, Linrui and Lin, Shaohui and Zhang, Baochang and Shen, Yunhang and Li, Ke and Qiao, Ruizhi and Ren, Bo and Li, Muqing and Yu, Zhou and Ma, Lizhuang}, year={2023}, month={Jun.}, pages={7731-7739},address={Washington, DC, USA},publisher={AAAI}}

@inproceedings{CL,
  author    = {Guocong Song and
               Wei Chai},
  title     = {Collaborative Learning for Deep Neural Networks},
  booktitle = {Neural Information Processing Systems},
  publisher = {MIT Press},
  address   = {Montréal Canada},
  month     = {Dec.},
  pages     = {1837--1846},
  year      = {2018}
}

@inproceedings{ONE,
  author    = {Xu Lan and
               Xiatian Zhu and
               Shaogang Gong},
  title     = {Knowledge Distillation by On-the-Fly Native Ensemble},
  booktitle = {Neural Information Processing Systems},
  publisher = {MIT Press},
  address   = {Montréal Canada},
  month     = {Dec.},
  pages     = {7528--7538},
  year      = {2018},
}

@inproceedings{tvm,
  title={$\{$TVM$\}$: An automated $\{$End-to-End$\}$ optimizing compiler for deep learning},
  author={Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and others},
  booktitle={Operating Systems Design and Implementation},
  month={CARLSBAD, CA, USA},
  publisher={USENIX},
  pages={578--594},
  year={2018}
}

@article{FFSD,
  author={Li, Shaojie and Lin, Mingbao and Wang, Yan and Wu, Yongjian and Tian, Yonghong and Shao, Ling and Ji, Rongrong},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Distilling a Powerful Student Model via Online Knowledge Distillation}, 
  year={2022},
  pages={1-10}
}

## Related Work belong to Dataset Distillation

@inproceedings{dd_remember_the_past,
  author       = {Zhiwei Deng and
                  Olga Russakovsky},
  title        = {Remember the Past: Distilling Datasets into Addressable Memories for Neural Networks},
  booktitle    = {Neural Information Processing Systems},
  year         = {2022},
  address      = {New Orleans, LA, USA},
  month        = {Nov.},
  publisher    = {NeurIPS},
}

@misc{
yin2024dataset,
title={Dataset Distillation in Large Data Era},
author={Zeyuan Yin and Zhiqiang Shen},
year={2024},
}

@article{FM_KT,
	title = {Precise Knowledge Transfer via Flow Matching},
	author = {Shao, Shitong and Shen, Zhiqiang and Gong, Linrui and Chen, Huanran and Dai, Xu},
	year = {2024},
    journal = {arXiv preprint arXiv:2402.02012},
}

@inproceedings{dataset_quantization,
  title={Dataset quantization},
  author={Zhou, Daquan and Wang, Kai and Gu, Jianyang and Peng, Xiangyu and Lian, Dongze and Zhang, Yifan and You, Yang and Feng, Jiashi},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={17205--17216},
  year={2023}
}

@inproceedings{dd_efficient_parameterization,
  author       = {Jang{-}Hyun Kim and
                  Jinuk Kim and
                  Seong Joon Oh and
                  Sangdoo Yun and
                  Hwanjun Song and
                  Joonhyun Jeong and
                  Jung{-}Woo Ha and
                  Hyun Oh Song},
  title        = {Dataset Condensation via Efficient Synthetic-Data Parameterization},
  booktitle    = {International Conference on Machine Learning},
  address      = {Baltimore, Maryland, {USA}},
  month        = {Jul.},
  volume       = {162},
  pages        = {11102--11118},
  publisher    = {{PMLR}},
  year         = {2022},
}

@inproceedings{dd_minimizing_acc_traj_error,
  author       = {Jiawei Du and
                  Yidi Jiang and
                  Vincent Y. F. Tan and
                  Joey Tianyi Zhou and
                  Haizhou Li},
  title        = {Minimizing the Accumulated Trajectory Error to Improve Dataset Distillation},
  booktitle    = {Computer Vision and Pattern Recognition},
  address      = {Vancouver, BC, Canada},
  month        = {Jun.},
  pages        = {3749--3758},
  publisher    = {{IEEE}},
  year         = {2023},
}

@inproceedings{dd_mtt,
  author       = {George Cazenavette and
                  Tongzhou Wang and
                  Antonio Torralba and
                  Alexei A. Efros and
                  Jun{-}Yan Zhu},
  title        = {Dataset Distillation by Matching Training Trajectories},
  booktitle    = {Computer Vision and Pattern Recognition},
  address      = {New Orleans, LA, USA},
  month        = {Jun.},
  publisher    = {{IEEE}},
  year         = {2022}}

@inproceedings{dd_convex_implicit_gradient,
  author       = {Noel Loo and
                  Ramin M. Hasani and
                  Mathias Lechner and
                  Daniela Rus},
  title        = {Dataset Distillation with Convexified Implicit Gradients},
  booktitle    = {International Conference on Machine Learning},
  address      = {Honolulu, Hawaii, {USA}},
  month        = {Jul.},
  volume       = {202},
  pages        = {22649--22674},
  publisher    = {{PMLR}},
  year         = {2023},
}

@article{dd_fix_dd_attack,
  title={Dataset Distillation Fixes Dataset Reconstruction Attacks},
  author={Loo, Noel and Hasani, Ramin and Lechner, Mathias and Rus, Daniela},
  journal={arXiv preprint arXiv:2302.01428},
  year={2023}
}

@article{dd_pro_subset_selection,
  title={Dataset Distillation Meets Provable Subset Selection},
  author={Tukan, Murad and Maalouf, Alaa and Osadchy, Margarita},
  journal={arXiv preprint arXiv:2307.08086},
  year={2023}
}

@inproceedings{dd_FRePo,
  author       = {Yongchao Zhou and
                  Ehsan Nezhadarya and
                  Jimmy Ba},
  title        = {Dataset Distillation using Neural Feature Regression},
  booktitle    = {Neural Information Processing Systems},
  year         = {2022},
  address      = {New Orleans, LA, USA},
  month        = {Nov.},
  publisher    = {NeurIPS},
}

@inproceedings{dd_model_augmentation,
  author       = {Lei Zhang and
                  Jie Zhang and
                  Bowen Lei and
                  Subhabrata Mukherjee and
                  Xiang Pan and
                  Bo Zhao and
                  Caiwen Ding and
                  Yao Li and
                  Dongkuan Xu},
  title        = {Accelerating Dataset Distillation via Model Augmentation},
  booktitle    = {Computer Vision and Pattern Recognition},
  address      = {Vancouver, BC, Canada},
  month        = {Jun.},
  publisher    = {{IEEE}},
  year         = {2023},
}

@inproceedings{dd_datadam,
  title={DataDAM: Efficient Dataset Distillation with Attention Matching},
  author={Sajedi, Ahmad and Khaki, Samir and Amjadian, Ehsan and Liu, Lucy Z and Lawryshyn, Yuri A and Plataniotis, Konstantinos N},
  booktitle={International Conference on Computer Vision},
  pages={17097--17107},
  year={2023},
  month = {Oct.},
  address = {Paris, France},
  publisher = {{IEEE}},
}

@article{dd_dream,
  author       = {Yanqing Liu and
                  Jianyang Gu and
                  Kai Wang and
                  Zheng Zhu and
                  Wei Jiang and
                  Yang You},
  title        = {{DREAM:} Efficient Dataset Distillation by Representative Matching},
  journal={arXiv preprint arXiv:2302.14416},
  year = {2023}
}

@article{dd_comprehensive_review,
  author       = {Ruonan Yu and
                  Songhua Liu and
                  Xinchao Wang},
  title        = {Dataset Distillation: {A} Comprehensive Review},
  journal      = {arXiv preprint arXiv:2301.07014},
  year         = {2023},
}

@article{dd_survey,
  title={Data distillation: A survey},
  author={Sachdeva, Noveen and McAuley, Julian},
  journal={arXiv preprint arXiv:2301.04272},
  year={2023}
}

@article{dd_begin,
  title={Dataset distillation},
  author={Wang, Tongzhou and Zhu, Jun-Yan and Torralba, Antonio and Efros, Alexei A},
  journal={arXiv preprint arXiv:1811.10959},
  year={2018}
}

@inproceedings{dd_gradient_matching,
  author       = {Bo Zhao and
                  Konda Reddy Mopuri and
                  Hakan Bilen},
  title        = {Dataset Condensation with Gradient Matching},
  booktitle    = {International Conference on Learning Representations},
  address      = {Virtual Event},
  month        = {May},
  publisher    = {OpenReview.net},
  year         = {2021},
}

@inproceedings{dd_dist_matching,
  title={Dataset condensation with distribution matching},
  author={Zhao, Bo and Bilen, Hakan},
  booktitle={Winter Conference on Applications of Computer Vision},
  pages={6514--6523},
  year={2023},
  publisher={{IEEE}},
  address={Waikoloa, Hawaii},
  month={Jan.}
}

@inproceedings{dd_RFAD,
  author       = {Noel Loo and
                  Ramin M. Hasani and
                  Alexander Amini and
                  Daniela Rus},
  title        = {Efficient Dataset Distillation using Random Feature Approximation},
  booktitle    = {International Conference on Learning Representations},
  address      = {Virtual Event},
  month        = {May},
  publisher    = {OpenReview.net},
  year         = {2021},
}

@article{dd_parameter_pruning,
  title={Dataset distillation using parameter pruning},
  author={Li, Guang and Togo, Ren and Ogawa, Takahiro and Haseyama, Miki},
  journal={IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences},
  year={2023},
  publisher={The Institute of Electronics, Information and Communication Engineers}
}

@inproceedings{dd_infinitely_wide_net,
  author       = {Timothy Nguyen and
                  Roman Novak and
                  Lechao Xiao and
                  Jaehoon Lee},
  title        = {Dataset Distillation with Infinitely Wide Convolutional Networks},
  booktitle    = {Neural Information Processing Systems},
  address      = {Virtual Event},
  month        = {Dec.},
  pages        = {5186--5198},
  year         = {2021},
}

@inproceedings{find_example,
  author       = {Mansheej PaulSurya and GanguliGintare and Karolina Dziugaite},
  title        = {Deep Learning on a Data Diet: Finding Important Examples Early in Training},
  booktitle    = {Neural Information Processing Systems},
  address      = {Virtual Event},
  month        = {Dec.},
  year         = {2021},
}

@inproceedings{liu2017learning,
  title={Learning efficient convolutional networks through network slimming},
  author={Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
  booktitle={International Conference on Computer Vision},
  publisher={IEEE},
  pages={2736--2744},
  year={2017}
}

@inproceedings{Prabhu_Torr_Dokania_2020,  
 title={GDumb: A Simple Approach that Questions Our Progress in Continual Learning},
 booktitle={European Conference on Computer Vision}, 
 author={Prabhu, Ameya and Torr, Philip H. S. and Dokania, Puneet K.},
 publisher={Springer},
 year={2020}, 
 month={Jan}, 
 pages={524–540}, 
 language={en-US} 
 }

@article{srinivas2015data,
  title={Data-free parameter pruning for deep neural networks},
  author={Srinivas, Suraj and Babu, R Venkatesh},
  journal={arXiv preprint arXiv:1507.06149},
  year={2015}
}

@inproceedings{dd_model_prior,
  author       = {George Cazenavette and
                  Tongzhou Wang and
                  Antonio Torralba and
                  Alexei A. Efros and
                  Jun{-}Yan Zhu},
  title        = {Generalizing Dataset Distillation via Deep Generative Prior},
  booktitle    = {Computer Vision and Pattern Recognition},

  address      = {Vancouver, BC, Canada}, 
  month        = {June},
  pages        = {3739--3748},
  publisher    = {{IEEE}},
  year         = {2023},
}

@article{dd_federated_learning_1,
  author       = {Shengyuan Hu and
                  Jack Goetz and
                  Kshitiz Malik and
                  Hongyuan Zhan and
                  Zhe Liu and
                  Yue Liu},
  title        = {FedSynth: Gradient Compression via Synthetic Data in Federated Learning},
  journal      = {arXiv preprint arXiv:2204.01273},
  year         = {2022}
}

@inproceedings{dd_federated_learning_2,
  author       = {Rui Song and
                  Dai Liu and
                  Dave Zhenyu Chen and
                  Andreas Festag and
                  Carsten Trinitis and
                  Martin Schulz and
                  Alois Knoll},
  title        = {Federated Learning via Decentralized Dataset Distillation in Resource-Constrained
                  Edge Environments},
  booktitle    = {International Joint Conference on Neural Networks},
  address      = {Gold Coast, Australia},
  month        = {Jun.},
  pages        = {1--10},
  publisher    = {{IEEE}},
  year         = {2023},
}

@inproceedings{dd_continual_learning_1,
  title={Reducing catastrophic forgetting with learning on synthetic data},
  author={Masarczyk, Wojciech and Tautkute, Ivona},
  booktitle={Computer Vision and Pattern Recognition Workshops},
  pages={252--253},
  year={2020},
  address={Virtual Event},
  month={Jun.},
  publisher={{IEEE}}
}

@inproceedings{dd_continual_learning_2,
  author       = {Mattia Sangermano and
                  Antonio Carta and
                  Andrea Cossu and
                  Davide Bacciu},
  title        = {Sample Condensation in Online Continual Learning},
  booktitle    = {International Joint Conference on Neural Networks},
  address      = {Padua, Italy},
  month        = {Jul.},
  pages        = {1--8},
  publisher    = {{IEEE}},
  year         = {2022},
}

@inproceedings{dd_continual_learning_3,
  author       = {Bo Zhao and
                  Hakan Bilen},
  editor       = {Marina Meila and
                  Tong Zhang},
  title        = {Dataset Condensation with Differentiable Siamese Augmentation},
  booktitle    = {International Conference on Machine Learning},
  address      = {Virtual Event},
  volume       = {139},
  pages        = {12674--12685},
  publisher    = {{PMLR}},
  year         = {2021},
}

@inproceedings{dd_nas_1,
  author       = {Felipe Petroski Such and
                  Aditya Rawal and
                  Joel Lehman and
                  Kenneth O. Stanley and
                  Jeffrey Clune},
  title        = {Generative Teaching Networks: Accelerating Neural Architecture Search by Learning to Generate Synthetic Training Data},
  booktitle    = {International Conference on Machine Learning},
  address      = {Virtual Event},
  month        = {Jul.},
  volume       = {119},
  pages        = {9206--9216},
  publisher    = {{PMLR}},
  year         = {2020},
}

@article{dd_gcn_1,
  title={Graph condensation for graph neural networks},
  author={Jin, Wei and Zhao, Lingxiao and Zhang, Shichang and Liu, Yozen and Tang, Jiliang and Shah, Neil},
  journal={arXiv preprint arXiv:2110.07580},
  year={2021}
}

@article{dd_gcn_2,
  title={Graph condensation via receptive field distribution matching},
  author={Liu, Mengyang and Li, Shanchuan and Chen, Xinshi and Song, Le},
  journal={arXiv preprint arXiv:2206.13697},
  year={2022}
}

@inproceedings{dd_3d_1,
  title={ToThePoint: Efficient Contrastive Learning of 3D Point Clouds via Recycling},
  author={Li, Xinglin and Chen, Jiajing and Ouyang, Jinhui and Deng, Hanhui and Velipasalar, Senem and Wu, Di},
  booktitle={Computer Vision and Pattern Recognition},
  pages={21781--21790},
  year={2023},
  address      = {Vancouver, BC, Canada}, 
  month        = {June},
  publisher    = {{IEEE}},}

@inproceedings{dd_tesla,
  author       = {Justin Cui and
                  Ruochen Wang and
                  Si Si and
                  Cho{-}Jui Hsieh},
  title        = {Scaling Up Dataset Distillation to ImageNet-1K with Constant Memory},
  booktitle    = {International Conference on Machine Learning},
  address      = {Honolulu, Hawaii, {USA}},
  volume       = {202},
  pages        = {6565--6590},
  publisher    = {{PMLR}},
  year         = {2023},
}

@article{dd_kip,
  title={Dataset meta-learning from kernel ridge-regression},
  author={Nguyen, Timothy and Chen, Zhourong and Lee, Jaehoon},
  journal={arXiv preprint arXiv:2011.00050},
  year={2020}
}

@inproceedings{dd_CAFE,
  title={Cafe: Learning to condense dataset by aligning features},
  author={Wang, Kai and Zhao, Bo and Peng, Xiangyu and Zhu, Zheng and Yang, Shuo and Wang, Shuo and Huang, Guan and Bilen, Hakan and Wang, Xinchao and You, Yang},
  booktitle={Computer Vision and Pattern Recognition},
  pages={12196--12205},
  address      = {New Orleans, LA, USA},
  month        = {Jun.},
  publisher    = {{IEEE}},
  year         = {2022}}

@inproceedings{deepinversion,
  title={Dreaming to distill: Data-free knowledge transfer via deepinversion},
  author={Yin, Hongxu and Molchanov, Pavlo and Alvarez, Jose M and Li, Zhizhong and Mallya, Arun and Hoiem, Derek and Jha, Niraj K and Kautz, Jan},
  booktitle={Computer Vision and Pattern Recognition},
  pages={8715--8724},
  year={2020},
  address={Virtual Event},
  month={Jun.},
  publisher={{IEEE}}
}

@inproceedings{kim2021comparing,
  title={Comparing kullback-leibler divergence and mean squared error loss in knowledge distillation},
  author={Kim, Taehyeon and Oh, Jaehoon and Kim, NakYil and Cho, Sangwook and Yun, Se-Young},
  booktitle={International Joint Conference on Artificial Intelligence},
  address={Virtual Event},
  month={Aug.},
  publisher={Morgan Kaufmann},
  year={2021}
}

@inproceedings{deit,
  author       = {Hugo Touvron and
                  Matthieu Cord and
                  Matthijs Douze and
                  Francisco Massa and
                  Alexandre Sablayrolles and
                  Herv{\'{e}} J{\'{e}}gou},
  editor       = {Marina Meila and
                  Tong Zhang},
  title        = {Training data-efficient image transformers {\&} distillation through
                  attention},
  booktitle    = {International Conference on Machine Learning},
  address      = {Virtual Event},
  month        = {Jul.},
  volume       = {139},
  pages        = {10347--10357},
  publisher    = {{PMLR}},
  year         = {2021},
}

@inproceedings{pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  booktitle    = {Neural Information Processing Systems},
  year         = {2019},
  month = {Dec.},
  address = {Vancouver, BC, Canada},
}

@article{bohdal2020flexible,
  title={Flexible Dataset Distillation: Learn Labels Instead of Images},
  author={Bohdal, Ondrej and Yang, Yongxin and Hospedales, Timothy},
  journal={arXiv preprint arXiv:2006.08572},
  year={2020}
}

@inproceedings{multi_teacher,
  author       = {Shan You and
                  Chang Xu and
                  Chao Xu and
                  Dacheng Tao},
  title        = {Learning from Multiple Teacher Networks},
  booktitle    = {International Conference on
                  Knowledge Discovery and Data Mining},
  address      = {Halifax, NS, Canada}, 
  month        = {Aug.},
  pages        = {1285--1294},
  publisher    = {{ACM}},
  year         = {2017},
}

@article{van2008visualizing,
  title={Visualizing data using t-{SNE}.},
  author={Van der Maaten, Laurens and Hinton, Geoffrey},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={11},
  year={2008}
}

@article{shao2023generalized,
	title = {Generalized Large-Scale Data Condensation via Various Backbone and Statistical Matching},
	author = {Shao, Shitong and Yin, Zeyuan and Zhang, Xindong and Shen, Zhiqiang},
	year = {2023},
    journal = {arXiv preprint arXiv:2311.17950},
}

@inproceedings{T_SEA,
  title={T-sea: Transfer-based self-ensemble attack on object detection},
  author={Huang, Hao and Chen, Ziyan and Chen, Huanran and Wang, Yongtao and Zhang, Kevin},
  booktitle={Computer Vision and Pattern Recognition},
  publisher={IEEE},
  pages={20514--20523},
  year={2023}
}

@article{zhou2024self,
  title={Self-supervised Dataset Distillation: A Good Compression Is All You Need},
  author={Zhou, Muxin and Yin, Zeyuan and Shao, Shitong and Shen, Zhiqiang},
  journal={arXiv preprint arXiv:2404.07976},
  year={2024}
}

@inproceedings{eccvw_sam,
  title={Bootstrap Generalization Ability from Loss Landscape Perspective},
  author={Chen, Huanran and Shao, Shitong and Wang, Ziyi and Shang, Zirui and Chen, Jin and Ji, Xiaofeng and Wu, Xinxiao},
  booktitle={European Conference on Computer Vision},
  pages={500--517},
  year={2022},
  organization={Springer}
}

@article{sam_llm,
  title={Sharpness-aware minimization improves language model generalization},
  author={Bahri, Dara and Mobahi, Hossein and Tay, Yi},
  journal={arXiv preprint arXiv:2110.08529},
  year={2021}
}

@inproceedings{cwa,
  title={Rethinking Model Ensemble in Transfer-based Adversarial Attacks},
  author={Chen, Huanran and Zhang, Yichi and Dong, Yinpeng and Zhu, Jun},
  booktitle={International Conference on Learning Representations },
  year={2024},
  publisher = {OpenReview.net},
  address = {Vienna, Austria},
  month = {May},
}

@inproceedings{lu2022maximum,
  title={Maximum likelihood training for score-based diffusion odes by high order denoising score matching},
  author={Lu, Cheng and Zheng, Kaiwen and Bao, Fan and Chen, Jianfei and Li, Chongxuan and Zhu, Jun},
  booktitle={International Conference on Machine Learning},
  pages={14429--14460},
  year={2022},
  organization={PMLR}
}

@inproceedings{nieblessing,
  title={The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing},
  author={Nie, Shen and Guo, Hanzhong Allan and Lu, Cheng and Zhou, Yuhao and Zheng, Chenyu and Li, Chongxuan},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{shao2023catch,
  title={Catch-up distillation: You only need to train once for accelerating sampling},
  author={Shao, Shitong and Dai, Xu and Yin, Shouyi and Li, Lujun and Chen, Huanran and Hu, Yang},
  journal={arXiv preprint arXiv:2305.10769},
  year={2023}
}

@inproceedings{SDXL,
  title={SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis},
  author={Podell, Dustin and English, Zion and Lacey, Kyle and Blattmann, Andreas and Dockhorn, Tim and M{\"u}ller, Jonas and Penna, Joe and Rombach, Robin},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@article{modelscope,
  title={Modelscope text-to-video technical report},
  author={Wang, Jiuniu and Yuan, Hangjie and Chen, Dayou and Zhang, Yingya and Wang, Xiang and Zhang, Shiwei},
  journal={arXiv preprint arXiv:2308.06571},
  year={2023}
}

@article{animatediff,
  title={Animatediff: Animate your personalized text-to-image diffusion models without specific tuning},
  author={Guo, Yuwei and Yang, Ceyuan and Rao, Anyi and Liang, Zhengyang and Wang, Yaohui and Qiao, Yu and Agrawala, Maneesh and Lin, Dahua and Dai, Bo},
  journal={arXiv preprint arXiv:2307.04725},
  year={2023}
}

@article{sun2024t2v,
  title={T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video Generation},
  author={Sun, Kaiyue and Huang, Kaiyi and Liu, Xian and Wu, Yue and Xu, Zihan and Li, Zhenguo and Liu, Xihui},
  journal={arXiv preprint arXiv:2407.14505},
  year={2024}
}

@article{yuan2024chronomagic,
  title={ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation},
  author={Yuan, Shenghai and Huang, Jinfa and Xu, Yongqi and Liu, Yaoyang and Zhang, Shaofeng and Shi, Yujun and Zhu, Ruijie and Cheng, Xinhua and Luo, Jiebo and Yuan, Li},
  journal={arXiv preprint arXiv:2406.18522},
  year={2024}
}

@article{chen2024unictrl,
  title={UniCtrl: Improving the Spatiotemporal Consistency of Text-to-Video Diffusion Models via Training-Free Unified Attention Control},
  author={Chen, Xuweiyi and Xia, Tian and Xu, Sihan},
  journal={arXiv preprint arXiv:2403.02332},
  year={2024}
}

@article{dpm_solver_v3,
  title={Dpm-solver-v3: Improved diffusion ode solver with empirical model statistics},
  author={Zheng, Kaiwen and Lu, Cheng and Chen, Jianfei and Zhu, Jun},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={55502--55542},
  year={2023}
}

@article{RoPE,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@article{VQVAE,
  title={Neural discrete representation learning},
  author={Van Den Oord, Aaron and Vinyals, Oriol and others},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{teng2024accelerating,
  title={Accelerating auto-regressive text-to-image generation with training-free speculative jacobi decoding},
  author={Teng, Yao and Shi, Han and Liu, Xian and Ning, Xuefei and Dai, Guohao and Wang, Yu and Li, Zhenguo and Liu, Xihui},
  journal={arXiv preprint arXiv:2410.01699},
  year={2024}
}

@inproceedings{randomsmoothing,
  title={Certified adversarial robustness via randomized smoothing},
  author={Cohen, Jeremy and Rosenfeld, Elan and Kolter, Zico},
  booktitle={international conference on machine learning},
  pages={1310--1320},
  year={2019},
  organization={PMLR}
}

@inproceedings{mao2023guided,
  title={Guided image synthesis via initial image editing in diffusion model},
  author={Mao, Jiafeng and Wang, Xueting and Aizawa, Kiyoharu},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={5321--5329},
  year={2023}
}

@inproceedings{shirakawa2024noisecollage,
  title={NoiseCollage: A Layout-Aware Text-to-Image Diffusion Model Based on Noise Cropping and Merging},
  author={Shirakawa, Takahiro and Uchida, Seiichi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8921--8930},
  year={2024}
}

@inproceedings{ddiminversion,
  title={Null-text inversion for editing real images using guided diffusion models},
  author={Mokady, Ron and Hertz, Amir and Aberman, Kfir and Pritch, Yael and Cohen-Or, Daniel},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6038--6047},
  year={2023}
}

@inproceedings{SDV3,
  title={Scaling rectified flow transformers for high-resolution image synthesis},
  author={Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\"u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@misc{erm,
  title={The nature of statistical learning theory},
  author={Sain, Stephan R},
  year={1996},
  publisher={Taylor \& Francis}
}

@article{liu2023dataset,
  title={Dataset Distillation via the Wasserstein Metric},
  author={Liu, Haoyang and Xing, Tiancheng and Li, Luwei and Dalal, Vibhu and He, Jingrui and Wang, Haohan},
  journal={arXiv preprint arXiv:2311.18531},
  year={2023}
}

% video diffusion models

@article{chen2023videocrafter1,
  title={Videocrafter1: Open diffusion models for high-quality video generation},
  author={Chen, Haoxin and Xia, Menghan and He, Yingqing and Zhang, Yong and Cun, Xiaodong and Yang, Shaoshu and Xing, Jinbo and Liu, Yaofang and Chen, Qifeng and Wang, Xintao and others},
  journal={arXiv preprint arXiv:2310.19512},
  year={2023}
}

@article{singer2022make,
  title={Make-a-video: Text-to-video generation without text-video data},
  author={Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and others},
  journal={arXiv preprint arXiv:2209.14792},
  year={2022}
}

@article{bao2024vidu,
  title={Vidu: a highly consistent, dynamic and skilled text-to-video generator with diffusion models},
  author={Bao, Fan and Xiang, Chendong and Yue, Gang and He, Guande and Zhu, Hongzhou and Zheng, Kaiwen and Zhao, Min and Liu, Shilong and Wang, Yaole and Zhu, Jun},
  journal={arXiv preprint arXiv:2405.04233},
  year={2024}
}

@article{soomro2012ucf101,
  title={UCF101: A dataset of 101 human actions classes from videos in the wild},
  author={Soomro, K},
  journal={arXiv preprint arXiv:1212.0402},
  year={2012}
}

@inproceedings{DIT,
  title={Scalable diffusion models with transformers},
  author={Peebles, William and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4195--4205},
  year={2023}
}

@article{freeinit,
  title={Freeinit: Bridging initialization gap in video diffusion models},
  author={Wu, Tianxing and Si, Chenyang and Jiang, Yuming and Huang, Ziqi and Liu, Ziwei},
  journal={arXiv preprint arXiv:2312.07537},
  year={2023}
}

@inproceedings{bain2021frozen,
  title={Frozen in time: A joint video and image encoder for end-to-end retrieval},
  author={Bain, Max and Nagrani, Arsha and Varol, G{\"u}l and Zisserman, Andrew},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1728--1738},
  year={2021}
}

@article{schuhmann2022laion,
  title={Laion-5b: An open large-scale dataset for training next generation image-text models},
  author={Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={25278--25294},
  year={2022}
}

@inproceedings{yang2020randomized,
  title={Randomized smoothing of all shapes and sizes},
  author={Yang, Greg and Duan, Tony and Hu, J Edward and Salman, Hadi and Razenshteyn, Ilya and Li, Jerry},
  booktitle={International Conference on Machine Learning},
  pages={10693--10705},
  year={2020},
  organization={PMLR}
}

@book{optimization_theory,
  title={Optimization theory with applications},
  author={Pierre, Donald A},
  year={1986},
  publisher={Courier Corporation}
}

@article{Adam,
  title={Adam: A method for stochastic optimization},
  author={Diederik, P Kingma},
  journal={(No Title)},
  year={2014}
}

@article{UMT_FVD,
  title={Fetv: A benchmark for fine-grained evaluation of open-domain text-to-video generation},
  author={Liu, Yuanxin and Li, Lei and Ren, Shuhuai and Gao, Rundong and Li, Shicheng and Chen, Sishuo and Sun, Xu and Hou, Lu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{li2023unmasked,
  title={Unmasked teacher: Towards training-efficient video foundation models},
  author={Li, Kunchang and Wang, Yali and Li, Yizhuo and Wang, Yi and He, Yinan and Wang, Limin and Qiao, Yu},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={19948--19960},
  year={2023}
}

@article{GPT4,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{LLAVA,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@article{hessel2021clipscore,
  title={Clipscore: A reference-free evaluation metric for image captioning},
  author={Hessel, Jack and Holtzman, Ari and Forbes, Maxwell and Bras, Ronan Le and Choi, Yejin},
  journal={arXiv preprint arXiv:2104.08718},
  year={2021}
}

@article{unterthiner2019fvd,
  title={FVD: A new metric for video generation},
  author={Unterthiner, Thomas and van Steenkiste, Sjoerd and Kurach, Karol and Marinier, Rapha{\"e}l and Michalski, Marcin and Gelly, Sylvain},
  year={2019}
}

@misc{wan2025,
      title={Wan: Open and Advanced Large-Scale Video Generative Models}, 
      author={WanTeam and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu},
      year={2025},
      archivePrefix={arXiv preprint arXiv:2503.20314},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.20314}, 
}

@inproceedings{dong2018boosting,
  title={Boosting adversarial attacks with momentum},
  author={Dong, Yinpeng and Liao, Fangzhou and Pang, Tianyu and Su, Hang and Zhu, Jun and Hu, Xiaolin and Li, Jianguo},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={9185--9193},
  year={2018}
}

@article{chen2024your,
  title={Your diffusion model is secretly a certifiably robust classifier},
  author={Chen, Huanran and Dong, Yinpeng and Shao, Shitong and Hao, Zhongkai and Yang, Xiao and Su, Hang and Zhu, Jun},
  journal={arXiv preprint arXiv:2402.02316},
  year={2024}
}

@inproceedings{kumar2020curse,
  title={Curse of dimensionality on randomized smoothing for certifiable robustness},
  author={Kumar, Aounon and Levine, Alexander and Goldstein, Tom and Feizi, Soheil},
  booktitle={International Conference on Machine Learning},
  pages={5458--5467},
  year={2020},
  organization={PMLR}
}

@article{ban2024crystal,
  title={The Crystal Ball Hypothesis in diffusion models: Anticipating object positions from initial noise},
  author={Ban, Yuanhao and Wang, Ruochen and Zhou, Tianyi and Gong, Boqing and Hsieh, Cho-Jui and Cheng, Minhao},
  journal={arXiv preprint arXiv:2406.01970},
  year={2024}
}

@article{groundsam,
  title={Grounded sam: Assembling open-world models for diverse visual tasks},
  author={Ren, Tianhe and Liu, Shilong and Zeng, Ailing and Lin, Jing and Li, Kunchang and Cao, He and Chen, Jiayu and Huang, Xinyu and Chen, Yukang and Yan, Feng and others},
  journal={arXiv preprint arXiv:2401.14159},
  year={2024}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@misc{strawberry,
  author = {OpenAI},
  year = {2024},
  urldate = {September 12, 2024},
  title = {Learning to Reason with LLMs}
}

@misc{fastvideo,
  author = {Hao-AI-Lab},
  year = {2025},
  urldate = {January 1, 2025},
  title = {FastVideo},
  howpublished={\url{https://github.com/hao-ai-lab/FastVideo/tree/main}},
}

@misc{deepspeed,
  author = {Microsoft},
  year = {2022},
  title = {DeepSpeed},
  howpublished={\url{https://github.com/microsoft/DeepSpeed}},
}

@article{moviegen,
  title={Movie gen: A cast of media foundation models},
  author={Polyak, Adam and Zohar, Amit and Brown, Andrew and Tjandra, Andros and Sinha, Animesh and Lee, Ann and Vyas, Apoorv and Shi, Bowen and Ma, Chih-Yao and Chuang, Ching-Yao and others},
  journal={arXiv preprint arXiv:2410.13720},
  year={2024}
}

@inproceedings{sadtalker,
  title={Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation},
  author={Zhang, Wenxuan and Cun, Xiaodong and Wang, Xuan and Zhang, Yong and Shen, Xi and Guo, Yu and Shan, Ying and Wang, Fei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8652--8661},
  year={2023}
}

@article{yin2024slow,
  title={From Slow Bidirectional to Fast Causal Video Generators},
  author={Yin, Tianwei and Zhang, Qiang and Zhang, Richard and Freeman, William T and Durand, Fredo and Shechtman, Eli and Huang, Xun},
  journal={arXiv preprint arXiv:2412.07772},
  year={2024}
}

@article{sid_lsg,
title={Long and Short Guidance in Score identity Distillation for One-Step Text-to-Image Generation},
author={Mingyuan Zhou and Zhendong Wang and Huangjie Zheng and Hai Huang},
journal={ArXiv 2406.01561},
url={https://arxiv.org/abs/2406.01561},
url_code={https://github.com/mingyuanzhou/SiD-LSG},
year={2024}
}

@article{flow_matching,
  title={Flow matching for generative modeling},
  author={Lipman, Yaron and Chen, Ricky TQ and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
  journal={arXiv preprint arXiv:2210.02747},
  year={2022}
}

@misc{FLUX,
  author = {Black Forest Labs},
  year = {2024},
  urldate = {August 1, 2024},
howpublished ={\url{https://blackforestlabs.ai/}},
  title = {FLUX}
}

@misc{AES,
  author = {Laion.ai},
  year = {2022},
  urldate = {August 16, 2022},
howpublished ={\url{https://laion.ai/blog/laion-aesthetics/}},
  title = {LAION-AESTHETICS}
}

@misc{Imagereward,
      title={ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation}, 
      author={Jiazheng Xu and Xiao Liu and Yuchen Wu and Yuxuan Tong and Qinkai Li and Ming Ding and Jie Tang and Yuxiao Dong},
      year={2023},
      eprint={2304.05977},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
}

@misc{SD35,
  author = {Stability.ai},
  year = {2024},
  urldate = {October 22, 2024},
howpublished ={\url{https://stability.ai/news/introducing-stable-diffusion-3-5}},
  title = {Introducing Stable Diffusion 3.5}
}

@article{fluid,
  title={Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens},
  author={Fan, Lijie and Li, Tianhong and Qin, Siyang and Li, Yuanzhen and Sun, Chen and Rubinstein, Michael and Sun, Deqing and He, Kaiming and Tian, Yonglong},
  journal={arXiv preprint arXiv:2410.13863},
  year={2024}
}

@misc{CLIP,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
}

@misc{HPSV2,
      title={Human Preference Score v2: A Solid Benchmark for Evaluating Human Preferences of Text-to-Image Synthesis}, 
      author={Xiaoshi Wu and Yiming Hao and Keqiang Sun and Yixiong Chen and Feng Zhu and Rui Zhao and Hongsheng Li},
      year={2023},
      eprint={2306.09341},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
}

@article{sennrich2015neural,
  title={Neural machine translation of rare words with subword units},
  author={Sennrich, Rico},
  journal={arXiv preprint arXiv:1508.07909},
  year={2015}
}

@inproceedings{
zigzag,
title={Zigzag Diffusion Sampling: The Path to Success ls Zigzag},
author={Anonymous},
booktitle={Submitted to The Thirteenth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=MKvQH1ekeY},
note={under review}
}

@inproceedings{tomesd,
  title={Token merging for fast stable diffusion},
  author={Bolya, Daniel and Hoffman, Judy},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4599--4603},
  year={2023}
}

@article{muse,
  title={Muse: Text-to-image generation via masked generative transformers},
  author={Chang, Huiwen and Zhang, Han and Barber, Jarred and Maschinot, AJ and Lezama, Jose and Jiang, Lu and Yang, Ming-Hsuan and Murphy, Kevin and Freeman, William T and Rubinstein, Michael and others},
  journal={arXiv preprint arXiv:2301.00704},
  year={2023}
}

@inproceedings{maskgit,
  title={Maskgit: Masked generative image transformer},
  author={Chang, Huiwen and Zhang, Han and Jiang, Lu and Liu, Ce and Freeman, William T},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11315--11325},
  year={2022}
}

@article{luminia_mgpt,
  title={Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation with multimodal generative pretraining},
  author={Liu, Dongyang and Zhao, Shitian and Zhuo, Le and Lin, Weifeng and Qiao, Yu and Li, Hongsheng and Gao, Peng},
  journal={arXiv preprint arXiv:2408.02657},
  year={2024}
}

@article{gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

@misc{RealVis,
 author={Mage.Space},
 year={2023},
 urldate= {December 12, 2024},
 title = {Realistic Vision model}}

@inproceedings{della3,
  title={Improving Image Generation with Better Captions},
  author={James Betker and Gabriel Goh and Li Jing and † TimBrooks and Jianfeng Wang and Linjie Li and † LongOuyang and † JuntangZhuang and † JoyceLee and † YufeiGuo and † WesamManassra and † PrafullaDhariwal and † CaseyChu and † YunxinJiao and Aditya Ramesh},
    year = {2023},
}

@inproceedings{chen2024panda,
  title={Panda-70m: Captioning 70m videos with multiple cross-modality teachers},
  author={Chen, Tsai-Shien and Siarohin, Aliaksandr and Menapace, Willi and Deyneka, Ekaterina and Chao, Hsiang-wei and Jeon, Byung Eun and Fang, Yuwei and Lee, Hsin-Ying and Ren, Jian and Yang, Ming-Hsuan and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13320--13331},
  year={2024}
}

@inproceedings{mou2024t2i,
  title={T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models},
  author={Mou, Chong and Wang, Xintao and Xie, Liangbin and Wu, Yanze and Zhang, Jian and Qi, Zhongang and Shan, Ying},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={5},
  pages={4296--4304},
  year={2024}
}

@article{CoT,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@inproceedings{msrvtt,
  title={Msr-vtt: A large video description dataset for bridging video and language},
  author={Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5288--5296},
  year={2016}
}

@article{meissonic,
  title={Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis},
  author={Bai, Jinbin and Ye, Tian and Chow, Wei and Song, Enxin and Chen, Qing-Guo and Li, Xiangtai and Dong, Zhen and Zhu, Lei and Yan, Shuicheng},
  journal={arXiv preprint arXiv:2410.08261},
  year={2024}
}

@article{llamagen,
  title={Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation},
  author={Sun, Peize and Jiang, Yi and Chen, Shoufa and Zhang, Shilong and Peng, Bingyue and Luo, Ping and Yuan, Zehuan},
  journal={arXiv preprint arXiv:2406.06525},
  year={2024}
}

@article{cui2024hallo3,
  title={Hallo3: Highly Dynamic and Realistic Portrait Image Animation with Diffusion Transformer Networks},
  author={Cui, Jiahao and Li, Hui and Zhan, Yun and Shang, Hanlin and Cheng, Kaihui and Ma, Yuqi and Mu, Shan and Zhou, Hang and Wang, Jingdong and Zhu, Siyu},
  journal={arXiv preprint arXiv:2412.00733},
  year={2024}
}

@article{portrait_transformer_1,
  title={MegActor-Sigma: Unlocking Flexible Mixed-Modal Control in Portrait Animation with Diffusion Transformer},
  author={Yang, Shurong and Li, Huadong and Wu, Juhao and Jing, Minhao and Li, Linze and Ji, Renhe and Liang, Jiajun and Fan, Haoqiang and Wang, Jin},
  journal={arXiv preprint arXiv:2408.14975},
  year={2024}
}

@article{kong2024hunyuanvideo,
  title={HunyuanVideo: A Systematic Framework For Large Video Generative Models},
  author={Kong, Weijie and Tian, Qi and Zhang, Zijian and Min, Rox and Dai, Zuozhuo and Zhou, Jin and Xiong, Jiangfeng and Li, Xin and Wu, Bo and Zhang, Jianwei and others},
  journal={arXiv preprint arXiv:2412.03603},
  year={2024}
}

@article{portrait_transformer_2,
  title={JambaTalk: Speech-Driven 3D Talking Head Generation Based on Hybrid Transformer-Mamba Model},
  author={Jafari, Farzaneh and Berretti, Stefano and Basu, Anup},
  journal={arXiv preprint arXiv:2408.01627},
  year={2024}
}

@article{portrait_transformer_3,
  title={FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait},
  author={Ki, Taekyung and Min, Dongchan and Chae, Gyoungsu},
  journal={arXiv preprint arXiv:2412.01064},
  year={2024}
}

@article{cui2024hallo2,
  title={Hallo2: Long-duration and high-resolution audio-driven portrait image animation},
  author={Cui, Jiahao and Li, Hui and Yao, Yao and Zhu, Hao and Shang, Hanlin and Cheng, Kaihui and Zhou, Hang and Zhu, Siyu and Wang, Jingdong},
  journal={arXiv preprint arXiv:2410.07718},
  year={2024}
}

@article{EMO,
  title={Vividtalk: One-shot audio-driven talking head generation based on 3d hybrid prior},
  author={Sun, Xusen and Zhang, Longhao and Zhu, Hao and Zhang, Peng and Zhang, Bang and Ji, Xinya and Zhou, Kangneng and Gao, Daiheng and Bo, Liefeng and Cao, Xun},
  journal={arXiv preprint arXiv:2312.01841},
  year={2023}
}

@inproceedings{zakharov2020fast,
  title={Fast bi-layer neural synthesis of one-shot realistic head avatars},
  author={Zakharov, Egor and Ivakhnenko, Aleksei and Shysheya, Aliaksandra and Lempitsky, Victor},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XII 16},
  pages={524--540},
  year={2020},
  organization={Springer}
}

@inproceedings{portrait_2023_difftalk,
  title={Difftalk: Crafting diffusion models for generalized audio-driven portraits animation},
  author={Shen, Shuai and Zhao, Wenliang and Meng, Zibin and Li, Wanhua and Zhu, Zheng and Zhou, Jie and Lu, Jiwen},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1982--1991},
  year={2023}
}

@misc{genmo2024mochi,
      title={Mochi 1},
      author={Genmo Team},
      year={2024},
      publisher = {GitHub},
      journal = {GitHub repository},
      howpublished={\url{https://github.com/genmoai/models}}
}

@article{univideo,
  title={Univideo: Unified understanding, generation, and editing for videos},
  author={Wei, Cong and Liu, Quande and Ye, Zixuan and Wang, Qiulin and Wang, Xintao and Wan, Pengfei and Gai, Kun and Chen, Wenhu},
  journal={arXiv preprint arXiv:2510.08377},
  year={2025}
}

@article{instructx,
  title={Instructx: Towards unified visual editing with mllm guidance},
  author={Mou, Chong and Sun, Qichao and Wu, Yanze and Zhang, Pengze and Li, Xinghui and Ye, Fulong and Zhao, Songtao and He, Qian},
  journal={arXiv preprint arXiv:2510.08485},
  year={2025}
}

@article{ditto,
  title={Scaling instruction-based video editing with a high-quality synthetic dataset},
  author={Bai, Qingyan and Wang, Qiuyu and Ouyang, Hao and Yu, Yue and Wang, Hanlin and Wang, Wen and Cheng, Ka Leong and Ma, Shuailei and Zeng, Yanhong and Liu, Zichen and others},
  journal={arXiv preprint arXiv:2510.15742},
  year={2025}
}

@article{lovora,
  title={LoVoRA: Text-guided and Mask-free Video Object Removal and Addition with Learnable Object-aware Localization},
  author={Xiao, Zhihan and Liu, Lin and Gao, Yixin and Zhang, Xiaopeng and Che, Haoxuan and Mai, Songping and Tian, Qi},
  journal={arXiv preprint arXiv:2512.02933},
  year={2025}
}

@article{editverse,
  title={Editverse: Unifying image and video editing and generation with in-context learning},
  author={Ju, Xuan and Wang, Tianyu and Zhou, Yuqian and Zhang, He and Liu, Qing and Zhao, Nanxuan and Zhang, Zhifei and Li, Yijun and Cai, Yuanhao and Liu, Shaoteng and others},
  journal={arXiv preprint arXiv:2509.20360},
  year={2025}
}

@article{reco,
	title={{Region-Constraint In-Context Generation for Instructional Video Editing}},
	author={Zhongwei Zhang and Fuchen Long and Wei Li and Zhaofan Qiu and Wu Liu and Ting Yao and Tao Mei},
	journal={arXiv preprint arXiv:2512.17650},
	year={2025}
}

@article{minimax_remover,
  title={MiniMax-Remover: Taming Bad Noise Helps Video Object Removal},
  author={Zi, Bojia and Peng, Weixuan and Qi, Xianbiao and Wang, Jianan and Zhao, Shihao and Xiao, Rong and Wong, Kam-Fai},
  journal={arXiv preprint arXiv:2505.24873},
  year={2025}
}

@inproceedings{sparge_attn,
  title={SpargeAttention: Accurate and Training-free Sparse Attention Accelerating Any Model Inference},
  author={Zhang, Jintao and Xiang, Chendong and Huang, Haofeng and Xi, Haocheng and Zhu, Jun and Chen, Jianfei and others},
  booktitle={Forty-second International Conference on Machine Learning}
}

@article{st_attn,
  title={Fast video generation with sliding tile attention},
  author={Zhang, Peiyuan and Chen, Yongqi and Su, Runlong and Ding, Hangliang and Stoica, Ion and Liu, Zhengzhong and Zhang, Hao},
  journal={arXiv preprint arXiv:2502.04507},
  year={2025}
}

@article{sw_attn,
  title={Sliding Window Attention Training for Efficient Large Language Models},
  author={Fu, Zichuan and Song, Wentao and Wang, Yejing and Wu, Xian and Zheng, Yefeng and Zhang, Yingying and Xu, Derong and Wei, Xuetao and Xu, Tong and Zhao, Xiangyu},
  journal={arXiv preprint arXiv:2502.18845},
  year={2025}
}

@inproceedings{dover_score,
  title={Exploring video quality assessment on user generated contents from aesthetic and technical perspectives},
  author={Wu, Haoning and Zhang, Erli and Liao, Liang and Chen, Chaofeng and Hou, Jingwen and Wang, Annan and Sun, Wenxiu and Yan, Qiong and Lin, Weisi},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={20144--20154},
  year={2023}
}

@article{tilelang,
  title={TileLang: A Composable Tiled Programming Model for AI Systems},
  author={Wang, Lei and Cheng, Yu and Shi, Yining and Tang, Zhengju and Mo, Zhiwen and Xie, Wenhao and Ma, Lingxiao and Xia, Yuqing and Xue, Jilong and Yang, Fan and others},
  journal={arXiv preprint arXiv:2504.17577},
  year={2025}
}

@misc{nanobanana,
  author = {Google},
  year = {2025},
  urldate = {November 20, 2025},
  title = {Nano Banana 2},
  howpublished={\url{https://gemini.google.com/app}},
}

@misc{pika,
  author = {PiKa},
  year = {2025},
  urldate = {June 20, 2025},
  title = {PiKa AI},
  howpublished={\url{https://pika-art.net/}},
}

@misc{kling15,
  author = {Kuai Shou},
  year = {2024},
  urldate = {June 1, 2024},
  title = {Kling 2.6},
  howpublished={\url{https://app.klingai.com/global/release-notes/c605hp1tzd?type=dialog}},
}

@article{seedance,
  title={Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model},
  author={Chen, Siyan and Chen, Yanfei and Chen, Ying and Chen, Zhuo and Cheng, Feng and Chi, Xuyan and Cong, Jian and Cui, Qinpeng and Dong, Qide and Fan, Junliang and others},
  journal={arXiv preprint arXiv:2512.13507},
  year={2025}
}

@article{hunyuanvideo,
  title={Hunyuanvideo 1.5 technical report},
  author={Wu, Bing and Zou, Chang and Li, Changlin and Huang, Duojun and Yang, Fang and Tan, Hao and Peng, Jack and Wu, Jianbing and Xiong, Jiangfeng and Jiang, Jie and others},
  journal={arXiv preprint arXiv:2511.18870},
  year={2025}
}

@article{z_image,
  title={Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer},
  author={Cai, Huanqia and Cao, Sihan and Du, Ruoyi and Gao, Peng and Hoi, Steven and Huang, Shijie and Hou, Zhaohui and Jiang, Dengyang and Jin, Xin and Li, Liangchen and others},
  journal={arXiv preprint arXiv:2511.22699},
  year={2025}
}

@article{ju2023direct,
  title={Direct inversion: Boosting diffusion-based editing with 3 lines of code},
  author={Ju, Xuan and Zeng, Ailing and Bian, Yuxuan and Liu, Shaoteng and Xu, Qiang},
  journal={arXiv preprint arXiv:2310.01506},
  year={2023}
}

@inproceedings{veggie,
  title={Veggie: Instructional editing and reasoning video concepts with grounded generation},
  author={Yu, Shoubin and Liu, Difan and Ma, Ziqiao and Hong, Yicong and Zhou, Yang and Tan, Hao and Chai, Joyce and Bansal, Mohit},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={15147--15158},
  year={2025}
}

@article{omniv2v,
  title={OmniV2V: Versatile Video Generation and Editing via Dynamic Content Manipulation},
  author={Liang, Sen and Yu, Zhentao and Zhou, Zhengguang and Hu, Teng and Wang, Hongmei and Chen, Yi and Lin, Qin and Zhou, Yuan and Li, Xin and Lu, Qinglin and Chen, Zhibo},
  journal={arXiv preprint arXiv:2506.01801},
  year={2025}
}

@inproceedings{cross_attn_cococo,
  title={Cococo: Improving text-guided video inpainting for better consistency, controllability and compatibility},
  author={Zi, Bojia and Zhao, Shihao and Qi, Xianbiao and Wang, Jianan and Shi, Yukai and Chen, Qianyu and Liang, Bin and Xiao, Rong and Wong, Kam-Fai and Zhang, Lei},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={10},
  pages={11067--11076},
  year={2025}
}

@inproceedings{cross_attn_avid,
  title={Avid: Any-length video inpainting with diffusion model},
  author={Zhang, Zhixing and Wu, Bichen and Wang, Xiaoyan and Luo, Yaqiao and Zhang, Luxin and Zhao, Yinan and Vajda, Peter and Metaxas, Dimitris and Yu, Licheng},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={7162--7172},
  year={2024}
}

@inproceedings{channel_fff_vdi,
  title={Video diffusion models are strong video inpainter},
  author={Lee, Minhyeok and Cho, Suhwan and Shin, Chajin and Lee, Jungho and Yang, Sunghun and Lee, Sangyoun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={4},
  pages={4526--4533},
  year={2025}
}

@article{cross_attn_vace,
  title={Vace: All-in-one video creation and editing},
  author={Jiang, Zeyinzi and Han, Zhen and Mao, Chaojie and Zhang, Jingfeng and Pan, Yulin and Liu, Yu},
  journal={arXiv preprint arXiv:2503.07598},
  year={2025}
}

@article{full_attn_unic,
  title={UNIC: Unified In-Context Video Editing},
  author={Ye, Zixuan and He, Xuanhua and Liu, Quande and Wang, Qiulin and Wang, Xintao and Wan, Pengfei and Zhang, Di and Gai, Kun and Chen, Qifeng and Luo, Wenhan},
  journal={arXiv preprint arXiv:2506.04216},
  year={2025}
}

@article{insv2v,
  title={Consistent video-to-video transfer using synthetic dataset},
  author={Cheng, Jiaxin and Xiao, Tianjun and He, Tong},
  journal={arXiv preprint arXiv:2311.00213},
  year={2023}
}

@article{lucyedit,
  title   = {Lucy Edit: Open-Weight Text-Guided Video Editing},
  author  = {DecartAI Team},
  year    = {2025},
  url     = { https://d2drjpuinn46lb.cloudfront.net/Lucy_Edit__High_Fidelity_Text_Guided_Video_Editing.pdf}
 }

@article{ivebench,
  title={Ivebench: Modern benchmark suite for instruction-guided video editing assessment},
  author={Chen, Yinan and Zhang, Jiangning and Hu, Teng and Zeng, Yuxiang and Xue, Zhucun and He, Qingdong and Wang, Chengjie and Liu, Yong and Hu, Xiaobin and Yan, Shuicheng},
  journal={arXiv preprint arXiv:2510.11647},
  year={2025}
}

@article{fa2,
  title={Flashattention-2: Faster attention with better parallelism and work partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}

@inproceedings{triton,
  title={Triton: an intermediate language and compiler for tiled neural network computations},
  author={Tillet, Philippe and Kung, H. T. and Cox, David},
  booktitle={Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
  pages={10--19},
  year={2019}
}

@article{gemini_2_5,
  title={Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities},
  author={Comanici, G. and Bieber, E. and Schaekermann, M. and others},
  journal={arXiv preprint arXiv:2507.06261},
  year={2025}
}

@article{five_benchmark,
  title={Five: A fine-grained video editing benchmark for evaluating emerging diffusion and rectified flow models},
  author={Li, M. and Xie, C. and Wu, Y. and others},
  journal={arXiv preprint arXiv:2503.13684},
  year={2025}
}

@inproceedings{tokenflow,
  title={Tokenflow: Unified image tokenizer for multimodal understanding and generation},
  author={Qu, Liao and Zhang, Huichao and Liu, Yiheng and Wang, Xu and Jiang, Yi and Gao, Yiming and Ye, Hu and Du, Daniel K and Yuan, Zehuan and Wu, Xinglong},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={2545--2555},
  year={2025}
}

@inproceedings{STDF,
  title={Unity in Diversity: Video Editing via Gradient-Latent Purification},
  author={Gao, Junyu and Yang, Kunlin and Yao, Xuan and Hu, Yufan},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={23401--23411},
  year={2025}
}

@article{senorita_2m,
  title={Se$\backslash$\~{} norita-2M: A High-Quality Instruction-based Dataset for General Video Editing by Video Specialists},
  author={Zi, Bojia and Ruan, Penghui and Chen, Marco and Qi, Xianbiao and Hao, Shaozhe and Zhao, Shihao and Huang, Youze and Liang, Bin and Xiao, Rong and Wong, Kam-Fai},
  journal={arXiv preprint arXiv:2502.06734},
  year={2025}
}

@article{icve,
  title={In-context learning with unpaired clips for instruction-based video editing},
  author={Liao, Xinyao and Zeng, Xianfang and Song, Ziye and Fu, Zhoujie and Yu, Gang and Lin, Guosheng},
  journal={arXiv preprint arXiv:2510.14648},
  year={2025}
}

@article{anyv2v,
  title={Anyv2v: A tuning-free framework for any video-to-video editing tasks},
  author={Ku, Max and Wei, Cong and Ren, Weiming and Yang, Harry and Chen, Wenhu},
  journal={arXiv preprint arXiv:2403.14468},
  year={2024}
}

@article{stablev2v,
  title={StableV2V: Stabilizing Shape Consistency in Video-to-Video Editing},
  author={Liu, Chang and Li, Rui and Zhang, Kaidong and Lan, Yunwei and Liu, Dong},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  year={2025},
  publisher={IEEE}
}
