\chapter{Supplementary figures}

\tikzstyle{my-box}=[rectangle,
    draw=hidden-draw,
    rounded corners,
    text opacity=1,
    minimum height=1.5em,
    minimum width=5em,
    inner sep=2pt,
    align=center,
    fill opacity=.5,
    line width=0.8pt,
]
\tikzstyle{leaf}=[my-box, minimum height=1.5em,
    fill=hidden-pink!80, text=black, align=left,font=\normalsize,
    inner xsep=2pt,
    inner ysep=4pt,
    line width=0.8pt,
]

% Fallback for \Description if not provided by the class/package
\providecommand{\Description}[1]{}

\begin{figure*}[!t]
\centering
\resizebox{0.99\linewidth}{!}{
\begin{forest}
    forked edges,
    for tree={
        grow=east,
        reversed=true,
        anchor=base west,
        parent anchor=east,
        child anchor=west,
        base=center,
        font=\footnotesize,
        rectangle,
        draw=hidden-draw,
        rounded corners,
        align=left,
        text centered,
        minimum width=4em,
        edge+={darkgray, line width=1pt},
        s sep=3pt,
        inner xsep=2pt,
        inner ysep=3pt,
        line width=0.8pt,
        ver/.style={rotate=90, child anchor=north, parent anchor=south, anchor=center},
    },
    where level=1{text width=14em,font=\normalsize,}{},
    where level=2{text width=12em,font=\normalsize,}{},
    where level=3{text width=15em,font=\footnotesize,}{},
    [
        Accelerated Sampling for Video Diffusion Model, ver
        [
            Step Distillation
            [
                Distribution Distillation
                [
Streaming Distillation, leaf,
                    content={ Streaming Distillation\: \cite{https://doi.org/10.48550/arxiv.2509.25161,https://doi.org/10.48550/arxiv.2506.03099,Yin_2025,yin2024slow,https://doi.org/10.48550/arxiv.2511.01266,Zhou_2025,https://doi.org/10.48550/arxiv.2510.08131,https://doi.org/10.48550/arxiv.2407.01392,https://doi.org/10.48550/arxiv.2405.11473,https://doi.org/10.48550/arxiv.2412.14169,https://doi.org/10.48550/arxiv.2506.09350,https://doi.org/10.48550/arxiv.2508.13009,https://doi.org/10.48550/arxiv.2510.02283,lu2025reward,Henschel_2025,https://doi.org/10.48550/arxiv.2506.08009,https://doi.org/10.48550/arxiv.2408.14837,https://doi.org/10.48550/arxiv.2504.13074,Sun_2025,huang2025live,https://doi.org/10.48550/arxiv.2510.03198,https://doi.org/10.48550/arxiv.2506.01380,https://doi.org/10.48550/arxiv.2509.22622,yi2025deep,wang2025restdiffusionbasedrealtimeendtoend,kodaira2025streamditrealtimestreamingtexttovideo,wang2025longdwmcrossgranularitydistillationbuilding,yi2025magicinfinitegeneratinginfinitetalking,guo2025endtoendtrainingautoregressivevideo,yu2025videossmautoregressivelongvideo,zhu2025memorizeandgeneratelongtermconsistencyrealtime,sun2025streamavatarstreamingdiffusionmodels,xiao2025knotforcingtamingautoregressive,ryu2025shragaframeworkforcombininghumaninspired,zhang2025infvsrbreakinglengthlimits,chen2024streamingvideodiffusiononline,du2025raprealtimeaudiodrivenportrait,zhang2025generativepretrainedautoregressivediffusion,zhang2025blockvidblockdiffusionhighquality,yuan2025lumos1autoregressivevideogeneration,liang2025lookingbackwardstreamingvideotovideo,yu2025autorefinerimprovingautoregressivevideo} }
                ]
                [
No Streaming Distillation, leaf,
                    content={ No Streaming Distillation\: \cite{gu2025blade,karnewar2025neodragon,https://doi.org/10.48550/arxiv.2503.06674,https://doi.org/10.48550/arxiv.2412.05899,https://doi.org/10.48550/arxiv.2510.12747,https://doi.org/10.48550/arxiv.2502.07701,https://doi.org/10.48550/arxiv.2211.11018,shao2025magicdistillationweaktostrongvideodistillation,nie2026transitionmatchingdistillationfast,liu2025equivariancefastsamplingvideo,gao2025seedance10exploringboundaries,bedel2023dreamrdiffusiondrivencounterfactualexplanation,chen2023diffusiontalkerpersonalizationaccelerationspeechdriven,teng2025gfixperceptuallyenhancedgaussian} }
                ]
            ]
            [
Consistency Distillation, leaf,
                    content={ Consistency Distillation\: \cite{https://doi.org/10.48550/arxiv.2406.06890,zhang2025mobilei2v,lv2025dcm,https://doi.org/10.48550/arxiv.2504.11143,https://doi.org/10.48550/arxiv.2312.09109,https://doi.org/10.48550/arxiv.2505.16239,Mao_2025,https://doi.org/10.48550/arxiv.2508.06082,https://doi.org/10.48550/arxiv.2410.05677,Xu_2025,Zhang_2025,https://doi.org/10.48550/arxiv.2412.15689,https://doi.org/10.48550/arxiv.2405.18750,wang2024phased,Liu_2025,zhang2025vividfacehighqualityefficientonestep,hu2024efficienttextdrivenmotiongeneration,yin2024lm2dlyricsmusicdrivendance,dai2024motionlcmrealtimecontrollablemotion,dao2025improvedtrainingtechniquelatent,yu2025lliaenablinglowlatency,zhang2025turbodiffusionacceleratingvideodiffusion} }
            ]
            [
                Adversarial distillation
                [
Combined distillation, leaf,
                    content={ Combined distillation\: \cite{https://doi.org/10.48550/arxiv.2507.18569,https://doi.org/10.48550/arxiv.2406.04324,https://doi.org/10.48550/arxiv.2503.19462,cheng2025pose,https://doi.org/10.48550/arxiv.2412.06578,https://doi.org/10.48550/arxiv.2403.12706,guo2024realtimeonestepdiffusionbasedexpressive,jiang2025motionpcmrealtimemotionsynthesis,xue2025moganimprovingmotionquality} }
                ]
                [
Independent distillation, leaf,
                    content={ Independent distillation\: \cite{https://doi.org/10.48550/arxiv.2501.08316,https://doi.org/10.48550/arxiv.2509.16507} }
                ]
            ]
        ]
        [
            Efficient Attention
            [
                Sparse Attention
                [
Dynamic Sparsity, leaf,
                    content={ Dynamic Sparsity\: \cite{https://doi.org/10.48550/arxiv.2505.18809,wu2025usv,https://doi.org/10.48550/arxiv.2510.02617,https://doi.org/10.48550/arxiv.2506.23858,https://doi.org/10.48550/arxiv.2502.01776,shmilovich2025liteattention,https://doi.org/10.48550/arxiv.2509.01085,https://doi.org/10.48550/arxiv.2505.14708,https://doi.org/10.48550/arxiv.2510.18692,https://doi.org/10.48550/arxiv.2508.21058,https://doi.org/10.48550/arxiv.2505.18875,https://doi.org/10.48550/arxiv.2509.16518,https://doi.org/10.48550/arxiv.2502.21079,https://doi.org/10.48550/arxiv.2505.13389,sparge_attn,https://doi.org/10.48550/arxiv.2412.20404,https://doi.org/10.48550/arxiv.2505.22918,https://doi.org/10.48550/arxiv.2504.12027,https://doi.org/10.48550/arxiv.2305.13077,kahatapitiya2024objectcentricdiffusionefficientvideo,zhang2025trainingfreeefficientvideogeneration,zhang2025spargeattentionaccuratetrainingfreesparse,qiao2025flashomniunifiedsparseattention,chen2025rainfusion20temporalspatialawarenesshardwareefficient,xu2025xattentionblocksparseattention,jabri2023scalableadaptivecomputationiterative,gao2025ditvrzeroshotdiffusiontransformer} }
                ]
                [
Static Sparsity, leaf,
                    content={ Static Sparsity\: \cite{https://doi.org/10.48550/arxiv.2312.06662,https://doi.org/10.48550/arxiv.2506.19852,https://doi.org/10.48550/arxiv.2506.16054,https://doi.org/10.48550/arxiv.2502.06155,https://doi.org/10.48550/arxiv.2506.03065,Wu_2023,Ruan_2023,Lin_2022,Shrivastava_2024,ghafoorian2025attention,li2025radialattentiononlogn,hassani2025generalizedneighborhoodattentionmultidimensional,meng2025holocineholisticgenerationcinematic,team2025longcat,lin2024opensoraplanopensourcelarge,xu2024mscmultiscalespatiotemporalcausal,wang2024qihoot2xefficientproxytokenizeddiffusion,ren2025groupingfirstattendingsmartly} }
                ]
            ]
            [
                Linear Attention
                [
Training-based, leaf,
                    content={ Training-based\: \cite{https://doi.org/10.48550/arxiv.2509.24695,huang2025linvideo,https://doi.org/10.48550/arxiv.2509.24006,wang2024unianimatetamingunifiedvideo,hu2024zigmaditstylezigzagmamba,ma2025consistentcontrollableimageanimation,ghafoorian2026rehyatrecurrenthybridattention,zhang2025efficientlongdurationtalkingvideo,zatsarynna2025mantadiffusionmambaefficient,wang2024opticalflowrepresentationalignment} }
                ]
                [
Training-free, leaf,
                    content={ Training-free\: \cite{} }
                ]
            ]
        ]
        [
            Model Compression
            [
                Quantization
                [
Quantization-aware training, leaf,
                    content={ Quantization-aware training\: \cite{https://doi.org/10.48550/arxiv.2509.23681,https://doi.org/10.48550/arxiv.2505.11497,https://doi.org/10.48550/arxiv.2506.04648,https://doi.org/10.48550/arxiv.2505.22167,https://doi.org/10.48550/arxiv.2503.06564,feng2025s,feng2025pmqveprogressivemultiframequantization,cao2025simplelowbitquantizationframework} }
                ]
                [
Post-training quantization, leaf,
                    content={ Post-training quantization\: \cite{https://doi.org/10.48550/arxiv.2505.18663,Chen_2025,https://doi.org/10.48550/arxiv.2406.02540,pan2025diga3dcoarsetofinediffusionalpropagation,zhang2025sageattentionaccurate8bitattention,zhang2026sageattention3microscalingfp4attention,zhang2025sageattention2efficientattentionthorough,yang2025lrqditlogrotationposttrainingquantization,feng2025s2qvditaccuratequantizedvideo,liu2024taqdittimeawarequantizationdiffusion,liu2025clqcrosslayerguidedorthogonalbased,chai2025quantvsrlowbitposttrainingquantization} }
                ]
            ]
            [
                Pruning
                [
Token pruning, leaf,
                    content={ Token pruning\: \cite{https://doi.org/10.48550/arxiv.2412.11706,Piergiovanni_2023,he2025fulldit2efficientincontextconditioning,zhang2025framecontextpackingdrift,liu2025astraeatokenwiseaccelerationframework,yuan2025vgdfrdiffusionbasedvideogeneration,gu2025longcontextautoregressivevideomodeling,bi2025efficientdiffusionbased3dhuman,yang2025scriptgraphstructuredqueryconditionedsemantic,shen2025fastviddynamicdensitypruning} }
                ]
                [
Channel pruning, leaf,
                    content={ Channel pruning\: \cite{https://doi.org/10.48550/arxiv.2412.07583} }
                ]
                [
Model pruning, leaf,
                    content={ Model pruning\: \cite{wu2025individualcontentmotiondynamics,kim2025vip,wu2025tamingdiffusiontransformerefficient,yan2024animatedstickersbringingstickers,zou2025turbovaedfaststabletransfer,li2025asymmetricvaeonestepvideo} }
                ]
            ]
        ]
        [
            Others
            [
                Cache
                [
Feature cache, leaf,
                    content={ Feature cache\: \cite{https://doi.org/10.48550/arxiv.2510.05367,https://doi.org/10.48550/arxiv.2507.02860,https://doi.org/10.48550/arxiv.2408.12588,Ceylan_2023,https://doi.org/10.48550/arxiv.2411.02397,https://doi.org/10.48550/arxiv.2410.19355,sun2024unveilingredundancydiffusiontransformers,zheng2025letfeaturesdecidesolvers,fan2025taocachestructuremaintainedvideogeneration,wang2025drivegen3dboostingfeedforwarddriving,liu2025fastcachefastcachingdiffusion,huang2025enerverseenvisioningembodiedfuture,zou2025acceleratingdiffusiontransformerstokenwise,zheng2025dynamictryontamingvideo,zou2025rethinkingtokenwisefeaturecaching,wu2025quantcacheadaptiveimportanceguidedquantization,liu2025reusingforecastingacceleratingdiffusion,zheng2025forecastcalibratefeaturecaching,feng2025hicachetrainingfreeaccelerationdiffusion,song2025herohierarchicalextrapolationrefresh,cui2025bwcacheacceleratingvideodiffusion,bu2025dicacheletdiffusionmodel,peng2025ertacacheerrorrectificationtimesteps,wei2025mixcachemixtureofcachevideodiffusion,ma2025magcachefastvideogeneration,chen2026sortblocksimilarityawarefeaturereuse,ji2025blockwiseadaptivecachingaccelerating,yang2025evctrlefficientcontroladapter,sun2024flexcacheflexibleapproximatecache,ye2025supergenefficientultrahighresolutionvideo,adnan2025foresightadaptivelayerreuse,yang2025rethinkingvideotokenizationconditioned,kong2025tamingflowbasedi2vmodels,shi2025aquariusfamilyindustrylevelvideo,li2025selfguidanceboostingflowdiffusion} }
                ]
                [
KV cache, leaf,
                    content={ KV cache\: \cite{https://doi.org/10.48550/arxiv.2411.16375,https://doi.org/10.48550/arxiv.2406.10981,jiang2025lovicefficientlongvideo,guo2025longcontexttuningvideo,xing2024live2difflivestreamtranslation,wang2025liftvsrliftingimagediffusion,zhang2025egolcdegocentricvideogeneration,zhang2026pretrainingframepreservationautoregressive,ye2025yanfoundationalinteractivevideo,li2026packcachetrainingfreeaccelerationmethod,feng2025vidarcembodiedvideodiffusion,wen2025dvladiffusionvisionlanguageactionmodel,song2025physicalautoregressivemodelrobotic,kong2025causnvsautoregressivemultiviewdiffusion,ji2025memflowflowingadaptivememory,yu2025videomarautoregressivevideogeneratio,he2025posegenincontextlorafinetuning} }
                ]
            ]
            [
                Latent Trajectory Tricks
                [
Noise / state modification, leaf,
                    content={ Noise / state modification\: \cite{https://doi.org/10.48550/arxiv.2310.15169,https://doi.org/10.48550/arxiv.2307.10373,https://doi.org/10.48550/arxiv.2304.08477,h√∏eg2024streamingdiffusionpolicyfast,liu2025free4dtuningfree4dscene} }
                ]
                [
Trajectory modification, leaf,
                    content={ Trajectory modification\: \cite{Zhang_2025,https://doi.org/10.48550/arxiv.2403.14148,https://doi.org/10.48550/arxiv.2503.18940,Esser_2023,https://doi.org/10.48550/arxiv.2410.05954,https://doi.org/10.48550/arxiv.2502.05179,zhu2025acceleratingdiffusionsamplingexploiting,liu2025gesturelsmlatentshortcutbased,kraljusic2025searchbasedrobotmotionplanning,sabour2024alignstepsoptimizingsampling,zhuang2025timestepmasterasymmetricalmixture,zhou2025unitenhancementguidanceframework,chen2026efficientcameracontrolledvideogeneration,ling2024motionclonetrainingfreemotioncloning,liu2025multimotion,wu2025attentionscalephasealignedrotary,wu2025ditpainterefficientvideoinpainting,li2025arlonboostingdiffusiontransformers,kim2025ondevicesoraenablingtrainingfree,cheng2025srdiffusionacceleratevideodiffusion,oshima2025inferencetimetexttovideoalignmentdiffusion,cheng2025adaptivebeginofvideotokensautoregressive,cao2025video} }
                ]
                [
Parallel computation, leaf,
                    content={ Parallel computation\: \cite{Zhang_2023,https://doi.org/10.48550/arxiv.2305.13077,bandyopadhyay2025blockcascadingtrainingfree,chen2025dbspacceleratingsparseattention,wang2025pipeditacceleratingdiffusiontransformers,fang2024xditinferenceenginediffusion} }
                ]
            ]
            [
                Other efficiency tricks, leaf,
                content={ Other efficiency tricks\: \cite{Shrivastava_2024,mo2024scalingdiffusionmambabidirectional,sun2025proavditprojectedlatentdiffusion,cheng2025leanvaeultraefficientreconstructionvae,li2024dicodediffusioncompresseddeeptokens,chen2025dcvideogenefficientvideogeneration,po2025longcontextstatespacevideoworld} }
            ]
        ]
    ]
\end{forest}

}
\caption{Taxonomy of accelerated sampling methods for video diffusion models based on the classified corpus (leaves show representative examples; citations to be added).}
\Description{A tree summarizing step distillation families, attention efficiency, model compression, caching/trajectory techniques, and related efficiency tricks for video diffusion models.}
\label{fig:video-accel-taxonomy}
\end{figure*}
